

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Elements of linear algebra &#8212; ECON2125/6012</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05.linear_algebra_orig';</script>
    <link rel="canonical" href="optim.iskh.me/05.linear_algebra_orig.html" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>


  <div class="bd-header-announcement container-fluid bd-header-announcement">
    <div class="bd-header-announcement__content">Latest update: solutions for Exercise set C (please reload the page)</div>
  </div>

  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="00.index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/penrose.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/penrose.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00.index.html">
                    ECON2125/6012
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01.introduction.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.optimization_intro.html">Univariate and bivariate optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="03.set_theory.html">Elements of set theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.basic_analysis.html">Basics of real analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="05.linear_algebra.html">Elements of linear algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="06.optimization_fundamentals.html">Fundamentals of optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="07.unconstrained.html">Unconstrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="08.constrained.html">Constrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="09.practical_session.html">Practical session</a></li>
<li class="toctree-l1"><a class="reference internal" href="10.envelope_maximum.html">Envelope and maximum theorems</a></li>
<li class="toctree-l1"><a class="reference internal" href="11.dynamic.html">Dynamic optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12.revision.html">Revision</a></li>
<li class="toctree-l1"><a class="reference internal" href="02.exercises.html">Exercise set A</a></li>
<li class="toctree-l1"><a class="reference internal" href="03.exercises.html">Exercise set B</a></li>
<li class="toctree-l1"><a class="reference internal" href="04.exercises.html">Exercise set C</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/fediskhakov/OptimCourse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/fediskhakov/OptimCourse/issues/new?title=Issue%20on%20page%20%2F05.linear_algebra_orig.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/05.linear_algebra_orig.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Elements of linear algebra</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#announcements-reminders">Announcements &amp; Reminders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-for-this-lecture">Plan for this lecture</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="elements-of-linear-algebra">
<h1>Elements of linear algebra<a class="headerlink" href="#elements-of-linear-algebra" title="Permalink to this heading">#</a></h1>
<p><strong>ECON2125/6012 Lecture 5</strong><br />
Fedor Iskhakov</p>
<section id="announcements-reminders">
<h2>Announcements &amp; Reminders<a class="headerlink" href="#announcements-reminders" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Tests due last night: feedback soon</p></li>
<li><p>This lecture is recorded</p></li>
<li><p>Next week we return to the class</p></li>
</ul>
</section>
<section id="plan-for-this-lecture">
<h2>Plan for this lecture<a class="headerlink" href="#plan-for-this-lecture" title="Permalink to this heading">#</a></h2>
<p>Review of some concepts in linear algebra</p>
<ol class="arabic simple">
<li><p>Vector spaces</p></li>
</ol>
<ul class="simple">
<li><p>Linear combinations, span, linear independence</p></li>
<li><p>Linear subspaces, bases and dimension</p></li>
<li><p>Linear maps and independence</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Linear equations</p></li>
</ol>
<ul class="simple">
<li><p>Basic operations with matrices</p></li>
<li><p>Square and invertible matrices</p></li>
<li><p>Determinant</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Quadratic forms</p></li>
</ol>
<ul class="simple">
<li><p>Symmetric matrices</p></li>
<li><p>Positive and negative definiteness</p></li>
</ul>
<p><strong>Supplementary reading:</strong></p>
<ul class="simple">
<li><p>Stachurski: 2.1, 3.1, 3.2</p></li>
<li><p>Simon &amp; Blume: 6.1, 6.2, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 11, 16, 23.7, 23.8</p></li>
<li><p>Sundaram: 1.3, Appendix C.1, C.2</p></li>
</ul>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{center}
\newtopic{New Topic}
\end{center}

\begin{center}
\newtopic{LINEAR ALGEBRA}
\end{center}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Motivation}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Linear algebra is used to study linear models

\vspace{1em}

Foundational for many disciplines related to economics


\vspace{1em}

\begin{itemize}
    \item Economic theory
    \item Econometrics and statistics
    \item Finance
    \item Operations research
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    
Equilibrium in a single market with price $p$
%
\begin{align*}
    q_d &amp; = a + b p
    \\
    q_s &amp; = c + d p
    \\
    q_s &amp; = q_d
\end{align*}

What price $p$ clears the market, and at what quantity $q = q_s = q_d$?

Remark: Here $a, b, c, d$ are the model \navy{parameters} or \navy{coefficients}

Treated as fixed for a single computation but might vary between
computations to better fit the data

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    

Determination of income 
%
\begin{align*}
    C &amp; = a + b(Y - T)
    \\
    E &amp; = C + I
    \\
    G &amp; = T
    \\
    Y &amp; = E
\end{align*}

Solve for $Y$ as a function of $I$ and $G$

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Bigger, more complex systems found in problems related to

\begin{itemize}
    \item Regression and forecasting
        \vspace{1em}
    \item Portfolio analysis
        \vspace{1em}
    \item Ranking systems
        \vspace{1em}
    \item Etc., etc. --- any number of applications
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A general system of equations:

\begin{equation*}
    \begin{array}{c}
        a_{11} x_1 + a_{12} x_2 + \cdots + a_{1K} x_K = b_1 \\
        a_{21} x_1 + a_{22} x_2 + \cdots + a_{2K} x_K = b_2 \\
        \vdots  \\
        a_{N1} x_1 + a_{N2} x_2 + \cdots + a_{NK} x_K = b_N 
    \end{array}
\end{equation*}

Typically 

\begin{itemize}
    \item the $a_{nm}$ and $b_n$ are exogenous / given / parameters
    \item the values $x_n$ are endogenous
\end{itemize}

Key question

\begin{itemize}
    \item What values of $x_1, \ldots, x_K$ solve this system?
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>We often write this in \navy{matrix form}

%
\begin{equation*}
    \left(
    \begin{array}{cccc}
        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1K} \\
        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NK} 
    \end{array}
    \right)
    \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        \vdots \\
        x_K
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        b_1 \\
        b_2 \\
        \vdots \\
        b_K
    \end{array}
    \right)
\end{equation*}

\vspace{1em}

or 

\begin{equation*}
    \boldA \boldx = \boldb
\end{equation*}

And we solve it on a computer
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<p>\begin{pythoncode}
In [1]: import numpy as np</p>
<p>In [2]: from scipy.linalg import solve</p>
<p>In [3]: A = [[0, 2, 4],
…:      [1, 4, 8],
…:      [0, 3, 7]]</p>
<p>In [4]: b = (1, 2, 0)</p>
<p>In [5]: A, b = np.asarray(A), np.asarray(b)</p>
<p>In [6]: solve(A, b)
Out[6]: array([ 0. ,  3.5, -1.5])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This tells us that the solution is 
</pre></div>
</div>
<p>\begin{pythoncode}
array([ 0. ,  3.5, -1.5])
\end{pythoncode}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>That is, 
$$
\boldx =
    \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        x_3
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        0 \\
        3.5 \\
        -1.5
    \end{array}
    \right)
$$


\vspace{1em}

Hey, this is easy --- what do we need to study for?

\vspace{2em}

But now let&#39;s try this similar looking problem
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<p>\begin{pythoncode}
In [1]: import numpy as np</p>
<p>In [2]: from scipy.linalg import solve</p>
<p>In [3]: A = [[0, 2, 4],
…:      [1, 4, 8],
…:      [0, 3, 6]]</p>
<p>In [4]: b = (1, 2, 0)</p>
<p>In [5]: A, b = np.asarray(A), np.asarray(b)</p>
<p>In [6]: solve(A, b)
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This is the output that we get

\begin{verbatim}
</pre></div>
</div>
<p>LinAlgError        Traceback (most recent call last)
<ipython-input-8-4fb5f41eaf7c> in <module>()
—-&gt; 1 solve(A, b)
/home/john/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.pyc in solve(a, b, sym_pos, lower, overwrite_a, overwrite_b, debug, check_finite)
97         return x
98     if info &gt; 0:
—&gt; 99         raise LinAlgError(“singular matrix”)
100     raise ValueError(‘illegal value in %d-th argument of internal gesv|posv’
LinAlgError: singular matrix
\end{verbatim}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>What does this mean?  How can we fix it?

Moral: We still need to understand the concepts
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Vector Space}</p>
<p>\begin{frame}
\frametitle{Vector Space}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Recall that $\RR^N := $ set of all $N$-vectors

\vspace{1em}

An $N$-vector $\boldx$ is a tuple of $N$ real numbers:
$$
\boldx = (x_1, \ldots, x_N)
    \quad
    \text{ where } 
    \quad x_n \in \RR \text{ for each } n
$$  

\vspace{1em}

We can also write $\boldx$ vertically, like so: 
%
\begin{equation*}
    \boldx = 
    \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        \vdots \\
        x_N
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec.tex}}
    \caption{\label{f:vec} Visualization of vector $\boldx$ in $\RR^2$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{vecs.pdf}}
    \caption{Three vectors in $\RR^2$ }
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The vector of ones will be denoted $\boldone$ 

%
\begin{equation*}
    \boldone := 
    \left(
    \begin{array}{c}
        1 \\
        \vdots \\
        1
    \end{array}
    \right)
\end{equation*}
%

Vector of zeros will be denoted $\boldzero$

%
\begin{equation*}
    \boldzero := 
    \left(
    \begin{array}{c}
        0 \\
        \vdots \\
        0
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Linear Operations}</p>
<p>\begin{frame}
\frametitle{Linear Operations}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\vspace{1em}

Two fundamental algebraic operations: 
%
\begin{enumerate}
    \item Vector addition 
    \item Scalar multiplication
\end{enumerate}


1. \navy{Sum} of $\boldx \in \RR^N$ and $\boldy \in \RR^N$ defined by

\begin{equation*}
    \boldx + \boldy 
    :=: 
    \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        \vdots \\
        x_N
    \end{array}
    \right)
    +
    \left(
    \begin{array}{c}
         y_1 \\
         y_2 \\
        \vdots \\
         y_N
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{c}
        x_1 + y_1 \\
        x_2 + y_2 \\
        \vdots \\
        x_N + y_N
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Example 1:

\begin{equation*}
    \left(
    \begin{array}{c}
        1 \\
        2 \\
        3 \\
        4
    \end{array}
    \right)
    +
    \left(
    \begin{array}{c}
         2 \\
         4 \\
         6 \\
         8
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{c}
        3 \\
        6 \\
        9 \\
        12
    \end{array}
    \right)
\end{equation*}
%

Example 2:

\begin{equation*}
    \left(
    \begin{array}{c}
        1 \\
        2 \\
        3 \\
        4
    \end{array}
    \right)
    +
    \left(
    \begin{array}{c}
         1 \\
         1 \\
         1 \\
         1
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{c}
        2 \\
        3 \\
        4 \\
        5
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec_add.tex}}
    \caption{\label{f:vec_add} Vector addition}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>2. \navy{Scalar product} of $\alpha \in \RR$ and $\boldx \in \RR^N$ defined by

\begin{equation*}
    \alpha \boldx 
    =
    \alpha \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        \vdots \\
        x_N
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{c}
        \alpha x_1 \\
        \alpha x_2 \\
        \vdots \\
        \alpha x_N
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Example 1:

\begin{equation*}
    0.5 
    \left(
    \begin{array}{c}
        1 \\
        2 \\
        3 \\
        4
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{c}
        0.5 \\
        1.0 \\
        1.5 \\
        2.0 
    \end{array}
    \right)
\end{equation*}
%

Example 2:

\begin{equation*}
    -1
    \left(
    \begin{array}{c}
        1 \\
        2 \\
        3 \\
        4
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{c}
        -1 \\
        -2 \\
        -3 \\
        -4
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec_scalar.tex}}
    \caption{\label{f:vec_scalar} Scalar multiplication}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Subtraction performed element by element, analogous to addition 

%
\begin{equation*}
    \boldx - \boldy 
    :=
    \left(
    \begin{array}{c}
        x_1 - y_1 \\
        x_2 - y_2 \\
        \vdots \\
        x_N - y_N
    \end{array}
    \right)
\end{equation*}
%

\vspace{2em}

Def can be given in terms of addition and scalar multiplication:

\begin{equation*}
  \boldx - \boldy := \boldx + (-1) \boldy      
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec_minus.tex}}
    \caption{\label{f:vec_minus} Difference between vectors}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Incidentally, most high level numerical libraries treat vector addition
and scalar multiplication in the same way --- elementwise
</pre></div>
</div>
<p>\begin{pythoncode}
In [1]: import numpy as np</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>In [2]: x = np.array((2, 4, 6))

In [3]: y = np.array((10, 10, 10))

In [4]: x + y  # Vector addition
Out[4]: array([12, 14, 16])

In [6]: 2 * x  # Scalar multiplication
Out[6]: array([ 4,  8, 12])
</pre></div>
</div>
<p>\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A \navy{linear combination} of vectors $\boldx_1,\ldots, \boldx_K$ in $\RR^N$ 
is a vector 
%
\begin{equation*}
    \boldy = \sum_{k=1}^K \alpha_k \boldx_k 
    =  \alpha_1 \boldx_1 + \cdots +  \alpha_K \boldx_K 
\end{equation*}
%
where $\alpha_1,\ldots, \alpha_K$ are scalars

\vspace{1em}

\Eg

%
\begin{equation*}
    0.5 \left(
    \begin{array}{c}
        6.0 \\
        2.0 \\
        8.0
    \end{array}
    \right)
    +
    3.0 \left(
    \begin{array}{c}
         0 \\
         1.0 \\
         -1.0
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        3.0 \\
        4.0 \\
        1.0
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Inner Product}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The \navy{inner product} of two vectors $\boldx$ and $\boldy$ in $\RR^N$ is
%
\begin{equation*}
    \boldx&#39;  \boldy :=
     \sum_{n=1}^N x_n y_n
\end{equation*}
%

Example: $\boldx = (2, 3)$ and $\boldy = (-1, 1)$ implies that
%
\begin{equation*}
    \boldx&#39;  \boldy 
    = 2 \times (-1) + 3 \times 1 
    = 1
\end{equation*}
%

Example: $\boldx = (1/N) \boldone$ and $\boldy = (y_1, \ldots, y_N)$ implies 
%
\begin{equation*}
    \boldx&#39;  \boldy 
    = \frac{1}{N} \sum_{n=1}^N y_n
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: x = np.array((1, 2, 3, 4))</p>
<p>In [3]: y = np.array((2, 4, 6, 8))</p>
<p>In [6]: np.sum(x * y)  # Inner product
Out[6]: 60
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact For any $\alpha, \beta \in \RR$ and any $\boldx, \boldy \in \RR^N$, the following 
statements are true:
%
\begin{enumerate}
    \item $\boldx&#39; \boldy = \boldy&#39; \boldx$
    \item $(\alpha \boldx)&#39; (\beta \boldy) =  \alpha \beta (\boldx&#39; \boldy)$
    \item $\boldx&#39; (\boldy + \boldz) =  \boldx&#39; \boldy + \boldx&#39; \boldz$
\end{enumerate}
%

\vspace{1em}

For example, item 2 is true because
%
\begin{equation*}
    (\alpha \boldx)&#39; (\beta \boldy) 
    = \sum_{n=1}^N \alpha x_n \beta y_n
    = \alpha \beta \sum_{n=1}^N x_n y_n
    =  \alpha \beta (\boldx&#39; \boldy)
\end{equation*}

\Ex Use above rules to show that 
$(\alpha \boldy + \beta \boldz)&#39;\boldx =  \alpha \boldx&#39; \boldy + \beta \boldx&#39; \boldz$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The next result is a generalization

\vspace{1em}


\Fact Inner products of linear combinations satisfy 
%
\begin{equation*}
  \left(
      \sum_{k=1}^K \alpha_k \boldx_k
  \right)&#39; 
  \left(
      \sum_{j=1}^J \beta_j \boldy_j 
  \right)
  =
      \sum_{k=1}^K \sum_{j=1}^J \alpha_k \beta_j \boldx_k&#39; \boldy_j 
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Norms and Distance}</p>
<p>\begin{frame}
\frametitle{Norms and Distance}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The (Euclidean) \navy{norm} of $\boldx \in \RR^N$ is defined as
%
\begin{equation*}
    \| \boldx \| 
    := \sqrt{\boldx&#39; \boldx } 
    = \left( \sum_{n=1}^N x_n^2 \right)^{1/2}
\end{equation*}
%

Interpretation:
%
\begin{itemize}
    \item $\| \boldx \|$ represents the ``length&#39;&#39; of $\boldx$
        \vspace{0.5em}
    \item $\| \boldx - \boldy \|$ represents distance between $\boldx$ and $\boldy$
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.95}{\input{../tikzfigs/vec.tex}}
    \caption{\label{f:vec_rpt} Length of red line $= \sqrt{x_1^2 + x_2^2}
        =: \|\boldx\|$ }
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$\| \boldx - \boldy \|$ represents distance between $\boldx$ and $\boldy$

\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec_minus.tex}}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact For any $\alpha \in \RR$ and any $\boldx, \boldy \in \RR^N$, the following 
statements are true:
%
\begin{enumerate}
    \item $\| \boldx \| \geq 0$ and $\| \boldx \| = 0$ if and only if
        $\boldx = \boldzero$
        \vspace{1em}
    \item $\| \alpha \boldx \| = |\alpha| \| \boldx \|$
        \vspace{1em}
    \item $\| \boldx + \boldy \| \leq  \| \boldx \| + \| \boldy \|$
        (\navy{triangle inequality})
        \vspace{1em}
    \item $| \boldx&#39; \boldy | \leq  \| \boldx \| \| \boldy \|$
        (\navy{Cauchy-Schwarz inequality})
\end{enumerate}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>For example, let&#39;s show that $\| \boldx \| = 0 \iff \boldx = \boldzero$

\vspace{1em}

First let&#39;s assume that $\| \boldx \| = 0$ and show $\boldx = \boldzero$

Since $\| \boldx \| = 0$ we have $\| \boldx \|^2 = 0$ and hence
$\sum_{n=1}^N x^2_n = 0$

That is $x_n = 0$ for all $n$, or, equivalently, $\boldx = \boldzero$

\vspace{1em}

Next let&#39;s assume that $\boldx = \boldzero$ and show $\| \boldx \| = 0$ 

This is immediate from the definition of the norm
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $\boldx \in \RR^N$ is nonzero, then the solution to the optimization problem 
%
\begin{equation*}
    \max_{\boldy} \boldx&#39;\boldy 
    \quad \text{ subject to }  \quad
    \boldy \in \RR^N \text{ and } \| \boldy \| = 1     
\end{equation*}
%
is $\hboldx := \boldx / \| \boldx \|$

\vspace{-1em}


\begin{figure}
   \begin{center}
    \scalebox{.65}{\input{../tikzfigs/max_inner_prod.tex}}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Proof: Fix nonzero $\boldx \in \RR^N$

Let $\hboldx := \boldx / \| \boldx \| := \alpha \boldx$ when $\alpha := 1/\|\boldx\|$

Evidently $\| \hboldx \| = 1$

Pick any other $\boldy \in \RR^N$ satisfying $\| \boldy \| = 1$

The Cauchy-Schwarz inequality yields
%
\begin{equation*}
    \boldy&#39; \boldx
    \leq |\boldy&#39; \boldx|
    \leq \| \boldy \| \| \boldx \|
    = \| \boldx \|
    = \frac{\boldx&#39; \boldx}{ \| \boldx \| }
    = \hboldx&#39; \boldx
\end{equation*}
%
Hence $\hboldx$ is the maximizer, as claimed
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Span}</p>
<p>\begin{frame}
\frametitle{Span}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $X \subset \RR^N$ be any nonempty set

\vspace{1em}

Set of all possible linear combinations of elements of $X$ is 
called the \navy{span} of $X$, denoted by $\Span(X)$

\vspace{1em}

For finite $X := \{\boldx_1,\ldots, \boldx_K\}$ the span can be expressed
as 
% 
\begin{equation*}
    \Span(X):= \left\{ \text{ all } \sum_{k=1}^K \alpha_k \boldx_k 
    \text{ such that }
     (\alpha_1,\ldots, \alpha_K) \in \RR^K \right\}
\end{equation*}
%

We are mainly interested in the span of finite sets...
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    
Let&#39;s start with the span of a singleton

Let $X = \{ \boldone \} \subset \RR^2$, where $\boldone := (1,1)$  

The span of $X$ is all vectors of the form 



$$
\alpha \boldone 
=
\left(
\begin{array}{c}
    \alpha \\
    \alpha
\end{array}
\right)
\quad \text{ with } \quad \alpha \in \RR  
$$

Constitutes a line in the plane that passes through

\begin{itemize}
    \item the vector $\boldone$  (set $\alpha = 1$)
    \item the origin $\boldzero$ (set $\alpha = 0$)
\end{itemize}

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \centering
    \scalebox{.4}{\includegraphics{span_of_one_vec.pdf}}
    \caption{The span of $\boldone := (1,1)$ in $\RR^2$}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    

Let $\boldx_1 = (3, 4, 2)$ and let $\boldx_2 = (3, -4, 0.4)$

By definition, the span is all vectors of the form


\begin{equation*}
    \boldy = 
    \alpha \left(
    \begin{array}{c}
        3 \\
        4 \\
        2
    \end{array}
    \right)
    +
    \beta \left(
    \begin{array}{c}
         3 \\
         -4 \\
         0.4
    \end{array}
    \right)
    \quad \text{where} \quad
    \alpha, \beta \in \RR
\end{equation*}
%

It turns out to be a plane that passes through

\begin{itemize}
    \item the vector $\boldx_1$
    \item the vector $\boldx_2$
    \item the origin $\boldzero$
\end{itemize}

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<p>\begin{figure}
\scalebox{.42}{\includegraphics[trim=110 0 0 0, clip]{span_plane.pdf}}
\caption{\label{f:span_plane} Span of <span class="math notranslate nohighlight">\(\boldx_1, \boldx_2\)</span>}
\end{figure}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $X \subset Y$, then $\Span(X) \subset \Span(Y)$

To see this, pick any nonempty $X \subset Y \subset \RR^N$

Letting $\boldz \in \Span(X)$, we have

\begin{equation*}
    \boldz = \sum_{k=1}^K \alpha_k \boldx_k 
    \text{ for some }
    \boldx_1, \ldots, \boldx_K \in X, \; 
    \alpha_1, \ldots, \alpha_K \in \RR
\end{equation*}
%

Since $X \subset Y$, each $\boldx_k$ is also in $Y$, giving us

\begin{equation*}
    \boldz = \sum_{k=1}^K \alpha_k \boldx_k 
    \text{ for some }
    \boldx_1, \ldots, \boldx_K \in Y, \; 
    \alpha_1, \ldots, \alpha_K \in \RR
\end{equation*}
%

Hence $\boldz \in \Span(Y)$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $Y$ be any subset of $\RR^N$, and let $X:= \{\boldx_1,\ldots, \boldx_K\}$ 

\vspace{1em}

If $Y \subset \Span(X)$, we say that the vectors in $X$ \navy{span the set} $Y$

\vspace{1em}

Alternatively, we say that $X$ is a \navy{spanning set} for $Y$

\vspace{1em}

A nice situation: $Y$ is large but $X$ is small

\vspace{1em}

$\implies$ large set $Y$ ``described&#39;&#39; by the small number of vectors in $X$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    

Consider the vectors $\{\bolde_1, \ldots, \bolde_N\} \subset \RR^N$, where

\begin{equation*}
    \bolde_1 := 
    \left(
    \begin{array}{c}
        1 \\
        0 \\
        \vdots \\
        0
    \end{array}
    \right),
    \quad 
    \bolde_2 := 
    \left(
    \begin{array}{c}
        0 \\
        1 \\
        \vdots \\
        0
    \end{array}
    \right),
    \; 
    \cdots,
    \;
    \bolde_N := 
    \left(
    \begin{array}{c}
        0 \\
        0 \\
        \vdots \\
        1
    \end{array}
    \right)
\end{equation*}
%

\vspace{1em}

That is, $\bolde_n$ has all zeros except for a $1$ as the $n$-th element

\vspace{1em}

Vectors $\bolde_1, \ldots, \bolde_N$ called the \navy{canonical basis vectors} of $\RR^N$

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec_canon.tex}}
    \caption{\label{f:vec_canon} Canonical basis vectors in $\RR^2$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact The span of $\{\bolde_1, \ldots, \bolde_N\}$ is equal
to all of $\RR^N$

Proof for $N=2$: 

Pick any $\boldy \in \RR^2$ 

We have
%
\begin{multline*}
    \boldy 
    :=
    \left(
    \begin{array}{c}
        y_1 \\
        y_2
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        y_1 \\
        0
    \end{array}
    \right)
    +
    \left(
    \begin{array}{c}
        0 \\
        y_1
    \end{array}
    \right)
    \\
    =
    y_1
    \left(
    \begin{array}{c}
        1 \\
        0
    \end{array}
    \right)
    +
    y_2
    \left(
    \begin{array}{c}
        0 \\
        1
    \end{array}
    \right)
    = y_1 \bolde_1 + y_2 \bolde_2
\end{multline*}
%
Thus, $\boldy \in \Span \{\bolde_1, \bolde_2\}$ 

Since $\boldy$ arbitrary, we have shown that $\Span \{\bolde_1,
\bolde_2\} = \RR^2$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.95}{\input{../tikzfigs/vec_canon_x.tex}}
    \caption{\label{f:vec_canon2} Canonical basis vectors in $\RR^2$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Consider the set 
%
\begin{equation*}
    P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}
\end{equation*}
%

Graphically, $P =$ flat plane in $\RR^3$, where height
coordinate $=0$

\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{flat_plane_no_vecs.pdf}}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\bolde_1$ and $\bolde_2$ be the canonical basis vectors in $\RR^3$

\underline{Claim}: $\Span\{\bolde_1, \bolde_2\} = P$ 

Proof:

Let $\boldx = (x_1, x_2, 0)$ be any element of $P$

We can write $\boldx$ as 

%
\begin{equation*}
    \boldx = 
    \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        0
    \end{array}
    \right)
    =
    x_1
    \left(
    \begin{array}{c}
        1 \\
        0 \\
        0
    \end{array}
    \right)
    + 
    x_2
    \left(
    \begin{array}{c}
        0 \\
        1 \\
        0
    \end{array}
    \right)
    = x_1 \bolde_1 + x_2 \bolde_2
\end{equation*}
%

In other words, $P \subset \Span\{\bolde_1, \bolde_2\}$

Conversely (check it) we have $\Span\{\bolde_1, \bolde_2\} \subset P$ 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{flat_plane_e_vecs.pdf}}
   \end{center}
   \caption{$\Span\{\bolde_1, \bolde_2\} = P$}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Linear Subspaces}</p>
<p>\begin{frame}
\frametitle{Linear Subspaces}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A nonempty $S \subset \RR^N$ called a \navy{linear
subspace} of $\RR^N$ if
%
\begin{equation*}
    \boldx, \boldy \in S \; \text{ and }  \;\alpha, \beta \in \RR
    \quad \implies \quad
    \alpha \boldx + \beta \boldy \in S 
\end{equation*}

In other words, $S \subset \RR^N$ is ``closed&quot; under vector addition
and scalar multiplication

\vspace{1em}

Note: Sometimes we just say \navy{subspace}...

\vspace{1em}

\Eg $\RR^N$ itself is a linear subspace of $\RR^N$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}

Fix $\bolda \in \RR^N$ and let $A := \setntn{\boldx \in \RR^N}{\bolda&#39; \boldx  = 0}$

\vspace{1em}

\Fact The set $A$ is a linear subspace of $\RR^N$

\vspace{1em}

Proof: Let $\boldx, \boldy \in A$ and let $\alpha, \beta \in \RR$

We must show that $\boldz := \alpha \boldx + \beta \boldy \in A$ 

Equivalently, that $\bolda&#39; \boldz = 0$

True because
%
$$
\bolda&#39; \boldz =
\bolda&#39; (\alpha \boldx + \beta \boldy) = \alpha
\bolda&#39; \boldx + \beta \bolda&#39; \boldy = 0 + 0 = 0
$$
    
\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $Z$ is a nonempty subset of $\RR^N$, then $\Span(Z)$ is a linear
subspace

Proof: If $\boldx, \boldy \in \Span(Z)$, then $\exists$ vectors $\boldz_k$ in $Z$ 
and scalars $\gamma_k$ and $\delta_k$ such that 
%
\begin{equation*}
    \boldx = \sum_{k=1}^K \gamma_k \boldz_k
    \quad \text{and} \quad
    \boldy = \sum_{k=1}^K \delta_k \boldz_k
\end{equation*}
%
\begin{equation*}
    \fore
    \alpha \boldx = \sum_{k=1}^K \alpha \gamma_k \boldz_k
    \quad \text{and} \quad
    \beta \boldy = \sum_{k=1}^K \beta \delta_k \boldz_k 
\end{equation*}
%
\begin{equation*}
    \fore
    \alpha \boldx + \beta \boldy 
    = \sum_{k=1}^K (\alpha \gamma_k + \beta \delta_k) \boldz_k  
\end{equation*}
%

This vector clearly lies in $\Span(Z)$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $S$ and $S&#39;$ are two linear subspaces of $\RR^N$, then $S
\cap S&#39;$ is also a linear subspace of $\RR^N$.

\vspace{1em}

Proof: Let $S$ and $S&#39;$ be two linear subspaces of $\RR^N$

Fix $\boldx, \boldy \in S \cap S&#39;$ and $\alpha, \beta \in \RR$

We claim that $\boldz := \alpha \boldx + \beta \boldy \in S \cap S&#39;$

\begin{itemize}
    \item Since $\boldx, \boldy \in S$ and $S$ is a linear subspace we have $\boldz \in S$
        \vspace{0.5em}
    \item Since $\boldx, \boldy \in S&#39;$ and $S&#39;$ is a linear subspace we have $\boldz \in S&#39;$
\end{itemize}

Therefore $\boldz \in S \cap S&#39;$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Other examples of linear subspaces
%    
\begin{itemize}
    \item The singleton $\{\boldzero\}$ in $\RR^N$
        \vspace{1em}
    \item Lines through the origin in $\RR^2$ and $\RR^3$
        \vspace{1em}
    \item Planes through the origin in $\RR^3$
\end{itemize}

        \vspace{1em}

\Ex Let $S$ be a linear subspace of $\RR^N$.  Show that

\begin{enumerate}
    \item $\boldzero \in S$
    \item If $X \subset S$, then $\Span(X) \subset S$
    \item $\Span(S) = S$
\end{enumerate}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Linear Independence}</p>
<p>\begin{frame}
\frametitle{Linear Independence}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Important applied questions
\vspace{1em}

\begin{itemize}
    \item When is a matrix invertible?
    \item When do regression arguments suffer from collinearity?
    \item When does a set of linear equations have a solution?
    \item When is that solution unique?
    \item How can we approximate complex functions parsimoniously?
    \item What is the rank of a matrix?
\end{itemize}

\vspace{1em}

All of these questions closely related to linear independence 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Definition}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A nonempty collection of vectors $X := \{\boldx_1,\ldots, \boldx_K\}
\subset \RR^N$ is called \navy{linearly independent} if
%
\begin{equation*}
    \sum_{k=1}^K \alpha_k \boldx_k
     = \boldzero 
    \; \implies \;
    \alpha_1 = \cdots = \alpha_K = 0
\end{equation*}

\vspace{1em}

As we&#39;ll see, linear independence of a set of vectors determines how large
a space they span

Loosely speaking, linearly independent sets span large spaces
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Let $\boldx := (1, 2)$ and $\boldy := (-5, 3)$
    
The set $X = \{\boldx, \boldy\}$ is linearly independent in $\RR^2$

    Indeed, suppose $\alpha_1$ and $\alpha_2$ are scalars with

    \begin{equation*}
        \alpha_1
        \left(
        \begin{array}{c}
            1 \\
            2
        \end{array}
        \right)
        + 
        \alpha_2
        \left(
        \begin{array}{c}
            -5 \\
            3
        \end{array}
        \right)
        =
        \boldzero
    \end{equation*}
    %
    Equivalently,
    %
    \begin{align*}
        \alpha_1 &amp; = 5 \alpha_2
        \\
        2 \alpha_1 &amp; = -3 \alpha_2
    \end{align*}

    Then $2(5\alpha_2) = 10 \alpha_2 = -3 \alpha_2$, implying $\alpha_2 = 0$
    and hence $\alpha_1 = 0$ 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    
The set of canonical basis vectors $\{\bolde_1, \ldots, \bolde_N\}$
is linearly independent in $\RR^N$

Proof: Let $\alpha_1, \ldots, \alpha_N$ be coefficients such that
$\sum_{k=1}^N \alpha_k \bolde_k = \boldzero$

\vspace{1em}

Then
%
\begin{equation*}
    \left(
    \begin{array}{c}
        \alpha_1 \\
        \alpha_2 \\
        \vdots \\
        \alpha_N
    \end{array}
    \right)
    = \sum_{k=1}^N \alpha_k \bolde_k 
    = \boldzero
    =
    \left(
    \begin{array}{c}
        0 \\
        0 \\
        \vdots \\
        0
    \end{array}
    \right)
\end{equation*}

\vspace{1em}

In particular, $\alpha_k = 0$ for all $k$

Hence  $\{\bolde_1, \ldots, \bolde_N\}$ linearly independent

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>As a first step to better understanding linear independence let&#39;s look at
some equivalences

\vspace{0.5em}

Take $X := \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$


\Fact For $K &gt; 1$ all of following statements are equivalent
%
\begin{enumerate}
    \item $X$ is linearly independent
        \vspace{0.6em}
    \item No $\boldx_i \in X$ can be written as linear combination of the others
        \vspace{0.6em}
    \item $X_0 \subsetneq X \implies \Span(X_0) \subsetneq \Span(X)$
\end{enumerate}
%

\vspace{1em}


\begin{itemize}
    \item Here $X_0 \subsetneq X$ means $X_0 \subset X$ and $X_0 \not= X$
        \vspace{0.5em}
    \item We say that $X_0$ is a \navy{proper subset} of $X$ 
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>As an exercise, let&#39;s show that the first two statements are equivalent

The first is
%
\begin{equation}
    \label{eq:cli} 
    \tag{$\star$}
    \sum_{k=1}^K \alpha_k \boldx_k
     = \boldzero 
    \; \implies \;
    \alpha_1 = \cdots = \alpha_K = 0
\end{equation}

The second is
%
\begin{equation}
    \label{eq:cli2} 
    \tag{$\star\star$}
    \text{no $\boldx_i \in X$ can be written as linear combination of others}
\end{equation}

We now show that
%
\begin{itemize}
    \item \eqref{eq:cli} $\implies$ \eqref{eq:cli2}, and
    \item \eqref{eq:cli2} $\implies$ \eqref{eq:cli}
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>To show that \eqref{eq:cli} $\implies$ \eqref{eq:cli2} let&#39;s suppose to the contrary that

\begin{enumerate}
    \item $\sum_{k=1}^K \alpha_k \boldx_k = \boldzero \implies \alpha_1 = \cdots = \alpha_K = 0$ 
    \item and yet some $\boldx_i$ can be written as a linear combination
        of the other elements of $X$
\end{enumerate}

\vspace{1em}

In particular, suppose that
$$
    \boldx_i = \sum_{k \not= i} \alpha_k \boldx_k 
$$
Then, rearranging,
%
$$
    \alpha_1 \boldx_1 + \cdots + (-1) \boldx_i 
    + \cdots + \alpha_K \boldx_K = \boldzero
$$

This contradicts 1., and hence \eqref{eq:cli2} holds
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>To show that \eqref{eq:cli2} $\implies$ \eqref{eq:cli} let&#39;s suppose to
the contrary that 

\begin{enumerate}
    \item no $\boldx_i$ can be written as a linear combination of others
    \item and yet $\exists$ $\alpha_1, \ldots, \alpha_K$ not all zero with $\alpha_1 \boldx_1 + \cdots + \alpha_K \boldx_K = \boldzero$ 
\end{enumerate}

Suppose without loss of generality that $\alpha_1 \not= 0$

(Similar argument works for any $\alpha_j$)

\vspace{1em}

Then
%
$$
    \boldx_1 = \frac{\alpha_2}{-\alpha_1} \boldx_2 
        + \cdots + \frac{\alpha_K}{-\alpha_1} \boldx_K 
$$

\vspace{1em}

This contradicts 1., and hence \eqref{eq:cli} holds
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let&#39;s show one more part of the proof as an exercise:
%
$$
 X \text{ linearly independent } 
 \implies
 \text{ proper subsets of $X$ have smaller span}
$$

Proof: Suppose to the contrary that 
%
\begin{enumerate}
    \item $X$ is linearly independent,
    \item $X_0 \subsetneq X$ and yet
    \item $\Span(X_0) = \Span(X)$
\end{enumerate}

Let $\boldx_j$ be in $X$ but not $X_0$

Since $\boldx_j \in \Span(X)$, we also have $\boldx_j \in \Span(X_0)$

But then $\boldx_j$ can be written as a linear combination of the other
elements of $X$

This contradicts linear independence
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Dropping any of the canonical basis vectors reduces span

Consider the $N=2$ case

We know that $\Span \{\bolde_1, \bolde_2\} =$ all of $\RR^2$

Removing either element of $\Span \{\bolde_1, \bolde_2\}$ reduces the span to a line

\begin{figure}
   \begin{center}
    \scalebox{.9}{\input{../tikzfigs/vec_h_axis.tex}}
    \caption{\label{f:vec_h_axis} The span of $\{\bolde_1\}$ alone is the horizonal axis}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg As another visual example of linear independence, consider the pair

$$
\boldx_1 =
    \begin{pmatrix}
        3 \\
        4 \\
        2
    \end{pmatrix}
\quad \text{and} \quad
\boldx_2 =
    \begin{pmatrix}
        3 \\
        -4 \\
        1
    \end{pmatrix}
$$

\vspace{2em}

The span of this pair is a plane in $\RR^3$

\vspace{2em}

But if we drop either one the span reduces to a line
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{nonredundant1.pdf}}
       \caption{The span of $\{\boldx_1, \boldx_2\}$ is a plane}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{nonredundant2.pdf}}
       \caption{The span of $\{\boldx_1\}$ alone is a line}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{nonredundant3.pdf}}
       \caption{The span of $\{\boldx_2\}$ alone is a line}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Linear Dependence}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>If $X$ is not linearly independent then it is called \navy{linearly dependent}

We saw above that 

\begin{center}
linear independence $\iff$ dropping any elements reduces span
\end{center}

Hence $X$ is linearly dependent when some
elements can be removed without changing $\Span(X)$

That is,
%
$$\exists \, X_0 \subsetneq X \; \st \; \Span(X_0) = \Span(X)$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg As an example with redundacy, consider $\{\boldx_1, \boldx_2\} \subset \RR^2$ where 
%
\begin{itemize}
    \item $\boldx_1 = \bolde_1 := (1, 0)$
    \item $\boldx_2 = (-2, 0)$
\end{itemize}


\begin{figure}
   \begin{center}
    \scalebox{.9}{\input{../tikzfigs/vec_noncanon.tex}}
    \caption{\label{f:vec_noncanon} The vectors $\boldx_1$ and $\boldx_2$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>We claim that $\Span \{\boldx_1, \boldx_2\} = \Span\{\boldx_1\}$

Proof: $\Span \{\boldx_1\} \subset \Span\{\boldx_1, \boldx_2\}$ is clear (why?)

To see the reverse, pick any $\boldy \in \Span \{\boldx_1, \boldx_2\}$ 

By definition, 
$$
    \exists \;
    \alpha_1,\alpha_2 \; \st \;
    \boldy 
    = \alpha_1 \boldx_1 + \alpha_2 \boldx_2
    =
    \alpha_1 
    \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}
    + \alpha_2 
    \begin{pmatrix}
        -2 \\
        0
    \end{pmatrix}
$$
%
\begin{equation*}
    \fore 
    \boldy 
    = \alpha_1 
    \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}
    - 2 \alpha_2 
    \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}
    = (\alpha_1 - 2 \alpha_2)
    \begin{pmatrix}
        1 \\
        0
    \end{pmatrix}
    = (\alpha_1  -2 \alpha_2 ) \boldx_1 
\end{equation*}

The right hand side is clearly in $\Span \{\boldx_1\}$

Hence $\Span \{\boldx_1, \boldx_2\} \subset \Span \{\boldx_1\}$ as claimed
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Implications of Independence}</p>
<p>\begin{frame}
\frametitle{Implications of Independence}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $X := \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$

\vspace{1em}

\Fact If $X$ is linearly independent, then $X$ does not contain $\boldzero$

\Ex Prove it

\Fact If $X$ is linearly independent, then every subset of $X$ is linearly independent 
\vspace{1em}

Sketch of proof: Suppose for example that $\{\boldx_1,\ldots,
\boldx_{K-1}\} \subset X$ is linearly dependent

Then $\exists \; \alpha_1, \ldots, \alpha_{K-1}$ not all zero with 
$\sum_{k=1}^{K-1} \alpha_k \boldx_k = \boldzero$

Setting $\alpha_K =0$ we can write this as $\sum_{k=1}^K \alpha_k \boldx_k = \boldzero$

Not all scalars zero so contradicts linear independence of $X$ 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $X:= \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$ is linearly
independent and $\boldz$ is an $N$-vector not in $\Span(X)$, then $X \cup
\{ \boldz \}$ is linearly independent 

\vspace{0.8em}

Proof: Suppose to the contrary that $X \cup \{ \boldz \}$ is linearly
dependent:
%
\begin{equation}
    \label{eq:m}
    \exists \; \alpha_1, \ldots, \alpha_K, \beta
    \text{ not all zero with }
    \sum_{k=1}^K \alpha_k \boldx_k + \beta \boldz = \boldzero
\end{equation}

If $\beta=0$, then by \eqref{eq:m} we have $\sum_{k=1}^K \alpha_k \boldx_k = \boldzero$ and 
$\alpha_k \not= 0$ for some $k$, a contradiction


If $\beta \not=0$, then by \eqref{eq:m} we have 
%
$$
\boldz = \sum_{k=1}^K \frac{-\alpha_k}{\beta} \boldx_k  
$$
Hence $\boldz \in \Span(X)$ --- contradiction
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Unique Representations}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let 
% 
\begin{itemize}
    \item $X := \{\boldx_1,\ldots,\boldx_K\} \subset \RR^N$
        \vspace{0.6em}
    \item $\boldy \in \RR^N$
\end{itemize}


We know that if $\boldy \in \Span(X)$, then exists representation
%
\begin{equation*}
    \boldy = \sum_{k=1}^K \alpha_k \boldx_k
\end{equation*}

But when is this representation unique?

\vspace{1em}

Answer: When $X$ is linearly independent
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact  If $X = \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$ is linearly independent and
$\boldy \in \RR^N$, then there is at most one set of scalars $\alpha_1,\ldots,\alpha_K$ such
that $\boldy = \sum_{k=1}^K \alpha_k \boldx_k$

\vspace{1em}

Proof: Suppose there are two such sets of scalars

That is,
    %
    \begin{equation*}
        \exists \;
        \alpha_1, \ldots, \alpha_K
        \text{ and } \beta_1, \ldots, \beta_K
        \; \st \; 
        \boldy 
        = \sum_{k=1}^K \alpha_k \boldx_k
        = \sum_{k=1}^K \beta_k \boldx_k
    \end{equation*}
    %
    \begin{equation*}
        \fore \sum_{k=1}^K (\alpha_k - \beta_k) \boldx_k = \boldzero
    \end{equation*} 

    \begin{equation*}
        \fore
        \alpha_k = \beta_k 
        \quad \text{for all} \quad k
    \end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Span and Independence}</p>
<p>\begin{frame}
\frametitle{Exchange Lemma}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Here&#39;s one of the most fundamental results in linear algebra

\Fact (Exchange lemma) If 
%
\begin{enumerate}
    \item $S$ is a linear subspace of $\RR^N$
    \item $S$ is spanned by $K$ vectors,
\end{enumerate}
%
then any linearly independent subset of $S$ has at most $K$
    vectors

\vspace{1em}

Proof: Omitted

\vspace{1em}

\Eg If $X := \{\boldx_1, \boldx_2, \boldx_3\} \subset \RR^2$
     then $X$ is linearly dependent


 \begin{itemize}
     \item because $\RR^2$ is spanned by the two vectors $\bolde_1, \bolde_2$
 \end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{vecs.pdf}}
    \caption{Must be linearly dependent}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    

Recall the plane
%
\begin{equation*}
    P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}
\end{equation*}
%

\begin{itemize}
    \item flat plane in $\RR^3$ where height coordinate $=$ zero  
\end{itemize}

We showed before that $\Span\{\bolde_1, \bolde_2\} = P$ for 

\begin{equation*}
    \bolde_1 := 
    \left(
    \begin{array}{c}
        1 \\
        0 \\
        0
    \end{array}
    \right),
    \quad 
    \bolde_2 := 
    \left(
    \begin{array}{c}
        0 \\
        1 \\
        0
    \end{array}
    \right)
\end{equation*}
%

Therefore any three vectors lying in $P$ are linearly dependent

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.36}{\includegraphics{flat_plane.pdf}}
       \caption{Any three vectors in $P$ are linearly dependent}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{When Do <span class="math notranslate nohighlight">\(N\)</span> Vectors Span <span class="math notranslate nohighlight">\(\RR^N\)</span>?}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>In general, linearly independent vectors have a relatively ``large&quot; span

\begin{itemize}
    \item No vector is redundant, so each contributes to the span
\end{itemize}

This helps explain the following fact:

Let $X := \{ \boldx_1, \ldots, \boldx_N \}$ be any $N$ vectors in $\RR^N$

\Fact $\Span(X) = \RR^N$ if and only if $X$ is linearly independent
        
\vspace{2em}

\Eg The vectors $\boldx = (1, 2)$ and $\boldy = (-5, 3)$ span $\RR^2$

\begin{itemize}
    \item We already showed $\{\boldx, \boldy\}$ is linearly independent
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let&#39;s start with the proof that 

\begin{center}
    $X= \{ \boldx_1, \ldots, \boldx_N \}$ linearly independent $\implies$ $\Span(X) = \RR^N$
\end{center}

Seeking a contradiction, suppose that 

\begin{enumerate}
    \item $X $ is linearly independent 
    \item and yet $\exists \, \boldz \in \RR^N$ with $\boldz \notin \Span(X)$ 
\end{enumerate}


But then $X \cup \{\boldz\} \subset \RR^N$ is linearly independent (why?)

This set has $N+1$ elements 

And yet $\RR^N$ is spanned by the $N$ canonical basis vectors

Contradiction (of what?)
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Next let&#39;s show the converse

\begin{center}
$\Span(X) = \RR^N$
$\implies$ 
$X= \{ \boldx_1, \ldots, \boldx_N \}$ linearly independent
\end{center}

Seeking a contradiction, suppose that 

\begin{enumerate}
    \item $\Span(X) = \RR^N$ 
    \item and yet $X$ is linearly dependent 
\end{enumerate}

Since $X$ not independent, $\exists X_0 \subsetneq X$ with $\Span(X_0) =
\Span(X)$

But by 1 this implies that $\RR^N$ is spanned by $K &lt; N$ vectors

But then the $N$ canonical basis vectors must be linearly dependent

Contradiction 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Bases}</p>
<p>\begin{frame}
\frametitle{Bases}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $S$ be a linear subspace of $\RR^N$ 

A set of vectors $B := \{\boldb_1, \ldots, \boldb_K\} \subset S$ is
called a \navy{basis of $S$} if
%
\begin{enumerate}
    \item $B$ is linearly independent
    \item $\Span(B) = S$
\end{enumerate}

\vspace{1em}

\Eg Canonical basis vectors form a basis of $\RR^N$

Indeed, if $E := \{\bolde_1, \ldots, \bolde_N\} \subset \RR^N$, then

\begin{itemize}
    \item $E$ is linearly independent -- we showed this earlier
        \vspace{0.3em}
    \item $\Span(E) = \RR^N$ -- we showed this earlier
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    

Recall the plane
%
\begin{equation*}
    P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}
\end{equation*}
%

We showed before that $\Span\{\bolde_1, \bolde_2\} = P$ for 

\begin{equation*}
    \bolde_1 := 
    \left(
    \begin{array}{c}
        1 \\
        0 \\
        0
    \end{array}
    \right),
    \quad 
    \bolde_2 := 
    \left(
    \begin{array}{c}
        0 \\
        1 \\
        0
    \end{array}
    \right)
\end{equation*}
%

Moreover, $\{\bolde_1, \bolde_2\}$ is linearly independent (why?)

\vspace{1em}

Hence $\{\bolde_1, \bolde_2\}$ is a basis for $P$

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{flat_plane_e_vecs.pdf}}
   \end{center}
   \caption{The pair $\{\bolde_1, \bolde_2\}$ form a basis for $P$}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>What are the implications of $B$ being a basis of $S$?

In short, every element of $S$ can be represented uniquely from the
smaller set $B$

\vspace{1em}

In more detail:

\begin{itemize}
    \item $B$ spans $S$ and, by linear independence, every element is
        needed to span $S$ --- a ``minimal&quot; spanning set
        \vspace{0.5em}
    \item Since $B$ spans $S$, every $\boldy$ in $S$ can be represented as
        a linear combination of the basis vectors
        \vspace{0.5em}
    \item By independence, this representation is unique
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>It&#39;s obvious given the definition that

\Fact If $B \subset \RR^N$ is linearly independent, then $B$ is a basis of
$\Span(B)$

\Eg Let $B := \{\boldx_1, \boldx_2\}$ where 

$$
\boldx_1 =
    \begin{pmatrix}
        3 \\
        4 \\
        2
    \end{pmatrix}
\quad \text{and} \quad
\boldx_2 =
    \begin{pmatrix}
        3 \\
        -4 \\
        1
    \end{pmatrix}
$$

We saw earlier that 

\begin{itemize}
    \item $S := \Span(B)$ is the plane in $\RR^3$ passing through
        $\boldx_1$, $\boldx_2$ and $\boldzero$
    \item $B$ is linearly independent in $\RR^3$  (dropping either reduces span)
\end{itemize}

Hence $B$ is a basis for the plane $S$ 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{nonredundant1.pdf}}
       \caption{The pair $\{\boldx_1, \boldx_2\}$ is a basis of its span}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Fundamental Properties of Bases}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $S$ is a linear subspace of $\RR^N$ distinct from $\{\boldzero\}$, then 
    %
    \begin{enumerate}
        \item $S$ has at least one basis, and
        \item every basis of $S$ has the same number of elements
    \end{enumerate}

\vspace{1em}


Proof of part 2: Let $B_i$ be a basis of $S$ with $K_i$ elements, $i=1, 2$

By definition, $B_2$ is a linearly independent subset of $S$
 
Moreover, $S$ is spanned by the set $B_1$, which has $K_1$ elements
 
 Hence $K_2 \leq K_1$ 
 
 Reversing the roles of $B_1$ and $B_2$ gives $K_1 \leq K_2$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Dimension}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $S$ be a linear subspace of $\RR^N$

We now know that every basis of $S$ has the same number of elements

This common number is called the \navy{dimension} of $S$

\vspace{1em}

\Eg $\RR^N$ is $N$ dimensional because the $N$ canonical basis vectors
form a basis

\Eg $P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}$ is
two dimensional because the first two canonical basis vectors
of $\RR^3$ form a basis 

\Eg In $\RR^3$, a line through the origin is one-dimensional, while a
plane through the origin is two-dimensional
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Dimension of Spans}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact  Let $X := \{\boldx_1,\ldots,\boldx_K\} \subset \RR^N$ 

The following statements are true:
%
\begin{enumerate}
    \item $\dimension(\Span(X)) \leq K$
    \item $\dimension(\Span(X)) = K$ $\;\iff\;$ $X$ is linearly independent
\end{enumerate}
%

\vspace{1.5em}

Proof that $\dimension(\Span(X)) \leq K$

If not then $\Span(X)$ has a basis with $M &gt; K$ elements

Hence $\Span(X)$ contains $M &gt; K$ linearly independent vectors

This is impossible, given that $\Span(X)$ is spanned by $K$ vectors
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Now consider the second claim: 

\begin{itemize}
    \item[1.] $X$ is linearly independent $\implies$ $\dim(\Span(X)) = K$  
\end{itemize}

Proof: True because the vectors $\boldx_1,\ldots,\boldx_K$ form
a basis of $\Span(X)$

\vspace{1em}

\begin{itemize}
    \item[2.] $\dimension(\Span(X)) = K$ $\implies$ $X$ linearly independent
\end{itemize}

Proof: If not then $\exists \, X_0 \subsetneq X$ such that $\Span(X_0) = \Span(X)$

By this equality and part 1 of the theorem, 
%
$$\dim(\Span(X)) = \dim(\Span(X_0)) \leq \# X_0 \leq K - 1$$

Contradiction
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $S$ a linear subspace of $\RR^N$, then 
$$
\dim(S) = N \; \iff \; S = \RR^N
$$

\vspace{1em}

Useful implications

\begin{itemize}
    \item The only $N$-dimensional subspace of $\RR^N$ is $\RR^N$
    \item To show $S = \RR^N$ just need to show that $\dim(S) = N$ 
\end{itemize}


\vspace{1em}


Proof: See course notes
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Linear Maps}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>In this section we investigate one of the most important classes of
functions

These are the so-called linear functions

Linear functions play a fundamental role in  all fields of science

\begin{itemize}
    \item In one-to-one correspondence with matrices
\end{itemize}

Even nonlinear functions can often be rewritten as partially linear

The properties of linear functions are closely tied to notions such as 

\begin{itemize}
    \item linear combinations, span
    \item linear independence, bases, etc.
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Linearity}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A function $T \colon \RR^K \to \RR^N$ is called
\navy{linear} if 
%
\begin{equation*}
        T(\alpha \boldx + \beta \boldy) = \alpha T\boldx + \beta T\boldy
        \qquad
        \forall \, 
        \boldx, \boldy \in \RR^K, \;
        \forall \,
        \alpha, \beta \in \RR
\end{equation*}
%

\vspace{1em}
Notation: 

\begin{itemize}
    \item Linear functions often written with upper case letters
        \vspace{0.5em}
    \item Typically omit parenthesis around arguments when convenient
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg    $T \colon \RR \to \RR$ defined by $Tx = 2x$ is linear 

Proof: Take any $\alpha, \beta, x, y$ in $\RR$ and observe that
%
\begin{equation*}
  T(\alpha x + \beta y)
  = 2(\alpha x + \beta y)
  = \alpha 2 x + \beta 2 y
  = \alpha Tx + \beta Ty  
\end{equation*}

\vspace{1em}

    
\Eg The function $f \colon \RR \to \RR$ defined by $f(x) = x^2$ is
\underline{non}linear

Proof: Set $\alpha = \beta = x = y = 1$

Then 
%
\begin{itemize}
    \item $f(\alpha x + \beta y) = f(2) = 4$
    \item But $\alpha f(x) + \beta f(y) = 1 + 1 = 2$
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
    
Given constants $c_1$ and $c_2$, the 
function $T \colon \RR^2 \to \RR$ defined by 
%
\begin{equation*}
    T \boldx = T (x_1, x_2) = c_1 x_1 + c_2 x_2   
\end{equation*}
%
is linear 

\end{example}

\vspace{1em}

Proof: If we take any $\alpha, \beta$ in $\RR$ and
$\boldx, \boldy$ in $\RR^2$, then 
%
\begin{align*}
  T(\alpha \boldx + \beta \boldy)
  &amp; = c_1 [\alpha x_1 + \beta y_1] + c_2 [\alpha x_2 + \beta y_2]
  \\
  &amp; = \alpha [c_1 x_1 + c_2 x_2] + \beta [c_1 y_1 + c_2 y_2]
  \\
  &amp; = \alpha T \boldx + \beta T \boldy  
\end{align*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{linfunc.pdf}}
       \caption{The graph of $T \boldx = c_1 x_1 + c_2 x_2$ is a plane
       through the origin}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Remark: Thinking of linear functions as those whose graph is a straight
line is not correct

\begin{example}
    
Function $f \colon \RR \to \RR$ defined by $f(x) = 1 + 2x$ is
\underline{non}linear

Proof: Take $\alpha = \beta = x = y = 1$

Then 

\begin{itemize}
    \item $f(\alpha x + \beta y) = f(2) = 5$
    \item But $\alpha f(x) + \beta f(y) = 3 + 3 = 6$
\end{itemize}

\end{example}

\vspace{1em}

This kind of function is called an \navy{affine} function
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\bolda_1, \ldots, \bolda_K$ be vectors in $\RR^N$

Let $T \colon \RR^K \to \RR^N$ be defined by 
%
$$
T\boldx 
=
T
\begin{pmatrix}
    x_1 \\
    \vdots \\
    x_K
\end{pmatrix}
=
x_1 \bolda_1 + \ldots + x_K \bolda_K
$$

\Ex Show that this function is linear

Remarks
%
\begin{itemize}
    \item This is a generalization of the previous linear examples 
    \item In a sense it is the most general representation of a linear map
        from $\RR^K$ to $\RR^N$
    \item It is also ``the same&quot; as the $N \times K$ matrix with
        columns $\bolda_1, \ldots, \bolda_K$ --- more on this later
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Implications of Linearity}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact  If $T \colon \RR^K \to \RR^N$ is a linear map and $\boldx_1,
\ldots, \boldx_J$ are vectors in $\RR^K$, then
for any linear combination we have
%
\begin{equation*}
    T
     \left[ \alpha_1 \boldx_1 + \cdots + \alpha_J \boldx_J \right]
    = \alpha_1 T \boldx_1 + \cdots + \alpha_J T \boldx_J
\end{equation*}

Proof for $J=3$:  Applying the def of linearity twice, 
%
\begin{align*}
    T
     \left[ \alpha_1 \boldx_1 + \alpha_2 \boldx_2 + \alpha_3 \boldx_3 \right]
     &amp; = T\left[ (\alpha_1 \boldx_1 + \alpha_2 \boldx_2) + \alpha_3 \boldx_3 \right]
     \\
     &amp; = T\left[ \alpha_1 \boldx_1 + \alpha_2 \boldx_2 \right] + \alpha_3 T \boldx_3 
     \\
     &amp; = \alpha_1 T \boldx_1 + \alpha_2 T \boldx_2  + \alpha_3 T \boldx_3 
\end{align*}

\vspace{1em}

\Ex Show that if $T$ is any linear function then $T\boldzero = \boldzero$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $T \colon \RR^K \to \RR^N$ is a linear map, then 
%
\begin{equation*}
    \range(T) = \Span(V) 
    \quad \text{where} \quad
    V := \{T\bolde_1, \ldots, T\bolde_K\}
\end{equation*}


\begin{itemize}
    \item Here $\bolde_k$ is the $k$-th canonical basis vector in $\RR^K$
\end{itemize}

\vspace{1em}

Proof: Any $\boldx \in \RR^K$ can be expressed as $\sum_{k=1}^K \alpha_k \bolde_k$

Hence $\range(T)$ is the set of all points of the form
%
\begin{equation*}
    T\boldx
    = T \left[ \sum_{k=1}^K \alpha_k \bolde_k \right]
    = \sum_{k=1}^K \alpha_k T \bolde_k 
\end{equation*}
%
as we vary $\alpha_1, \ldots, \alpha_K$ over all combinations

This coincides with the definition of $\Span(V)$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{example}
Let $T \colon \RR^2 \to \RR^2$ be defined by 
%
$$
    T\boldx 
    =
    T(x_1, x_2)
    =
    x_1 
    \begin{pmatrix}
        1 \\
        2
    \end{pmatrix}
    +
    x_2 
    \begin{pmatrix}
        0 \\
        -2
    \end{pmatrix}
$$

Then 
$$
    T\bolde_1
    =
    \begin{pmatrix}
        1 \\
        2
    \end{pmatrix}
    \quad \text{and} \quad
    T\bolde_2
    =
    \begin{pmatrix}
        0 \\
        -2
    \end{pmatrix}
$$

\vspace{1em}

\Ex Show that $V := \{T\bolde_1, T\bolde_2\}$ is linearly independent
    

\vspace{1em}

We conclude that the range of $T$ is all of $\RR^2$  (why?)

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The \navy{null space} or \navy{kernel} of linear map $T \colon \RR^K \to
\RR^N$ is
%
\begin{equation*}
    \kernel(T) := \setntn{\boldx \in \RR^K}{T\boldx = \boldzero}
\end{equation*}
%

\Ex Show that $\kernel(T)$ is a linear subspace of $\RR^K$
        \vspace{0.5em}

        \vspace{0.5em}

\Fact $\kernel(T) = \{\boldzero\}$ if and only if $T$ is one-to-one

        \vspace{0.5em}

Proof of $\implies$: Suppose that $T\boldx = T\boldy$ for arbitrary $\boldx, \boldy \in \RR^K$

Then $\boldzero = T\boldx - T\boldy  = T(\boldx - \boldy)$

In other words, $\boldx - \boldy \in \kernel(T)$

Hence $\kernel(T) = \{\boldzero\}$ $\implies$ $\boldx = \boldy$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\frametitle{Linearity and Bijections}

Many scientific and practical problems are ``inverse&quot; problems

\begin{itemize}
    \item We observe outcomes but not what caused them
    \item How can we work backwards from outcomes to causes?
\end{itemize}

Examples

\begin{itemize}
    \item What consumer preferences generated observed market behavior?
    \item What kinds of expectations led to given shift in exchange rates?
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Loosely, we can express an inverse problem as

\begin{figure}
   \begin{center}
    \scalebox{.5}{\input{inverse_prob.pdf_t}}
   \end{center}
\end{figure}

\begin{itemize}
    \item Does this problem have a solution?
    \item Is it unique?
\end{itemize}

Answers depend on whether $F$ is one-to-one, onto, etc.

The best case is a bijection

But other situations also arise
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Recall that an arbitrary function can be 

\begin{itemize}
    \item one-to-one
    \item onto
    \item both (a bijection)
    \item neither 
\end{itemize}

For linear functions from $\RR^N$ to $\RR^N$, the first three are all
equivalent!

In particular, 
$$
\text{onto } \iff \text{ one-to-one } \iff \text{ bijection}
$$

The next theorem summarizes
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact     If $T$ is a linear function from $\RR^N$ to $\RR^N$ then all of the
following are equivalent:
%
\begin{enumerate}
    \item $T$ is a bijection
    \item $T$ is onto
    \item $T$ is one-to-one
    \item $\kernel(T) = \{ \boldzero \}$
    \item The set of vectors $V := \{T\bolde_1, \ldots, T\bolde_N\}$ is
        linearly independent
\end{enumerate}

If any one of these equivalent conditions is true, then $T$ is called
\navy{nonsingular}

\vspace{1em}

\begin{itemize}
    \item Don&#39;t forget: We are talking about $\RR^N$ to $\RR^N$ here
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{linbijec.pdf}}
    \caption{The case of $N=1$, nonsingular and singular}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Proof that $T$ onto $\iff$ $V := \{T\bolde_1, \ldots, T\bolde_N\}$ is
        linearly independent

\vspace{1em}

Recall that for any linear map $T$ we have $\range(T) =  \Span(V)$

Using this fact and the definitions,
%
\begin{align*}
    T \text{ onto } 
    &amp; \iff \range(T) = \RR^N
    \\
    &amp; \iff \Span(V)  = \RR^N
    \\
    &amp; \iff V \text{ is linearly indepenent}
\end{align*}

(We saw that $N$ vectors span $\RR^N$ iff linearly indepenent)

\vspace{1em}

Rest of proof: Solved exercises
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $T \colon \RR^N \to \RR^N$ is nonsingular then so is $T^{-1}$.  

\vspace{1em}

What is the implication here?

If $T$ is a bijection then so is $T^{-1}$

Hence the only real claim is that $T^{-1}$ is also linear

The proof is an exercise...
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Maps Across Different Dimensions}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Remember that these results apply to maps from $\RR^N$ to $\RR^N$

Things change when we look at linear maps across dimensions

\vspace{1em}

The general rules for linear maps are 

\begin{itemize}
    \item Maps from lower to higher dimensions cannot be onto
    \item Maps from higher to lower dimensions cannot be one-to-one
\end{itemize}

In either case they cannot be bijections

\vspace{1em}

The next fact summarizes
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact For a linear map $T$ from $\RR^K \to \RR^N$, the following statements are
true:
%
\begin{enumerate}
    \item If $K &lt; N$ then $T$ is not onto
    \item If $K &gt; N$ then $T$ is not one-to-one
\end{enumerate}
%
\vspace{1em}

Proof of part 1: Let $K &lt; N$ and let $T \colon \RR^K \to \RR^N$ be linear  

Letting $V := \{T\bolde_1, \ldots, T\bolde_K\}$, we have
%
$$
\dim(\range(T)) = \dim(\Span(V)) \leq K &lt; N
$$
%
$$
\fore 
    \range(T) \not= \RR^N
$$

Hence $T$ is not onto 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Proof of part 2: $K &gt; N$ $\implies$ $T$ is not one-to-one

\vspace{1em}

Suppose to the contrary that $T$ is one-to-one

Let $\alpha_1, \ldots, \alpha_K$ be a collection of vectors such that 
%
$$
\alpha_1 T \bolde_1 + \cdots + \alpha_K T \bolde_K = \boldzero
$$
%
$$
\fore 
T (\alpha_1 \bolde_1 + \cdots + \alpha_K \bolde_K) = \boldzero
\qquad (\text{by linearity})
$$
%
$$
\fore
\alpha_1 \bolde_1 + \cdots + \alpha_K \bolde_K = \boldzero
\qquad (\text{since $\ker(T) = \{\boldzero\}$})
$$
%
$$
\fore 
\alpha_1 = \cdots = \alpha_K = 0
\qquad (\text{by independence of $\{\bolde_1, \ldots \bolde_K\}$)}
$$

We have shown that $\{T\bolde_1, \ldots, T\bolde_K\}$ is linearly
independent

But then $\RR^N$ contains a linearly independent set
with $K &gt; N$ vectors --- contradiction
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{cost_min_2.pdf}}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\Eg Cost function <span class="math notranslate nohighlight">\(c(k, \ell) = rk + w\ell\)</span> cannot be one-to-one</p>
<p>\end{frame}</p>
<p>\section{Matrices}</p>
<p>\begin{frame}
\frametitle{Matrices and Linear Equations}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>We now begin our study of matrices

As we&#39;ll see, there&#39;s an isomorphic relationship between 
%
\begin{enumerate}
    \item matrices 
    \item linear maps
\end{enumerate}


Often properties of matrices are best understood via those of linear maps
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Matrices}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Typical \navy{$N \times K$ matrix}: 

\begin{equation*}
    \boldA = 
    \left(
    \begin{array}{cccc}
        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1K} \\
        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NK} 
    \end{array}
    \right)
\end{equation*}
%

\vspace{2em}

Symbol $a_{nk}$ stands for element in the 
%
\begin{itemize}
\item $n$-th row 
\item $k$-th column
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Often matrices correspond to coefficients of a linear equation

\begin{equation}
    \begin{array}{c}
        a_{11} x_1 + a_{12} x_2 + \cdots + a_{1K} x_K = b_1 \\
        a_{21} x_1 + a_{22} x_2 + \cdots + a_{2K} x_K = b_2 \\
        \vdots  \\
        a_{N1} x_1 + a_{N2} x_2 + \cdots + a_{NK} x_K = b_N 
    \end{array}
\end{equation}

\vspace{1em}

Given the $a_{nm}$ and $b_n$, what values of $x_1, \ldots, x_K$ solve this
system?

\vspace{1em}

We now investigate this and other related questions

But first some background on matrices...
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>An $N \times K$ matrix also called a 
%
\begin{itemize}
    \item \navy{row vector} if $N = 1$
        \vspace{1em}
    \item \navy{column vector} if $K = 1$
\end{itemize}

\vspace{1em}

\Egs

$$
\boldb 
= 
\begin{pmatrix}
    b_1 \\
    \vdots \\
    b_N 
\end{pmatrix}
\text{ is }\; N \times 1,
\qquad
\boldc 
= 
\begin{pmatrix}
    c_1 \cdots c_K 
\end{pmatrix}
\text{ is }\; 
1 \times K
$$

If $N = K$, then $\boldA$ is called \navy{square}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>We use 

\begin{itemize}
    \item $\col_k(\boldA)$ to denote the $k$-th column of $\boldA$
    \item $\row_n(\boldA)$ to denote the $n$-th row of $\boldA$
\end{itemize}

Example

%
\begin{equation*}
    \col_1(\boldA)
    =
    \col_1
    \left(
    \begin{array}{ccc}
        \red{a_{11}} &amp; \cdots &amp; a_{1K} \\
        \red{a_{21}} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp; \vdots \\
        \red{a_{N1}} &amp; \cdots &amp; a_{NK} 
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        a_{11} \\
        a_{21} \\
        \vdots \\
        a_{N1} 
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The \navy{zero matrix} is
%
\begin{equation*}
    \boldzero := 
    \left(
    \begin{array}{cccc}
        0 &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; 0 &amp; \cdots &amp; 0 \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 0 \\
    \end{array}
    \right)
\end{equation*}
%

The \navy{identity matrix} is
%
\begin{equation*}
    \boldI := 
    \left(
    \begin{array}{cccc}
        1 &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; 1 &amp; \cdots &amp; 0 \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; 1 \\
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Algebraic Operations for Matrices}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Addition and scalar multiplication are also defined for matrices
</pre></div>
</div>
<p>Both are element by element, as in the vector case</p>
<p>Scalar multiplication:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{equation*}
    \gamma 
    \left(
    \begin{array}{cccc}
        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1K} \\
        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NK} \\
    \end{array}
    \right)
    :=
    \left(
    \begin{array}{cccc}
        \gamma a_{11} &amp; \gamma a_{12} &amp; \cdots &amp; \gamma a_{1K} \\
        \gamma a_{21} &amp; \gamma a_{22} &amp; \cdots &amp; \gamma a_{2K} \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        \gamma a_{N1} &amp; \gamma a_{N2} &amp; \cdots &amp; \gamma a_{NK} \\
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Addition:
% 
\begin{multline*}
    \left(
    \begin{array}{ccc}
        a_{11} &amp; \cdots &amp; a_{1K} \\
        a_{21} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp; \vdots \\
        a_{N1} &amp; \cdots &amp; a_{NK} \\
    \end{array}
    \right)
    +
    \left(
    \begin{array}{ccc}
        b_{11} &amp; \cdots &amp; b_{1K} \\
        b_{21} &amp; \cdots &amp; b_{2K} \\
        \vdots &amp; \vdots &amp; \vdots \\
        b_{N1} &amp; \cdots &amp; b_{NK} \\
    \end{array}
    \right)
    \\
    :=
    \left(
    \begin{array}{ccc}
        a_{11} + b_{11} &amp;  \cdots &amp; a_{1K} + b_{1K} \\
        a_{21} + b_{21} &amp;  \cdots &amp; a_{2K} + b_{2K} \\
        \vdots &amp; \vdots &amp; \vdots \\
        a_{N1} + b_{N1} &amp;  \cdots &amp; a_{NK} + b_{NK} \\
    \end{array}
    \right)
\end{multline*}
%

Note that matrices must be same dimension
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Multiplication of matrices:  

Product $\boldA \boldB$: 
$i,j$-th element is inner product of $i$-th row of $\boldA$ and 
$j$-th column of $\boldB$  

%
\begin{equation*}
    \left(
    \begin{array}{ccc}
        \red{a_{11}} &amp; \red{\cdots} &amp; \red{a_{1K}} \\
        a_{21} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp; \vdots \\
        a_{N1} &amp; \cdots &amp; a_{NK} \\
    \end{array}
    \right)
    \left(
    \begin{array}{ccc}
        \red{b_{11}} &amp; \cdots &amp; b_{1J} \\
        \red{b_{21}} &amp; \cdots &amp; b_{2J} \\
        \red{\vdots} &amp; \vdots &amp; \vdots \\
        \red{b_{K1}} &amp; \cdots &amp; b_{KJ} \\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{ccc}
        \red{c_{11}} &amp; \cdots &amp; c_{1J} \\
        c_{21} &amp; \cdots &amp; c_{2J} \\
        \vdots &amp; \vdots &amp; \vdots \\
        c_{N1} &amp; \cdots &amp; c_{NJ} \\
    \end{array}
    \right)
\end{equation*}
%

In this display, 
%
\begin{equation*}
    c_{11} = \sum_{k=1}^K a_{1k} b_{k1}
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Suppose $\boldA$ is $N \times K$ and $\boldB$ is $J \times M$

\begin{itemize}
    \item $\boldA \boldB$ defined only if $K = J$
    \item Resulting matrix $\boldA \boldB$ is $N \times M$
\end{itemize}


The rule to remember:
%
\begin{equation*}
    \text{product of } N \times K \text{ and } K \times M
    \text{ is }  N \times M
\end{equation*}
%

Important: Multiplication is not commutative

In particular, it is not in general true that $\boldA \boldB = \boldB \boldA$ 

\begin{itemize}
    \item In fact $\boldB \boldA$ is not well-defined unless $N = M$ also holds 
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Useful observation:

\begin{align*}
    \boldA \boldx
    &amp; = 
    \left(
    \begin{array}{cccc}
        a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1K} \\
        a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2K} \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        a_{N1} &amp; a_{N2} &amp; \cdots &amp; a_{NK} 
    \end{array}
    \right)
    \left(
    \begin{array}{c}
        x_1 \\
        x_2 \\
        \vdots \\
        x_K
    \end{array}
    \right)
    \\
    &amp; =
    x_1 \left(
    \begin{array}{c}
        a_{11} \\
        a_{21} \\
        \vdots \\
        a_{N1} 
    \end{array}
    \right)
    +
    x_2 \left(
    \begin{array}{c}
        a_{12} \\
        a_{22} \\
        \vdots \\
        a_{N2} 
    \end{array}
    \right)
    + \cdots + 
    x_K \left(
    \begin{array}{c}
        a_{1K} \\
        a_{2K} \\
        \vdots \\
        a_{NK} 
    \end{array}
    \right)
    \\
    &amp; = 
    \sum_{k=1}^K x_k \col_k(\boldA)
\end{align*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Rules for multiplication:

\Fact Given scalar $\alpha$ and conformable $\boldA$, $\boldB$ and $\boldC$, we have

\begin{enumerate}
    \item $\boldA (\boldB \boldC) = (\boldA \boldB) \boldC$
        \vspace{0.4em}
    \item $\boldA (\boldB + \boldC) = \boldA \boldB + \boldA \boldC$
        \vspace{0.4em}
    \item $(\boldA + \boldB) \boldC = \boldA \boldC + \boldB \boldC$
        \vspace{0.4em}
    \item $\boldA \alpha \boldB = \alpha \boldA \boldB$
\end{enumerate}

\vspace{0.4em}

(Here ``conformable&#39;&#39; means operation makes sense)
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The \navy{$k$-th power} of a square matrix $\boldA$ is 

$$ \boldA^k := \underbrace{\boldA \cdots \boldA}_{k \text{ terms}} $$

\vspace{1em}

If it exists, the \navy{square root} of $\boldA$ is written $\boldA^{1/2}$ 

Defined as the matrix $\boldB$ such that $\boldB^2$ is $\boldA$

\vspace{1em}

More on these later...
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>In matrix multiplication, $\boldI$ is the multiplicative unit

That is,  assuming conformability, we always have
%
\begin{equation*}
    \boldA \boldI = \boldI \boldA = \boldA
\end{equation*}

\Ex Check it using the definition of matrix multiplication

\vspace{1em}

Note: If $\boldI$ is $K \times K$, then
$$
\col_k(\boldI)
= \bolde_k
= \text{ $k$-th canonical basis vector in } \RR^K
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: A = [[2, 4],
…:      [4, 2]]</p>
<p>In [3]: A = np.array(A)  # Convert A to array</p>
<p>In [4]: B = np.identity(2)</p>
<p>In [5]: B
Out[5]:
array([[ 1.,  0.],
[ 0.,  1.]])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [6]: A + B        # Matrix addition
Out[6]:
array([[ 3.,  4.],
[ 4.,  3.]])</p>
<p>In [7]: np.dot(A, B)  # Matrix multiplication
Out[7]:
array([[ 2.,  4.],
[ 4.,  2.]])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\section{Matrices as Maps}</p>
<p>\begin{frame}
\frametitle{Matrices as Maps}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Any $N \times K$ matrix $\boldA$ can be thought of as a function
$\boldx \mapsto \boldA \boldx$
%
\begin{itemize}
    \item In $\boldA \boldx$ the $\boldx$ is understood to be
        a column vector
\end{itemize}

It turns out that every such map is linear

To see this fix $N \times K$ matrix $\boldA$ and let $T$ be defined by
\begin{equation*}
    T \colon \RR^K \to \RR^N, 
    \qquad
    T\boldx = \boldA \boldx
\end{equation*}

Pick any $\boldx$, $\boldy$ in $\RR^K$, and any scalars $\alpha$ and $\beta$
 
 The rules of matrix arithmetic tell us that
%
\begin{equation*}
    T(\alpha \boldx + \beta \boldy) 
    := \boldA (\alpha \boldx + \beta \boldy)
    = \alpha \boldA \boldx  + \beta \boldA \boldy
    =: \alpha T\boldx + \beta T\boldy 
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>So matrices make linear functions

How about examples of linear functions that don&#39;t involve matrices?  

Actually there are none!

\vspace{1em}

\Fact If $T \colon \RR^K \to \RR^N$ then 
%
$$
T \text{ is linear }
\; \iff \;
\exists \; N \times K \text{ matrix } \boldA  \st T\boldx = \boldA \boldx, 
\;\forall \, \boldx \in \RR^K
$$
%

\vspace{1em}

\begin{itemize}
    \item On the last slide we showed the $\Longleftarrow$ part
        \vspace{1em}
    \item On the next slide we show the $\implies$ part
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $T \colon \RR^K \to \RR^N$ be linear

We aim to construct an $N \times K$ matrix $\boldA$ such that
%
$$ T\boldx = \boldA \boldx, \qquad \forall \, \boldx \in \RR^K $$

As usual, let
$\bolde_k$ be the $k$-th canonical basis vector in $\RR^K$

Define a matrix $\boldA$ by $\col_k(\boldA) = T\bolde_k$

Pick any $\boldx = (x_1, \ldots, x_K) \in \RR^K$

By linearity we have 
%
\begin{equation*}
    T\boldx 
    = T \left[\sum_{k=1}^K x_k \bolde_k \right]
    = \sum_{k=1}^K x_k T \bolde_k
    = \sum_{k=1}^K x_k \col_k(\boldA)
    = \boldA \boldx
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Matrix Product as Composition}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let

\begin{itemize}
    \item $\boldA$ be $N \times K$ and $\boldB$ be $K \times M$
    \item $T \colon \RR^K \to \RR^N$ be the linear map $T\boldx = \boldA\boldx$ 
    \item $U \colon \RR^M \to \RR^K$ be the linear map $U\boldx = \boldB\boldx$ 
\end{itemize}

The matrix product $\boldA \boldB$ corresponds exactly to the
\underline{composition} of $T$ and $U$ 

Proof:
$$
(T \circ U) (\boldx)
= T( U\boldx)
= T( \boldB \boldx)
= \boldA \boldB \boldx
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This helps us understand a few things

For example, let
%
\begin{itemize}
    \item $\boldA$ be $N \times K$ and $\boldB$ be $J \times M$
    \item $T \colon \RR^K \to \RR^N$ be the linear map $T\boldx = \boldA\boldx$ 
    \item $U \colon \RR^M \to \RR^J$ be the linear map $U\boldx = \boldB\boldx$ 
\end{itemize}

Then $\boldA \boldB$ is only defined when $K = J$

This is because $\boldA \boldB$ corresponds to $T \circ U$

But for $T \circ U$ to be well defined we need $K = J$

Then $U$ maps $\RR^M$ to $\RR^K$ and $T$ maps $\RR^K$ to $\RR^N$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Rank}</p>
<p>\begin{frame}
\frametitle{Column Space}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\boldA$ be an $N \times K$ matrix

The \navy{column space} of $\boldA$ is defined as the span of its columns
%
\begin{align*}
\Span(\boldA) 
    &amp; = \Span \{ \col_1 (\boldA), \ldots, \col_K(\boldA) \}
    \\
    &amp; = \text{all vectors of the form } \sum_{k=1}^K x_k \col_k(\boldA)
\end{align*}

Equivalently,
%
\begin{equation*}
    \Span(\boldA) := \setntn{\boldA \boldx}{ \boldx \in \RR^K}
\end{equation*}
%

This is exactly the range of the associated linear map
%
\begin{center}
$T \colon \RR^K \to \RR^N$ defined by $T \boldx = \boldA \boldx$
\end{center}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg If
%
$$
\boldA =
\begin{pmatrix}
   1 &amp; -5 \\
   2 &amp; 3
\end{pmatrix}
$$
%

then the span is all linear combinations
%
\begin{equation*}
    x_1
    \left(
    \begin{array}{c}
        1 \\
        2
    \end{array}
    \right)
    + 
    x_2
    \left(
    \begin{array}{c}
        -5 \\
        3
    \end{array}
    \right)
    \quad
    \text{where}
    \quad
    (x_1, x_2) \in \RR^2
\end{equation*}
%

These columns are linearly independent (shown earlier)

Hence the column space is all of $\RR^2$ (why?)

\vspace{1em}

\Ex Show that the column space of any $N \times K$ matrix is a linear
subspace of $\RR^N$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Rank}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Equivalent questions
%
\begin{itemize}
    \item How large is the range of the linear map $T \boldx = \boldA \boldx$?
    \item How large is the column space of $\boldA$?
\end{itemize}

\vspace{1em}

The obvious measure of size for a linear subspace is its dimension

The dimension of $\Span(\boldA)$ is known as the \navy{rank} of $\boldA$  
%
\begin{equation*}
    \rank(\boldA) := \dimension(\Span(\boldA))
\end{equation*}
%

Because $\Span(\boldA)$ is the span of $K$ vectors, we have
%
\begin{equation*}
    \rank(\boldA) = \dimension(\Span(\boldA)) \leq K
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$\boldA$ is said to have \navy{full column rank} if 
%
\begin{equation*}
    \rank(\boldA) = \text{ number of columns of } \boldA
\end{equation*}


\Fact For any matrix $\boldA$,  the following statements are equivalent:
%
\begin{enumerate}
    \item $\boldA$ is of full column rank
        \vspace{0.3em} 
    \item The columns of $\boldA$ are linearly independent
        \vspace{0.3em} 
    \item If $\boldA \boldx = \boldzero$, then $\boldx = \boldzero$
\end{enumerate}
%

\vspace{1em}

\Ex Check this, recalling that
%
$$
\dim(\Span\{\bolda_1, \ldots, \bolda_K\}) = K
\, \iff \,
\{\bolda_1, \ldots, \bolda_K\} \text{ linearly indepenent}
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: A = [[2.0, 1.0],
…:      [6.3, 3.0]]</p>
<p>In [3]: np.linalg.matrix_rank(A)
Out[3]: 2</p>
<p>In [4]: A = [[2.0, 1.0],  # Col 2 is half col 1
…:      [6.0, 3.0]]</p>
<p>In [5]: np.linalg.matrix_rank(A)
Out[5]: 1
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Reminder I}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Suppose we want to find the $x$ that solves $f(x) = y$

The ideal case is when $f$ is a bijection

\begin{figure}
   \begin{center}
       \scalebox{0.6}{\input{../tikzfigs/function2.tex}}
   \end{center}
\end{figure}

\vspace{-1em}

Equivalent:
%
\begin{itemize}
    \item $f$ is a bijection
    \item each $y \in B$ has a unique preimage
    \item $f(x) = y$ has a unique solution $x$ for each $y$
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Reminder II}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $T$ be a linear function from $\RR^N$ to $\RR^N$ 

\vspace{1em}

We saw that in this case all of the following are equivalent:
%
\begin{enumerate}
    \item $T$ is a bijection
    \item $T$ is onto
    \item $T$ is one-to-one
    \item $\kernel(T) = \{ \boldzero \}$
    \item $V := \{T\bolde_1, \ldots, T\bolde_N\}$ is linearly independent
\end{enumerate}

\vspace{1em}

We then say that $T$ is nonsingular ($=$ linear bijection)
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{<span class="math notranslate nohighlight">\(N \times N\)</span> Linear Equations}</p>
<p>\begin{frame}
\frametitle{Linear Equations}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let&#39;s look at solving linear equations such as $\boldA \boldx = \boldb$ 

We start with the ``best&quot; case: 

\begin{center}
number of equations $=$ number of unknowns
\end{center}

Thus, 

\begin{itemize}
    \item Take $N \times N$ matrix $\boldA$ and $N \times 1$ vector $\boldb$ as given 
    \item Search for an $N \times 1$ solution $\boldx$
\end{itemize}


But does such a solution exist?  If so is it unique?
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The best way to think about this is to consider 
the corresponding linear map
%
$$
T \colon \RR^N \to \RR^N,
\qquad T\boldx = \boldA \boldx
$$

\begin{figure}
   \begin{center}
       \scalebox{0.6}{\input{../tikzfigs/linbijec.tex}}
   \end{center}
\end{figure}


Equivalent:
%
\begin{enumerate}
    \item $\boldA \boldx = \boldb$ has a unique solution $\boldx$ for any
        given $\boldb$
    \item $T \boldx = \boldb$ has a unique solution $\boldx$ for any
        given $\boldb$
    \item $T$ is a bijection
\end{enumerate}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>We already have conditions for linear maps to be bijections

Just need to translate these into the matrix setting

\vspace{1em}

Recall that $T$ called nonsingular if $T$ is a linear bijection

We say that $\boldA$ is \navy{nonsingular} if $T$ is nonsingular

Equivalent:

\begin{itemize}
    \item $\boldx \mapsto \boldA \boldx$ is a bijection from $\RR^N$ to $\RR^N$
\end{itemize}

\vspace{1em}

We now list equivalent conditions for nonsingularity
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\boldA$ be an $N \times N$ matrix

\Fact All of the following conditions are equivalent
%
\begin{enumerate}
    \item $\boldA$ is nonsingular
        \vspace{0.3em}
    \item The columns of $\boldA$ are linearly independent
        \vspace{0.3em}
    \item $\rank(\boldA) = N$
        \vspace{0.3em}
    \item $\Span(\boldA) = \RR^N$
        \vspace{0.3em}
    \item If $\boldA \boldx = \boldA \boldy$, then $\boldx = \boldy$
        \vspace{0.3em}
    \item If $\boldA \boldx = \boldzero$, then $\boldx = \boldzero$
        \vspace{0.3em}
    \item For each $\boldb \in \RR^N$, the equation $\boldA \boldx = \boldb$
        has a solution
        \vspace{0.3em}
    \item For each $\boldb \in \RR^N$, the equation $\boldA \boldx = \boldb$
        has a unique solution
\end{enumerate}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>All equivalent ways of saying that $T\boldx = \boldA \boldx$ is a bijection!

\vspace{1em} 

\Eg For condition 5 the equivalence is
% 
\begin{align*}
    \text{ if $\boldA \boldx = \boldA \boldy$,} &amp; \text{ then $\boldx = \boldy$ }
     \\
     &amp; \iff \text{ if $T \boldx = T \boldy$, then $\boldx = \boldy$ }
     \\
     &amp;\iff  \text{ $T$ is one-to-one }
     \\
</pre></div>
</div>
<p>\text{Since <span class="math notranslate nohighlight">\(T\)</span> is a linear map from} &amp; \text{ <span class="math notranslate nohighlight">\(\RR^N\)</span> to <span class="math notranslate nohighlight">\(\RR^N\)</span>, }
\
&amp;\iff \text{ <span class="math notranslate nohighlight">\(T\)</span> is a bijection }
\end{align*}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg For condition 6 the equivalence is
% 
\begin{align*}
    \text{ if $\boldA \boldx = \boldzero$,} &amp; \text{ then $\boldx = \boldzero$ }
     \\
     &amp;\iff \setntn{\boldx}{\boldA \boldx = \boldzero} =\{ \boldzero \}
     \\
     &amp;\iff \setntn{\boldx}{T \boldx = \boldzero} =\{ \boldzero \}
     \\
     &amp;\iff \ker(T) = \{ \boldzero \} 
     \\
</pre></div>
</div>
<p>\text{Since <span class="math notranslate nohighlight">\(T\)</span> is a linear map from} &amp; \text{ <span class="math notranslate nohighlight">\(\RR^N\)</span> to <span class="math notranslate nohighlight">\(\RR^N\)</span>, }
\
&amp;\iff \text{ <span class="math notranslate nohighlight">\(T\)</span> is a bijection }
\end{align*}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg For condition 7 the equivalence is 

\begin{align*}
    \text{for each $\boldb \in \RR^N$, } &amp; \text{the equation $\boldA \boldx = \boldb$
        has a solution}
     \\
     &amp;\iff \text{ every $\boldb \in \RR^N$ has an $\boldx$ such that $\boldA\boldx = \boldb$ }
     \\
     &amp;\iff \text{ every $\boldb \in \RR^N$ has an $\boldx$ such that $T\boldx = \boldb$ }
     \\
     &amp;\iff \text{ $T$ is onto}
     \\
</pre></div>
</div>
<p>\text{Since <span class="math notranslate nohighlight">\(T\)</span> is a linear } &amp; \text{map from <span class="math notranslate nohighlight">\(\RR^N\)</span> to <span class="math notranslate nohighlight">\(\RR^N\)</span>, }
\
&amp;\iff \text{ <span class="math notranslate nohighlight">\(T\)</span> is a bijection }
\end{align*}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Now consider condition 2:

\begin{center}
    The columns of $\boldA$ are linearly independent
\end{center}

Let $\bolde_n$ be the $n$-th canonical basis vector in $\RR^N$

Observe that $\boldA \bolde_n = \col_n (\boldA)$
%
\vspace{0.5em}
$$
\fore T\bolde_n = \col_n (\boldA)
$$
\vspace{-0.5em}
$$
\fore 
V := \{T \bolde_1, \ldots, T\bolde_N\}
= \text{ columns of $\boldA$}
$$

And $V$ is linearly independent if and only if $T$ is a bijection
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Consider a one good linear market system 
%
\begin{align*}
    q &amp; = a - b p \qquad (\text{demand}) \\
    q &amp; = c + d p  \qquad (\text{supply})
\end{align*}

Treating $q$ and $p$ as the unknowns, let&#39;s write in matrix form as 
%
$$
\begin{pmatrix}
    1 &amp; b \\
    1 &amp; -d
\end{pmatrix}
\begin{pmatrix}
    q\\
    p
\end{pmatrix}
=
\begin{pmatrix}
    a \\
    c 
\end{pmatrix}
$$

A unique solution exists whenever the columns are linearly independent

\begin{itemize}
    \item means that $(b, -d)$ is not a scalar multiple of $\boldone$
    \item means that $b \not= -d$ 
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{not_multiple_of_one.pdf}}
    \caption{$(b, -d)$ is not a scalar multiple of $\boldone$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Recall when we try to solve the system $\boldA \boldx = \boldb$
of this form
</pre></div>
</div>
<p>\begin{pythoncode}
In [1]: import numpy as np
In [2]: from scipy.linalg import solve</p>
<p>In [3]: A = [[0, 2, 4],
…:      [1, 4, 8],
…:      [0, 3, 6]]</p>
<p>In [4]: b = (1, 2, 0)</p>
<p>In [5]: A, b = np.asarray(A), np.asarray(b)</p>
<p>In [6]: solve(A, b)
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This is the output that we got

\begin{verbatim}
</pre></div>
</div>
<p>LinAlgError        Traceback (most recent call last)
<ipython-input-8-4fb5f41eaf7c> in <module>()
—-&gt; 1 solve(A, b)
/home/john/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.pyc in solve(a, b, sym_pos, lower, overwrite_a, overwrite_b, debug, check_finite)
97         return x
98     if info &gt; 0:
—&gt; 99         raise LinAlgError(“singular matrix”)
100     raise ValueError(‘illegal value in %d-th argument of internal gesv|posv’
LinAlgError: singular matrix
\end{verbatim}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The problem is that $\boldA$ is singular (not nonsingular)

\begin{itemize}
    \item In particular, $\col_3(\boldA) = 2 \col_2(\boldA)$
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Inverse Matrices}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Given square matrix $\boldA$, suppose $\exists$ square matrix $\boldB$ such
that 
$$\boldA \boldB = \boldB \boldA = \boldI$$  
%
Then
%
\begin{itemize}
    \item $\boldB$ is called the \navy{inverse} of $\boldA$, and written $\boldA^{-1}$
    \item $\boldA$ is called \navy{invertible}
\end{itemize}

        \vspace{0.3em} 

\vspace{1em}

\Fact A square matrix $\boldA$ is nonsingular if and only if it is
invertible

Remark
%
\begin{itemize}
    \item $\boldA^{-1}$ is just the matrix corresponding to the linear map $T^{-1}$
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact Given nonsingular $N \times N$ matrix $\boldA$ and $\boldb \in \RR^N$, the unique
solution to $\boldA \boldx = \boldb$ is given by 
%
$$    \boldx_b := \boldA^{-1} \boldb $$
%

\vspace{0.4em}

Proof: Since $\boldA$ is nonsingular we already know any solution is
unique

\begin{itemize}
    \item $T$ is a bijection, and hence one-to-one
    \item if $\boldA \boldx = \boldA \boldy = \boldb$ then $\boldx = \boldy$
\end{itemize}

To show that $\boldx_b$ is indeed a solution we need to show that 
    $\boldA \boldx_b = \boldb$ 

To see this, observe that
%
$$
\boldA \boldx_b  = \boldA \boldA^{-1} \boldb = \boldI \boldb = \boldb
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Recall the one good linear market system 
%
$$
\begin{array}{c}
    q = a - b p  \\
    q = c + d p
\end{array}
\quad \iff \quad
\begin{pmatrix}
    1 &amp; b \\
    1 &amp; -d
\end{pmatrix}
\begin{pmatrix}
    q\\
    p
\end{pmatrix}
=
\begin{pmatrix}
    a \\
    c 
\end{pmatrix}
$$

Suppose that $a=5$, $b=2$, $c=1$, $d=1.5$

The matrix system is $\boldA \boldx = \boldb$ where
%
$$
\boldA :=
\begin{pmatrix}
    1 &amp; 2 \\
    1 &amp; -1.5
\end{pmatrix},
\;
\boldx :=
\begin{pmatrix}
    q\\
    p
\end{pmatrix},
\;
\boldb :=
\begin{pmatrix}
    5 \\
    1 
\end{pmatrix}
$$

Since $b \not= -d$ we can solve for the unique solution

Easy by hand but let&#39;s try on the computer
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np
In [2]: from scipy.linalg import inv</p>
<p>In [3]: A = [[1, 2],
…:     [1, -1.5]]</p>
<p>In [4]: b = [5, 1]</p>
<p>In [5]: q, p = np.dot(inv(A), b)  # A^{-1} b</p>
<p>In [6]: q
Out[6]: 2.7142857142857144
In [7]: p
Out[7]: 1.1428571428571428
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{simple_mkt.pdf}}
    \caption{Equilibrium $(p^*, q^*)$ in the one good case}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact In the $2 \times 2$ case, the inverse has the form

$$
    \left(
    \begin{array}{cc}
        a &amp; b  \\
        c &amp; d  \\
    \end{array}
    \right)^{-1} = 
    \frac{1}{ad - bc}
    \left(
    \begin{array}{cc}
        d &amp; -b  \\
        -c &amp; a  \\
    \end{array}
    \right)
$$
%

\vspace{1em}

\Eg 

$$
\boldA = 
    \left(
    \begin{array}{cc}
        1 &amp; 2  \\
        1 &amp; -1.5  \\
    \end{array}
    \right)
    \quad \implies \quad
    \boldA^{-1} = 
    \frac{1}{-3.5}
    \left(
    \begin{array}{cc}
        -1.5 &amp; -2  \\
        -1 &amp; 1  \\
    \end{array}
    \right)
$$
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Consider the $N$ good linear demand system
%
\begin{equation}
    \label{eq:ds}
    q_n = \sum_{k=1}^N a_{nk} p_k + b_n,
    \quad n = 1, \ldots N
\end{equation}

Task: take quantities $q_1, \ldots, q_N$ as given and find
corresponding prices $p_1, \ldots, p_N$ --- the ``inverse demand curves&quot;

We can write \eqref{eq:ds} as 
%
$$
\boldq = \boldA \boldp + \boldb
$$
where vectors are $N$-vectors and $\boldA$ is $N \times N$

If the columns of $\boldA$ are linearly independent then a unique solution
exists for each fixed $\boldq$ and $\boldb$, and is given by
%
$$
\boldp = \boldA^{-1} (\boldq - \boldb)
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Left and Right Inverses}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\vspace{0.5em}

Given square matrix $\boldA$, a matrix $\boldB$ is called 
% 
\begin{itemize}
    \item a \navy{left inverse} of $\boldA$ if $\boldB \boldA = \boldI$
    \item a \navy{right inverse} of $\boldA$ if $\boldA \boldB = \boldI$
\end{itemize}

By definition, a matrix that is both an left inverse and a right inverse is an inverse

\vspace{0.5em}

\Fact  If square matrix $\boldB$ is either a left or right inverse for
$\boldA$, then $\boldA$ is nonsingular and $\boldA^{-1} = \boldB$

\vspace{1em}

In other words, for square matrices,
%
\vspace{-0.4em}
\begin{center}
    left inverse $\iff$ right inverse $\iff$ inverse
\end{center}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Rules for Inverses}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $\boldA$ is nonsingular and $\alpha \not= 0$, then
%
\begin{enumerate}
    \item $\boldA^{-1}$ is nonsingular and $(\boldA^{-1})^{-1} = \boldA$
    \item $\alpha \boldA$ is nonsingular and $(\alpha \boldA)^{-1} = \alpha^{-1} \boldA^{-1}$
\end{enumerate}

\vspace{1em}

Proof of part 2: 

It suffices to show that $\alpha^{-1} \boldA^{-1}$ is the right inverse of 
$\alpha \boldA$

\vspace{1em}

This is true because
%
$$
    \alpha \boldA \alpha^{-1} \boldA^{-1} 
    =
    \alpha \alpha^{-1} \boldA \boldA^{-1} 
    = \boldI
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $\boldA$ and $\boldB$ are $N \times N$ and nonsingular then
%
\begin{enumerate}
    \item $\boldA \boldB$ is also nonsingular
        \vspace{0.5em}
    \item $(\boldA \boldB)^{-1} = \boldB^{-1} \boldA^{-1}$
\end{enumerate}

Proof I: Let $T$ and $U$ be the linear maps corresponding to $\boldA$ and
$\boldB$

Recall that

\begin{itemize}
    \item $T \circ U$ is the linear map corresponding to $\boldA \boldB$
    \item Compositions of linear maps are linear
    \item Compositions of bijections are bijections
\end{itemize}

Hence $T \circ U$ is a linear bijection with $(T \circ U)^{-1} = U^{-1} \circ T^{-1}$

That is, $\boldA\boldB$ is nonsingular with inverse $\boldB^{-1} \boldA^{-1}$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Proof II: 

A different proof that $\boldA \boldB$ is nonsingular with inverse 
$\boldB^{-1} \boldA^{-1}$

Suffices to show that $\boldB^{-1} \boldA^{-1}$ is the right inverse of $\boldA \boldB$

To see this, observe that 
%
$$
    \boldA \boldB \boldB^{-1} \boldA^{-1}
    = \boldA  \boldA^{-1}
    = \boldI
$$

Hence $\boldB^{-1} \boldA^{-1}$ is a right inverse as claimed
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{The Singular Case}</p>
<p>\begin{frame}
\frametitle{When the Conditions Fail}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Suppose as before we have 
%
\begin{itemize}
    \item an $N \times N$ matrix $\boldA$ 
    \item an $N \times 1$ vector $\boldb$ 
\end{itemize}

\vspace{0.3em}

We seek a solution $\boldx$ to the equation $\boldA \boldx = \boldb$

\vspace{0.3em}

What if $\boldA$ is \underline{singular}?

\vspace{0.3em}

Then $T\boldx = \boldA \boldx$ is not a bijection, and in fact
%
\begin{itemize}
    \item $T$ cannot be onto (otherwise it&#39;s a bijection)
    \vspace{0.3em}
    \item $T$ cannot be one-to-one (otherwise it&#39;s a bijection)
\end{itemize}

    \vspace{0.3em}

Hence neither existence nor uniqueness is guaranteed
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg The matrix $\boldA$ with columns
%
$$
\bolda_1 :=
    \begin{pmatrix}
        3 \\
        4 \\
        2
    \end{pmatrix},
    \quad
\bolda_2 :=
    \begin{pmatrix}
        3 \\
        -4 \\
        1
    \end{pmatrix}
\quad \text{and} \quad
\bolda_3 :=
    \begin{pmatrix}
        -3 \\
        4 \\
        -1
    \end{pmatrix}
$$
%
is singular ($\bolda_3 = - \bolda_2$)

\vspace{1em}

Its column space $\Span(\boldA)$ is just a plane in $\RR^2$

Recall $\boldb \in \Span(\boldA)$ 
%
\begin{itemize}
    \item[] $\iff$ $\exists \, x_1, \ldots, x_N$ such that $\sum_{k=1}^N
        x_k \col_k(\boldA) = \boldb$
        \vspace{0.5em}
    \item[] $\iff$ $\exists \, \boldx$ such that $\boldA \boldx = \boldb$
\end{itemize}


\vspace{1em}

Thus if $\boldb$ is not in this plane then $\boldA \boldx = \boldb$ has no
solution
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
       \scalebox{.4}{\includegraphics{not_in_span.pdf}}
       \caption{The vector $\boldb$ is not in $\Span(\boldA)$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>When $\boldA$ is $N \times N$ and singular how rare is scenario $\boldb \in
\Span(\boldA)$?

Answer: In a sense, very rare

We know that $\dim(\Span(\boldA)) &lt; N$

Such sets are always ``very small&quot; subset of $\RR^N$ in terms of ``volume&quot;

\begin{itemize}
    \item A $K &lt; N$ dimensional subspace has ``measure zero&quot; in $\RR^N$
    \item A ``randomly chosen&quot; $\boldb$ has zero probability of being in such
        a set
\end{itemize}

\Eg Consider the case where $N = 3$ and $K=2$

A two-dimensional linear subspace is a 2D plane in $\RR^3$

This set has no volume because planes have no ``thickness&#39;&#39;
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>All this means that if $\boldA$ is singular then existence
of a solution to $\boldA \boldx = \boldb$ typically fails

In fact the problem is worse --- uniqueness fails as well

\vspace{1em}

\Fact If $\boldA$ is a singular matrix and $\boldA \boldx = \boldb$ has a
solution then it has an infinity (in fact a continuum) of solutions

Proof: Let $\boldA$ be singular and let $\boldx$ be a solution

Since $\boldA$ is singular there exists a nonzero $\boldy$ with $\boldA
\boldy = \boldzero$

But then $\alpha \boldy + \boldx$ is also a solution for any $\alpha \in
\RR$ because
%
$$ 
    \boldA(\alpha \boldy + \boldx) 
    = \alpha \boldA \boldy + \boldA \boldx
    = \boldA \boldx = \boldb
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Determinants}</p>
<p>\begin{frame}
\frametitle{Determinants}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $S(N)$ be set of all bijections from $\{1, \ldots, N\}$ to itself

For $\pi \in S(N)$ we define the \navy{signature} of $\pi$ as 
%
\begin{equation*}
    \sgn(\pi) := \prod_{m &lt; n} \frac{\pi(m) - \pi(n)}{m - n}
\end{equation*}
%

The \navy{determinant} of $N \times N$ matrix $\boldA$ is then given as
%
\begin{equation*}
    \det(\boldA) 
    := \sum_{\pi \in S(N)} \sgn(\pi) \prod_{n=1}^N a_{\pi(n) n}
\end{equation*}

\begin{itemize}
    \item You don&#39;t need to understand or remember this for our course
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact In the $N = 2$ case this definition reduces to

$$
\det 
    \left(
    \begin{array}{cc}
        a &amp; b  \\
        c &amp; d  \\
    \end{array}
    \right)
    = ad - bc
$$
%

\vspace{1em}

\begin{itemize}
    \item Remark: But you do need to remember this $2 \times 2$ case
\end{itemize}

\vspace{1em}

\begin{example}
    
%
\begin{equation*}
    \det 
    \left(
    \begin{array}{cc}
        2 &amp; 0  \\
        7 &amp; -1  \\
    \end{array}
    \right)
    = (2 \times -1) - (7 \times 0) = -2
\end{equation*}
%

\end{example}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Important facts concerning the determinant

\vspace{1em}

\Fact If $\boldI$ is the $N \times N$ identity, $\boldA$ and $\boldB$ are $N
\times N$ matrices and $\alpha \in \RR$, then
%
\begin{enumerate}
    \item $\det(\boldI) = 1$  
        \vspace{0.6em}
    \item $\boldA$ is nonsingular if and only if $\det(\boldA)
        \not= 0$
        \vspace{0.6em}
    \item $\det(\boldA\boldB) = \det(\boldA)
        \det(\boldB)$
        \vspace{0.6em}
    \item $\det(\alpha \boldA) = \alpha^N \det(\boldA)$
        \vspace{0.6em}
    \item $\det(\boldA^{-1}) = (\det(\boldA))^{-1}$
\end{enumerate}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Thus singularity in the $2 \times 2$ case is equivalent to
%
$$
\det(\boldA)
=
\det 
    \left(
    \begin{array}{cc}
        a_{11} &amp; a_{12}  \\
        a_{21} &amp; a_{22}  \\
    \end{array}
    \right)
    = a_{11}a_{22} - a_{12} a_{21} = 0
$$
%

\vspace{1.5em}

\Ex Let $\bolda_i := \col_i(\boldA)$ and assume that $a_{ij} \not= 0$ for each $i, j$

Show the following are equivalent:
%
\begin{enumerate}
    \item $a_{11}a_{22} = a_{12} a_{21}$
    \item $\bolda_1 = \lambda \bolda_2$ for some $\lambda \in \RR$
\end{enumerate}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: A = np.random.randn(2, 2)  # Random matrix</p>
<p>In [3]: A
Out[3]:
array([[-0.70120551,  0.57088203],
[ 0.40757074, -0.72769741]])</p>
<p>In [4]: np.linalg.det(A)
Out[4]: 0.27759063032043652</p>
<p>In [5]: 1.0 / np.linalg.det(np.linalg.inv(A))
Out[5]: 0.27759063032043652
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>As an exercise, let&#39;s now show that any right inverse is an inverse

Fix square $\boldA$ and suppose $\boldB$ is a right inverse:
%
\begin{equation}
    \label{eq:ud}
    \boldA \boldB = \boldI
\end{equation}

Applying the determinant to both sides gives $\det(\boldA) \det(\boldB) = 1$

Hence $\boldB$ is nonsingular (why?) and we can
%
\begin{enumerate}
    \item multiply \eqref{eq:ud} by $\boldB$ to get $\boldB \boldA \boldB = \boldB$
    \item then postmultiply by $\boldB^{-1}$ to get $\boldB \boldA = \boldI$
\end{enumerate}

We see that $\boldB$ is also left inverse, and therefore an inverse of $\boldA$ 

\Ex Do the left inverse case 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Other Linear Equations}</p>
<p>\begin{frame}
\frametitle{Other Linear Equations}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>So far we have considered the nice $N \times N$ case for equations

\begin{itemize}
    \item number of equations $=$ number of unknowns
\end{itemize}


\vspace{0.51em}

We have to deal with other cases too

Underdetermined systems: 

\begin{itemize}
    \item eqs $&lt;$ unknowns
\end{itemize}

Overdetermined systems: 

\begin{itemize}
    \item eqs $&gt;$ unknowns
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Overdetermined Systems}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Consider the system $\boldA \boldx = \boldb$ where $\boldA$ is $N \times K$ and $K &lt; N$

\vspace{1em}

\begin{itemize}
    \item The elements of $\boldx$ are the unknowns 
        \vspace{0.5em}
    \item More equations than unknowns ($N &gt; K$)
\end{itemize}

\vspace{1em}

May not be able to find an $\boldx$ that satisfies all $N$ equations  

\vspace{1em}

Let&#39;s look at this in more detail...
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Fix $N \times K$ matrix $\boldA$ with $K &lt; N$

Let $T \colon \RR^K \to \RR^N$ be defined by $T \boldx = \boldA \boldx$

\vspace{1em}

We know these to be equivalent:
%
\begin{enumerate}
    \item there exists an $\boldx \in \RR^K$ with $\boldA \boldx = \boldb$
        \vspace{0.3em}
    \item $\boldb$ has a preimage under $T$
        \vspace{0.3em}
    \item $\boldb$ is in $\range(T)$
        \vspace{0.3em}
    \item $\boldb$ is in $\Span(\boldA)$
\end{enumerate}

\vspace{1em}

We also know $T$ \underline{cannot} be onto (maps small to big space)

Hence $\boldb \in \Span(\boldA)$ will not always hold
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Given our assumption that $K &lt; N$, how rare is the scenario $\boldb \in
\Span(\boldA)$?

\vspace{1em}

Answer: We talked about this before --- it&#39;s very rare

\vspace{1em}
We know that $\dim(\range(T)) = \dim(\Span(\boldA)) \leq K &lt; N$

\vspace{1em}

A $K &lt; N$ dimensional subspace has ``measure zero&quot; in $\RR^N$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>So should we give up on solving $\boldA \boldx = \boldb$ in the
overdetermined case?

What&#39;s typically done is we try to find a best approximation

To define ``best&#39;&#39; we need a way of ranking approximations

The standard way is in terms of Euclidean norm

In particular, we search for the $\boldx$ that solves

$$ \min_{\boldx \in \RR^K} \| \boldA \boldx - \boldb\|$$

Details later
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Underdetermined Systems}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Now consider $\boldA \boldx = \boldb$ when $\boldA$ is $N \times K$ and $K &gt; N$

Let $T \colon \RR^K \to \RR^N$ be defined by $T \boldx = \boldA \boldx$

Now $T$ maps from a larger to a smaller place

This tells us that $T$ is not one-to-one

Hence solutions are not in general unique

In fact the following is true

\Ex Show that $\boldA \boldx =
\boldb$ has a solution and $K &gt; N$, then the same equation has an infinity of solutions

\vspace{0.3em}

Remark: Working with underdetermined systems is relatively rare in economics / elsewhere
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Transpose}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The \navy{transpose} of $\boldA$ is the matrix $\boldA&#39;$ defined by 
$$\col_n(\boldA&#39;) = \row_n(\boldA)$$


\Egs If
%
\begin{equation*}
    \label{eq:aandb}
    \boldA := 
    \left(
    \begin{array}{cc}
        10 &amp; 40  \\
        20 &amp; 50  \\
        30 &amp; 60
    \end{array}
    \right)
    \quad \text{then} \quad
    \boldA&#39; = 
    \left(
    \begin{array}{ccc}
        10 &amp; 20 &amp; 30 \\
        40 &amp; 50 &amp; 60 
    \end{array}
    \right)
\end{equation*}
%

If
%
\begin{equation*}
    \boldB := 
    \left(
    \begin{array}{ccc}
        1 &amp; 3 &amp; 5 \\
        2 &amp; 4 &amp; 6 \\
    \end{array}
    \right)
    \quad \text{then} \quad
    \boldB&#39; := 
    \left(
    \begin{array}{cc}
        1 &amp; 2  \\
        3 &amp; 4  \\
        5 &amp; 6 
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact For conformable matrices $\boldA$ and $\boldB$, transposition satisfies
%
\begin{enumerate}
    \item $(\boldA&#39;)&#39; = \boldA$
        \vspace{0.4em}
    \item $(\boldA \boldB)&#39; = \boldB&#39; \boldA&#39;$
        \vspace{0.4em}
    \item $(\boldA + \boldB)&#39; = \boldA&#39; +  \boldB&#39;$
        \vspace{0.4em}
    \item $(c \boldA)&#39; = c \boldA&#39;$ for any constant $c$
\end{enumerate}
%

\vspace{1em}


For each square matrix $\boldA$, 
%
\begin{enumerate}
    \item $\det(\boldA&#39;) = \det(\boldA)$
        \vspace{0.4em}
    \item If $\boldA$ is nonsingular then so is $\boldA&#39;$, and $(\boldA&#39;)^{-1}= (\boldA^{-1})&#39;$
\end{enumerate}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: A = np.random.randn(2, 2)</p>
<p>In [3]: np.linalg.inv(A.transpose())
Out[3]:
array([[ 4.52767206, -1.83628665],
[ 0.90504942,  1.5014984 ]])</p>
<p>In [4]: np.linalg.inv(A).transpose()
Out[4]:
array([[ 4.52767206, -1.83628665],
[ 0.90504942,  1.5014984 ]])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A square matrix $\boldA$ is called \navy{symmetric} if $\boldA&#39; = \boldA$

\vspace{1em}

Equivalent: $a_{nk} = a_{kn}$ for all $n, k$

\vspace{1em}

\Egs

\begin{equation*}
    \boldA 
    := 
    \left(
    \begin{array}{cc}
        10 &amp; 20  \\
        20 &amp; 50  
    \end{array}
    \right),
    \qquad
    \boldB 
    := 
    \left(
    \begin{array}{ccc}
        1 &amp; 2 &amp; 3  \\
        2 &amp; 0 &amp; 0 \\ 
        3 &amp; 0 &amp; 2 
    \end{array}
    \right)
\end{equation*}
%

\vspace{1em}

\Ex For any matrix $\boldA$, show that $\boldA&#39; \boldA$ and $\boldA \boldA&#39;$ are always
%
\begin{enumerate}
    \item well-defined (multiplication makes sense)
    \item symmetric
\end{enumerate}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The \navy{trace} of a square matrix is defined by 
%
\vspace{0.5em}
\begin{equation*}
    \trace \left(
    \begin{array}{ccc}
        a_{11} &amp;  \cdots &amp; a_{1N} \\
        \vdots &amp; &amp;  \vdots \\
        a_{N1} &amp;  \cdots &amp; a_{NN} \\
    \end{array}
    \right)
    = 
    \sum_{n=1}^N a_{nn}
\end{equation*}
%

\vspace{0.5em}

\Fact $\trace(\boldA) = \trace(\boldA&#39;)$

\vspace{0.5em}

\Fact If $\boldA$ and $\boldB$ are square matrices and
$\alpha, \beta \in \RR$, then 
%
\begin{equation*}
    \trace(\alpha \boldA + \beta \boldB) 
    = \alpha \trace(\boldA) + \beta \trace(\boldB)
\end{equation*}
%

\vspace{0.5em}

\Fact When conformable, $\trace(\boldA \boldB) = \trace(\boldB \boldA)$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A square matrix $\boldA$ is called \navy{idempotent} if $\boldA \boldA = \boldA$

\vspace{1em}

\Egs

\begin{equation*}
    \boldA 
    := 
    \left(
    \begin{array}{cc}
        1 &amp; 1  \\
        0 &amp; 0  
    \end{array}
    \right),
    \qquad
    \boldI 
    := 
    \left(
    \begin{array}{ccc}
        1 &amp; 0 &amp; 0  \\
        0 &amp; 1 &amp; 0 \\ 
        0 &amp; 0 &amp; 1 
    \end{array}
    \right)
\end{equation*}
%

\vspace{1em}

The next result is often used in statistics / econometrics:

\vspace{1em}

\Fact If $\boldA$ is idempotent, then $\rank(\boldA) = \trace(\boldA)$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Diagonal Matrices}</p>
<p>\begin{frame}
\frametitle{Diagonal Matrices}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Consider a square $N \times N$ matrix $\boldA$

\vspace{1em}

The $N$ elements of the form $a_{nn}$ are called the \navy{principal diagonal}


\vspace{1em}


\begin{equation*}
    \left(
    \begin{array}{cccc}
        \red{a_{11}} &amp; a_{12} &amp; \cdots &amp; a_{1N} \\
        a_{21} &amp; \red{a_{22}} &amp; \cdots &amp; a_{2N} \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        a_{N1} &amp; a_{N2} &amp; \cdots &amp; \red{a_{NN}} \\
    \end{array}
    \right)
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A square matrix $\boldD$ is called \navy{diagonal} if all entries off the
principal diagonal are zero

\begin{equation*}
    \boldD = 
    \left(
    \begin{array}{cccc}
        d_1 &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; d_2 &amp; \cdots &amp; 0 \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; d_N \\
    \end{array}
    \right)
\end{equation*}

\vspace{1em}

Often written as

\begin{equation*}
    \boldD = \diag(d_1, \ldots, d_N) 
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Incidentally, the same notation works in Python

\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: D = np.diag((2, 4, 6, 8, 10))</p>
<p>In [3]: D
Out[3]:
array([[ 2,  0,  0,  0,  0],
[ 0,  4,  0,  0,  0],
[ 0,  0,  6,  0,  0],
[ 0,  0,  0,  8,  0],
[ 0,  0,  0,  0, 10]])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Diagonal systems are very easy to solve

\vspace{0.5em}

\Eg

\begin{equation*}
    \begin{pmatrix}
        d_1 &amp; 0  &amp; 0 \\
        0 &amp; d_2 &amp; 0 \\
        0 &amp; 0 &amp; d_3 
    \end{pmatrix}
    \begin{pmatrix}
        x_1  \\
        x_2  \\
        x_3 
    \end{pmatrix}
    =
    \begin{pmatrix}
        b_1  \\
        b_2  \\
        b_3 
    \end{pmatrix}
\end{equation*}

is equivalent to

\begin{equation*}
    \begin{array}{c}
        d_1 x_1  = b_1  \\
        d_2x_2 = b_2 \\
        d_3 x_3 = b_3
    \end{array}
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $\boldC = \diag(c_1, \ldots, c_N)$ and $\boldD = \diag(d_1, \ldots,
d_N)$ then
%
\begin{enumerate}
    \item $\boldC + \boldD = \diag(c_1 + d_1, \ldots, c_N + d_N)$
        \vspace{0.4em}
    \item $\boldC \boldD = \diag(c_1 d_1, \ldots, c_N d_N)$
        \vspace{0.4em}
    \item $\boldD^k = \diag(d^k_1, \ldots, d^k_N)$ for any $k \in \NN$
        \vspace{0.4em}
    \item $d_n \geq 0$ for all $n$ $\implies$ $\boldD^{1/2}$ exists and equals
        %
        $$\diag(\sqrt{d_1}, \ldots, \sqrt{d_N})$$
        \vspace{-1.5em}
    \item $d_n \not= 0$ for all $n$ $\implies$ $\boldD$ is nonsingular and 
        %
        $$\boldD^{-1} = \diag(d_1^{-1}, \ldots, d_N^{-1})$$
\end{enumerate}
%

\vspace{1em}

Proofs: Check 1 and 2 directly, other parts follow
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np</p>
<p>In [2]: D = np.diag((2, 4, 10, 100))</p>
<p>In [3]: np.linalg.inv(D)
Out[3]:
array([[ 0.5 ,  0.  ,  0.  ,  0.  ],
[ 0.  ,  0.25,  0.  ,  0.  ],
[ 0.  ,  0.  ,  0.1 ,  0.  ],
[ 0.  ,  0.  ,  0.  ,  0.01]])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A square matrix is called \navy{lower triangular} if every element strictly above the
principle diagonal is zero

\Eg
%
\begin{equation*}
    \label{eq:ltute}
    \boldL :=
    \left(
    \begin{array}{ccc}
        1 &amp; 0 &amp; 0  \\
        2 &amp; 5 &amp; 0 \\
        3 &amp; 6 &amp; 1
    \end{array}
    \right)
\end{equation*}
%

A square matrix is called \navy{upper triangular} if every element
strictly below the principle diagonal is zero

\Eg
%
\begin{equation*}
    \boldU :=
    \left(
    \begin{array}{ccc}
        1 &amp; 2 &amp; 3  \\
        0 &amp; 5 &amp; 6 \\
        0 &amp; 0 &amp; 1
    \end{array}
    \right)
\end{equation*}
%

Called \navy{triangular} if either upper or lower triangular
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<p>Associated linear equations also simple to solve</p>
<p>\vspace{0.7em}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg
% 
\begin{equation*}
    \left(
    \begin{array}{ccc}
        4 &amp; 0 &amp; 0  \\
        2 &amp; 5 &amp; 0 \\
        3 &amp; 6 &amp; 1
    \end{array}
    \right)
    \left(
    \begin{array}{ccc}
        x_1 \\
        x_2 \\
        x_3 
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        b_1 \\
        b_2 \\
        b_3 \\
    \end{array}
    \right)
\end{equation*}

becomes

\begin{equation*}
    \begin{array}{c}
        4x_1  = b_1  \\
        2x_1 + 5x_2 = b_2 \\
        3x_1 + 6x_2 + x_3 = b_3
    \end{array}
\end{equation*}
%

Top equation involves only $x_1$, so can solve for it directly

Plug that value into second equation, solve out for $x_2$, etc.
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Eigenvalues}</p>
<p>\begin{frame}
\frametitle{Eigenvalues and Eigenvectors}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\boldA$ be $N \times N$  

In general $\boldA$ maps $\boldx$ to some arbitrary new location $\boldA \boldx$

But sometimes $\boldx$ will only be \underline{scaled}:
%
\begin{equation}
    \label{eq:eiei}
    \boldA \boldx = \lambda \boldx
    \quad \text{for some scalar $\lambda$}
\end{equation}
%

If \eqref{eq:eiei} holds and $\boldx$ is nonzero, then 
%
\begin{enumerate}
    \item $\boldx$ is called an \navy{eigenvector} of $\boldA$
        and $\lambda$ is called an \navy{eigenvalue}
        \vspace{0.3em}
    \item $(\boldx, \lambda)$ is called an \navy{eigenpair}
\end{enumerate}

Clearly $(\boldx, \lambda)$ is an eigenpair of $\boldA$ $\implies$
$(\alpha \boldx, \lambda)$ is an eigenpair of $\boldA$ for any nonzero $\alpha$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg Let
%
$$
\boldA :=
\begin{pmatrix}
    1 &amp; -1 \\
    3 &amp; 5
\end{pmatrix}
$$

Then

$$
\lambda = 2 
\quad \text{ and } \quad
\boldx
=
\begin{pmatrix}
    1 \\
    -1
\end{pmatrix}
$$

form an eigenpair because $\boldx \not= \boldzero$ and

$$
\boldA \boldx =
\begin{pmatrix}
    1 &amp; -1 \\
    3 &amp; 5
\end{pmatrix}
\begin{pmatrix}
    1 \\
    -1
\end{pmatrix}
=
\begin{pmatrix}
    2 \\
    -2
\end{pmatrix}
= 2
\begin{pmatrix}
    1 \\
    -1
\end{pmatrix}
=
\lambda \boldx 
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg
</pre></div>
</div>
<p>\begin{pythoncode}
In [3]: import numpy as np
In [4]: A = [[1, 2],
…:      [2, 1]]</p>
<p>In [5]: eigvals, eigvecs = np.linalg.eig(A)</p>
<p>In [6]: x = eigvecs[:,0]  # Let x = first eigenvector
In [7]: lm = eigvals[0]   # Let lm = first eigenvalue</p>
<p>In [8]: np.dot(A, x)      # Compute Ax
Out[8]: array([ 2.12132034,  2.12132034])
In [9]: lm * x            # Compute lm x
Out[9]: array([ 2.12132034,  2.12132034])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{eigenvecs.pdf}}
    \caption{The eigenvectors of $\boldA$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Consider the matrix 
%
\begin{equation*}
    \boldR := 
    \left(
    \begin{array}{cc}
        0 &amp; -1  \\
        1 &amp; 0  \\
    \end{array}
    \right)
\end{equation*}
%

Induces counter-clockwise rotation on any point by $90^{\circ}$

Hence no point $\boldx$ is scaled

Hence there exists \underline{no} pair $\lambda \in \RR$ and $\boldx \not=
\boldzero$
such that
%
$$\boldR \boldx = \lambda \boldx$$  
%

\begin{itemize}
    \item In other words, no \underline{real-valued} eigenpairs exist
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{rotation_1.pdf}}
    \caption{The matrix $\boldR$ rotates points by $90^{\circ}$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{rotation_2.pdf}}
    \caption{The matrix $\boldR$ rotates points by $90^{\circ}$}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>But $\boldR \boldx = \lambda \boldx$ can hold \underline{if} we allow
complex values

\vspace{1em}

\Eg 
%
\begin{equation*}
    \left(
    \begin{array}{cc}
        0 &amp; -1  \\
        1 &amp; 0  \\
    \end{array}
    \right)
    \begin{pmatrix}
        1 \\
        -i
    \end{pmatrix}
    =
    \begin{pmatrix}
        i \\
        1
    \end{pmatrix}
    =
    i
    \begin{pmatrix}
        1 \\
        -i
    \end{pmatrix}
\end{equation*}

That is,
%
\begin{equation*}
    \boldR \boldx = \lambda \boldx
    \quad \text{for} \quad
    \lambda := i
    \quad \text{and} \quad
    \boldx := 
    \begin{pmatrix}
        1 \\
        -i
    \end{pmatrix}
\end{equation*}


Hence $(\boldx, \lambda)$ is an eigenpair provided we admit complex values 

We do, since this is standard
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact For any square matrix $\boldA$ 
%
\begin{equation*}
    \lambda \text{ is an eigenvalue of } \boldA \; \iff \;
    \det(\boldA - \lambda \boldI) = 0
\end{equation*}
%

\vspace{1em}

Proof:  Let $\boldA$ by $N \times N$ and let $\boldI$ be the $N \times N$
identity

We have
%
\begin{align*}
    \det(\boldA - \lambda \boldI) = 0
    &amp; \iff \boldA - \lambda \boldI \text{ is singular}
    \\
    &amp; \iff \exists \, \boldx \not= \boldzero \st
        (\boldA - \lambda \boldI) \boldx = \boldzero
    \\
    &amp; \iff \exists \, \boldx \not= \boldzero \st
    \boldA \boldx = \lambda \boldx
    \\
    &amp; \iff \lambda 
    \text{ is an eigenvalue of } \boldA
\end{align*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Eg In the $2 \times 2$ case,
%
\begin{equation*}
    \boldA =
    \left(
    \begin{array}{cc}
        a &amp; b  \\
        c &amp; d  \\
    \end{array}
    \right)
    \quad \implies \quad
    \boldA - \lambda \boldI =
    \left(
    \begin{array}{cc}
        a - \lambda &amp; b  \\
        c &amp; d - \lambda 
    \end{array}
    \right)
\end{equation*}
%
%
\begin{align*}
    \fore
\det(\boldA - \lambda \boldI) 
&amp; = (a - \lambda)(d - \lambda) - bc
\\
&amp; = \lambda^2 - (a + d) \lambda + (ad - bc)
\end{align*}

Hence the eigenvalues of $\boldA$ are given by the two roots of 
%
\begin{equation*}
    \lambda^2 - (a + d) \lambda + (ad - bc) = 0
\end{equation*}

Equivalently,
%
\begin{equation*}
    \lambda^2 - \trace(\boldA) \lambda + \det(\boldA) = 0
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Existence of Eigenvalues}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Fix $N \times N$ matrix $\boldA$ 

\vspace{0.5em}

\Fact There exist complex numbers $\lambda_1, \ldots, \lambda_N$ such that
%
\begin{equation*}
    \det(\boldA - \lambda \boldI) = \prod_{n=1}^N (\lambda_n - \lambda)
\end{equation*}
%

Each such $\lambda_i$  is an eigenvalue of $\boldA$ because
%
\begin{equation*}
    \det(\boldA - \lambda_i \boldI) 
    = \prod_{n=1}^N (\lambda_n - \lambda_i) 
    = 0
\end{equation*}
%

Important: Not all are necessarily distinct --- there can be repeats
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact Given $N \times N$ matrix $\boldA$ with eigenvalues $\lambda_1, \ldots, \lambda_N$
we have
%
\begin{enumerate}
        \vspace{-0.6em}
    \item $\det(\boldA) = \prod_{n=1}^N \lambda_n$
        \vspace{0.4em}
    \item $\trace(\boldA) = \sum_{n=1}^N \lambda_n$
        \vspace{0.4em}
    \item If $\boldA$ is symmetric, then $\lambda_n \in \RR$ for all $n$
        \vspace{0.4em}
    \item If $\boldA = \diag(d_1, \ldots, d_N)$, then $\lambda_n = d_n$ for all $n$
\end{enumerate}

\vspace{0.8em}

Hence $\boldA$ is nonsingular $\iff$ all eigenvalues are nonzero (why?)

\vspace{0.4em}

\Fact If $\boldA$ is nonsingular, then 
%
$$
\text{eigenvalues of } \boldA^{-1}
= 1/\lambda_1, \ldots, 1/\lambda_N
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}
\frametitle{Diagonalization}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Square matrix $\boldA$ is said to be \navy{similar} to square matrix $\boldB$ if 
%
$$
\exists \text{ invertible matrix $\boldP$ such that }
\boldA = \boldP \boldB \boldP^{-1}
$$  


\begin{figure}
   \begin{center}
       \scalebox{0.6}{\input{../tikzfigs/diagonalize.tex}}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $\boldA$ is similar to $\boldB$, then $\boldA^t$ is similar to
$\boldB^t$ for all $t \in \NN$

\vspace{1em}

Proof for case $t=2$: 
%
\begin{align*}
\boldA^2
&amp; = \boldA \boldA \\
&amp; = \boldP \boldB \boldP^{-1} \boldP \boldB \boldP^{-1} \\
&amp; = \boldP \boldB \boldB \boldP^{-1} \\
&amp; = \boldP \boldB^2 \boldP^{-1} 
\end{align*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>If $\boldA$ is similar to a diagonal matrix, then $\boldA$ is
called \navy{diagonalizable}

\vspace{1em}

\Fact  Let $\boldA$ be diagonalizable with $\boldA = \boldP \boldD
\boldP^{-1}$ and let
%
\begin{enumerate}
    \item $\boldD = \diag(\lambda_1, \ldots, \lambda_N)$
    \item $\boldp_n := \col_n(\boldP)$
\end{enumerate}
%
Then $(\boldp_n, \lambda_n)$ is an eigenpair of $\boldA$ for each $n$

\vspace{1em}

Proof: From $\boldA = \boldP \boldD \boldP^{-1}$ we get $\boldA \boldP = \boldP \boldD$

Equating $n$-th column on each side gives 
%
$$
\boldA \boldp_n = \lambda_n \boldp_n
$$

Moreover $\boldp_n \not= \boldzero$ because $\boldP$ is invertible (which
facts?)
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact  If $N \times N$ matrix $\boldA$ has $N$ distinct eigenvalues
$\lambda_1, \ldots, \lambda_N$, then
$\boldA$ is diagonalizable as $\boldA = \boldP \boldD \boldP^{-1}$ where
%
\begin{enumerate}
    \item $\boldD = \diag(\lambda_1, \ldots, \lambda_N)$
    \item $\col_n(\boldP)$ is an eigenvector for $\lambda_n$
\end{enumerate}

\Eg Let
$$
\boldA :=
\begin{pmatrix}
    1 &amp; -1 \\
    3 &amp; 5
\end{pmatrix}
$$

The eigenvalues of $\boldA$ are 2 and 4, while the eigenvectors are
%
$$
\boldp_1 :=
\begin{pmatrix}
    1 \\
    -1
\end{pmatrix}
\quad \text{and} \quad
\boldp_2 :=
\begin{pmatrix}
    1 \\
    -3
\end{pmatrix}
$$

Hence
%
\begin{equation*}
    \boldA = \boldP \diag(2, 4) \boldP^{-1}
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}[fragile]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{pythoncode}
</pre></div>
</div>
<p>In [1]: import numpy as np
In [2]: from numpy.linalg import inv</p>
<p>In [3]: A = [[1, -1],
…:      [3, 5]]</p>
<p>In [4]: D = np.diag((2, 4))</p>
<p>In [5]: P = [[1, 1],  # Matrix of eigenvectors
…:     [-1, -3]]</p>
<p>In [6]: np.dot(P, np.dot(D, inv(P)))  # PDP^{-1} = A?
Out[6]:
array([[ 1., -1.],
[ 3.,  5.]])
\end{pythoncode}</p>
<p>\end{frame}</p>
<p>\section{Matrix Norm}</p>
<p>\begin{frame}
\frametitle{The Euclidean Matrix Norm}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The concept of norm is very helpful for working with vectors

\begin{itemize}
    \item provides notions of distance, similarity, convergence
\end{itemize}

How about an analogous concept for matrices?

\vspace{1em}

Given $N \times K$ matrix $\boldA$, we define 
%
\begin{equation*}
    \| \boldA \| :=
    \max \left\{ 
        \frac{\| \boldA \boldx \|}{ \| \boldx \|} \,: \, 
        \boldx \in \RR^K, \; \boldx \not= \boldzero
        \right\}
\end{equation*}
%

\begin{itemize}
    \item LHS is the \navy{matrix norm} of $\boldA$
    \item RHS is ordinary Euclidean vector norms
\end{itemize}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>In the maximization we can restrict attention to $\boldx$ s.t. $\| \boldx \| = 1$

To see this let
%
\begin{equation*}
    a :=
    \max_{\boldx \not= \boldzero} \frac{\| \boldA \boldx \|}{ \| \boldx \|} 
    \qquad \text{and} \qquad
    b :=
    \max_{\| \boldx \| = 1} \frac{\| \boldA \boldx \|}{ \| \boldx \|} 
    = \max_{\| \boldx \| = 1} \| \boldA \boldx \|
\end{equation*}
%

Evidently $a \geq b$ because max is over a larger domain

To see the reverse let 

\begin{itemize}
    \item $\boldx_a$ be the maximizer over $\boldx \not= \boldzero$ and
        let $\alpha := 1 / \| \boldx_a\|$
    \item $\boldx_b := \alpha \boldx_a $ 
\end{itemize}

Then 
$$
    b 
    \geq \frac{ \| \boldA \boldx_b \| }{ \| \boldx_b \|}
    = \frac{ \| \alpha \boldA \boldx_a \| }{ \| \alpha \boldx_a \|}
    = \frac{\alpha}{\alpha} \frac{\| \boldA \boldx_a \|}{ \| \boldx_a \|}
    = a
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Ex Show that for any $\boldx$ we have $\|\boldA \boldx\| \leq \| \boldA \| \| \boldx \|$

If $\| \boldA \| &lt; 1$ then $\boldA$ is called \navy{contractive} --- it
shrinks the norm

\begin{figure}
   \begin{center}
       \scalebox{0.85}{\input{../tikzfigs/contractive.tex}}
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>The matrix norm has similar properties to the Euclidean norm

\vspace{1em}

\Fact For conformable matrices $\boldA$ and $\boldB$, we have
%
\begin{enumerate}
    \item $\| \boldA \| = \boldzero$ if and only if all entries of
        $\boldA$ are zero
        \vspace{1em}
    \item $\| \alpha \boldA \| = |\alpha| \| \boldA \|$ for any scalar
        $\alpha$
        \vspace{1em}
    \item $\| \boldA + \boldB \| \leq \| \boldA \| + \| \boldB \|$
        \vspace{1em}
    \item $\| \boldA \boldB \| \leq \| \boldA \| \| \boldB \|$
\end{enumerate}

        \vspace{1em}
The last inequality is called the submultiplicative property of the matrix
norm

For square $\boldA$ it implies that $\|\boldA^k\| \leq \|\boldA \|^k$ for any $k \in \NN$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact For the diagonal matrix
%
\begin{equation*}
    \boldD = 
    \diag(d_1, \ldots, d_N) 
    =
    \left(
    \begin{array}{cccc}
        d_1 &amp; 0 &amp; \cdots &amp; 0 \\
        0 &amp; d_2 &amp; \cdots &amp; 0 \\
        \vdots &amp; \vdots &amp;  &amp; \vdots \\
        0 &amp; 0 &amp; \cdots &amp; d_N \\
    \end{array}
    \right)
\end{equation*}


we have

$$ 
    \| \boldD \| = \max_n |d_n|
$$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\{ \boldA_j \}$ and $\boldA$ be $N \times K$ matrices

\begin{itemize}
    \item If $\| \boldA_j - \boldA \| \to 0$ then we say that $\boldA_j$
        \navy{converges} to $\boldA$
        \vspace{1em}
    \item If $\sum_{j=1}^J \boldA_j$ converges to some matrix
    $\boldB_{\infty}$ as $J \to \infty$ we write
    %
    \begin{equation*}
        \sum_{j=1}^{\infty} \boldA_j = \boldB_{\infty}
    \end{equation*}
\end{itemize}


\vspace{1em}

In other words,
%
\begin{equation*}
    \boldB_{\infty} = \sum_{j=1}^{\infty} \boldA_j 
    \quad \iff \quad
    \lim_{J \to \infty}
    \;
    \left\| \sum_{j=1}^J \boldA_j  - \boldB_{\infty} \right\|
    \to 0
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Neumann Series}</p>
<p>\begin{frame}
\frametitle{Neumann Series}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Consider the difference equation $\boldx_{t+1} = \boldA \boldx_t + \boldb$, where 
%
\begin{itemize}
    \item $\boldx_t \in \RR^N$ represents the values of some variables at time $t$
        \vspace{0.4em}
    \item $\boldA$ and $\boldb$ form the parameters in the law of motion for $\boldx_t$
\end{itemize}

Question of interest: is there an $\boldx$ such that 
$$
    \boldx_t = \boldx  \; \implies \;  \boldx_{t+1} = \boldx
$$

In other words, we seek an $\boldx \in \RR^N$ that solves the system of equations
%
\begin{equation*}
    \boldx = \boldA \boldx + \boldb, 
    \quad \text{where} \quad \boldA \text{ is } N \times N
    \text{ and } \boldb \text{ is } N \times 1
\end{equation*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>We can get some insight from the scalar case $x = a x + b$

\vspace{1em}

If $|a| &lt; 1$, then this equation has the solution
%
\begin{equation*}
    \bar x 
    = \frac{b}{1-a} 
     = b \sum_{k=0}^{\infty} a^k 
\end{equation*}

\vspace{1em}
Does an analogous result hold in the vector case $\boldx = \boldA \boldx + \boldb$?

\vspace{1em}
Yes, if we replace condition $|a| &lt; 1$ with $\| \boldA \| &lt; 1$ 
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Let $\boldb$ be any vector in $\RR^N$ and $\boldA$ be an $N \times N$ matrix

The next result is called the \navy{Neumann series lemma} 

\vspace{1em}

\Fact  If $\| \boldA^k \| &lt; 1$ for some $k \in \NN$, then $\boldI - \boldA$
is invertible and
%
\begin{equation*}
     (\boldI - \boldA)^{-1} = \sum_{j=0}^{\infty} \boldA^j
\end{equation*}
%

\vspace{1em}

In this case $\boldx = \boldA \boldx + \boldb$ has the unique solution
%
\begin{equation*}
    \bar \boldx = \sum_{j=0}^{\infty} \boldA^j  \boldb 
\end{equation*}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Sketch of proof that $(\boldI - \boldA)^{-1} = \sum_{j=0}^{\infty} \boldA^j$ 
for case $\|\boldA \| &lt; 1$

We have $(\boldI - \boldA) \sum_{j=0}^{\infty} \boldA^j =
\boldI $ because
%
\begin{align*}
   \left\|
    (\boldI - \boldA) \sum_{j=0}^{\infty} \boldA^j - \boldI
   \right\|
   &amp; 
   =
   \left\|
    (\boldI - \boldA) \lim_{J \to \infty}\sum_{j=0}^J \boldA^j - \boldI
   \right\|
   \\
   &amp; 
   =
   \lim_{J \to \infty}
   \left\|
    (\boldI - \boldA) \sum_{j=0}^J \boldA^j - \boldI
   \right\|
   \\
   &amp; =
   \lim_{J \to \infty}
   \left\|
    \boldA^J 
   \right\|
   \\
   &amp; \leq
   \lim_{J \to \infty}
   \left\| \boldA \right\|^J = 0
\end{align*}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>How to test the hypotheses of the Neumann series lemma?

\vspace{1em}

The \navy{spectral radius} of square matrix $\boldA$ is
%
\begin{equation*}
    \rho(\boldA) := \max \setntn{ |\lambda| }
    { \lambda \text{ is an eigenvalue of } \boldA}
\end{equation*}

\vspace{1em}

Here $|\lambda|$ is the \navy{modulus} of the possibly complex number $\lambda$

\vspace{1em}

\Eg  If $\lambda = a + ib$, then 
$$|\lambda| = (a^2 + b^2)^{1/2}$$

\Eg  If $\lambda \in \RR$, then $|\lambda|$ is the absolute value
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact If $\rho(\boldA) &lt; 1$, then $\| \boldA^j \| &lt; 1$ for some $j \in \NN$    

\vspace{1em}

Proof, for diagonalizable $\boldA$:

\vspace{0.3em}

We have $\boldA^j = \boldP \boldD^j \boldP^{-1}$ where
%
$$
\boldD = \diag(\lambda_1, \ldots, \lambda_N)
\quad \text{and hence} \quad
\boldD^j = \diag(\lambda_1^j, \ldots, \lambda_N^j)
$$

Hence
%
$$
    \| \boldA^j \|
    = \| \boldP \boldD^j \boldP^{-1} \|
    \leq \| \boldP \|  \| \boldD^j \| \| \boldP^{-1} \|
$$

In particular, when $C := \| \boldP \|\| \boldP^{-1} \|$,
$$
    \| \boldA^j \| 
    \leq C \max_n |\lambda_n^j| 
    = C \max_n |\lambda_n|^j
    = C \rho(\boldA)^j
$$

This is $&lt;1$ for large enough $j$ because $\rho(\boldA) &lt; 1$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\section{Quadratic Forms}</p>
<p>\begin{frame}
\frametitle{Quadratic Forms}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Up till now we have studied linear functions extensively

\vspace{1em}

Next level of complexity is quadratic maps

\vspace{1em}

Let $\boldA$ be $N \times N$ and symmetric, and let $\boldx$ be $N \times 1$

\vspace{1em}

The \navy{quadratic function} on $\RR^N$ associated with $\boldA$ is the
map 
%
\begin{equation*}
    Q \colon \RR^N \to \RR, \qquad
    Q(\boldx) := \boldx&#39; \boldA \boldx = \sum_{j=1}^N \sum_{i=1}^N a_{ij} x_i x_j
\end{equation*}
%

The properties of $Q$ depend on $\boldA$
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>An $N \times N$ symmetric matrix $\boldA$ is called

\vspace{1em}

\begin{enumerate}
    \item \navy{nonnegative definite} if $\boldx&#39; \boldA \boldx \geq 0$
        for all $\boldx \in \RR^N$ 
        \vspace{1em}
    \item \navy{positive definite} if $\boldx&#39; \boldA \boldx &gt; 0$ for all $\boldx
        \in \RR^N$ with $\boldx \not= \boldzero$
        \vspace{1em}
    \item \navy{nonpositive definite} if $\boldx&#39; \boldA \boldx \leq 0$
        for all $\boldx \in \RR^N$
        \vspace{1em}
    \item \navy{negative definite} if $\boldx&#39; \boldA \boldx &lt; 0$ for all $\boldx
        \in \RR^N$ with $\boldx \not= \boldzero$
\end{enumerate}
%
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{qform_pd.pdf}}
    \caption{\label{f:qform_pd} A positive definite case: $Q(\boldx) = \boldx&#39; \boldI \boldx$ }
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{qform_nd.pdf}}
    \caption{\label{f:qform_nd} A negative definite case: $Q(\boldx) =
    \boldx&#39;(-\boldI)\boldx$ }
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Note that some matrices have none of these properties

        \vspace{1em}

\begin{itemize}
    \item $\boldx&#39; \boldA \boldx &lt; 0$ for some $\boldx$
        \vspace{1em}
    \item $\boldx&#39; \boldA \boldx &gt; 0$ for other $\boldx$
\end{itemize}

        \vspace{1em}

In this case $\boldA$ is called \navy{indefinite}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\begin{figure}
   \begin{center}
    \scalebox{.4}{\includegraphics{qform_indef.pdf}}
    \caption{\label{f:qform_indef} Indefinite quadratic function $Q(\boldx) = x_1^2/2 +
        8 x_1 x_2 + x_2^2/2$ }
   \end{center}
\end{figure}
</pre></div>
</div>
<p>\end{frame}</p>
<p>\begin{frame}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\Fact A symmetric matrix $\boldA$ is 
%
\begin{enumerate}
    \item positive definite $\iff$ all eigenvalues are strictly positive
        \vspace{0.4em}
    \item negative definite $\iff$ all eigenvalues are strictly negative
        \vspace{0.4em}
    \item nonpositive definite $\iff$ all eigenvalues are nonpositive
        \vspace{0.4em}
    \item nonnegative definite $\iff$ all eigenvalues are nonnegative
\end{enumerate}
%

\vspace{1em}

It follows that 
%
\begin{itemize}
    \item $\boldA$ is positive definite $\implies$ $\det(\boldA) &gt; 0$ 
\end{itemize}

\vspace{1em}

In particular, $\boldA$ is nonsingular
</pre></div>
</div>
<p>\end{frame}</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#announcements-reminders">Announcements &amp; Reminders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plan-for-this-lecture">Plan for this lecture</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Fedor Iskhakov
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>