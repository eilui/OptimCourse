---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
---

# Elements of linear algebra
**ECON2125/6012 Lecture 5**\
Fedor Iskhakov

## Announcements & Reminders

- Tests due last night: feedback soon

- This lecture is recorded

- Next week we return to the class

## Plan for this lecture

Review of some concepts in linear algebra

1. Vector spaces 
  - Linear combinations, span, linear independence
  - Linear subspaces, bases and dimension
  - Linear maps and independence
2. Linear equations
  - Basic operations with matrices 
  - Square and invertible matrices
  - Determinant
3. Quadratic forms
  - Symmetric matrices
  - Positive and negative definiteness

**Supplementary reading:**
- Stachurski: 2.1, 3.1, 3.2
- Simon & Blume: 6.1, 6.2, 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 11, 16, 23.7, 23.8
- Sundaram: 1.3, Appendix C.1, C.2





\begin{frame}

    \begin{center}
    \newtopic{New Topic}
    \end{center}
    
    \begin{center}
    \newtopic{LINEAR ALGEBRA}
    \end{center}

\end{frame}


\begin{frame}
    \frametitle{Motivation}

    Linear algebra is used to study linear models

    \vspace{1em}
    
    Foundational for many disciplines related to economics


    \vspace{1em}

    \begin{itemize}
        \item Economic theory
        \item Econometrics and statistics
        \item Finance
        \item Operations research
    \end{itemize}

\end{frame}


\begin{frame}
    
    \begin{example}
        
    Equilibrium in a single market with price $p$
    %
    \begin{align*}
        q_d & = a + b p
        \\
        q_s & = c + d p
        \\
        q_s & = q_d
    \end{align*}

    What price $p$ clears the market, and at what quantity $q = q_s = q_d$?

    Remark: Here $a, b, c, d$ are the model \navy{parameters} or \navy{coefficients}
    
    Treated as fixed for a single computation but might vary between
    computations to better fit the data

    \end{example}

\end{frame}


\begin{frame}

    \begin{example}
        
    
    Determination of income 
    %
    \begin{align*}
        C & = a + b(Y - T)
        \\
        E & = C + I
        \\
        G & = T
        \\
        Y & = E
    \end{align*}

    Solve for $Y$ as a function of $I$ and $G$

    \end{example}

\end{frame}

\begin{frame}
    
    Bigger, more complex systems found in problems related to

    \begin{itemize}
        \item Regression and forecasting
            \vspace{1em}
        \item Portfolio analysis
            \vspace{1em}
        \item Ranking systems
            \vspace{1em}
        \item Etc., etc. --- any number of applications
    \end{itemize}


\end{frame}

\begin{frame}

    A general system of equations:
    
    \begin{equation*}
        \begin{array}{c}
            a_{11} x_1 + a_{12} x_2 + \cdots + a_{1K} x_K = b_1 \\
            a_{21} x_1 + a_{22} x_2 + \cdots + a_{2K} x_K = b_2 \\
            \vdots  \\
            a_{N1} x_1 + a_{N2} x_2 + \cdots + a_{NK} x_K = b_N 
        \end{array}
    \end{equation*}

    Typically 
    
    \begin{itemize}
        \item the $a_{nm}$ and $b_n$ are exogenous / given / parameters
        \item the values $x_n$ are endogenous
    \end{itemize}
    
    Key question

    \begin{itemize}
        \item What values of $x_1, \ldots, x_K$ solve this system?
    \end{itemize}


\end{frame}


\begin{frame}
    
    We often write this in \navy{matrix form}

    %
    \begin{equation*}
        \left(
        \begin{array}{cccc}
            a_{11} & a_{12} & \cdots & a_{1K} \\
            a_{21} & a_{22} & \cdots & a_{2K} \\
            \vdots & \vdots &  & \vdots \\
            a_{N1} & a_{N2} & \cdots & a_{NK} 
        \end{array}
        \right)
        \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            \vdots \\
            x_K
        \end{array}
        \right)
        =
        \left(
        \begin{array}{c}
            b_1 \\
            b_2 \\
            \vdots \\
            b_K
        \end{array}
        \right)
    \end{equation*}
    
    \vspace{1em}

    or 
    
    \begin{equation*}
        \boldA \boldx = \boldb
    \end{equation*}

    And we solve it on a computer

\end{frame}



\begin{frame}[fragile]
    

\begin{pythoncode}
In [1]: import numpy as np

In [2]: from scipy.linalg import solve

In [3]: A = [[0, 2, 4],
   ...:      [1, 4, 8],
   ...:      [0, 3, 7]]

In [4]: b = (1, 2, 0)

In [5]: A, b = np.asarray(A), np.asarray(b)

In [6]: solve(A, b)
Out[6]: array([ 0. ,  3.5, -1.5])
\end{pythoncode}


\end{frame}


\begin{frame}[fragile]

    This tells us that the solution is 

\begin{pythoncode}
array([ 0. ,  3.5, -1.5])
\end{pythoncode}

    That is, 
    $$
    \boldx =
        \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            x_3
        \end{array}
        \right)
        =
        \left(
        \begin{array}{c}
            0 \\
            3.5 \\
            -1.5
        \end{array}
        \right)
    $$
    

    \vspace{1em}

    Hey, this is easy --- what do we need to study for?

    \vspace{2em}

    But now let's try this similar looking problem

\end{frame}


\begin{frame}[fragile]
    

\begin{pythoncode}
In [1]: import numpy as np

In [2]: from scipy.linalg import solve

In [3]: A = [[0, 2, 4],
   ...:      [1, 4, 8],
   ...:      [0, 3, 6]]

In [4]: b = (1, 2, 0)

In [5]: A, b = np.asarray(A), np.asarray(b)

In [6]: solve(A, b)
\end{pythoncode}


\end{frame}


\begin{frame}[fragile]
    
    This is the output that we get

    \begin{verbatim}
LinAlgError        Traceback (most recent call last)
<ipython-input-8-4fb5f41eaf7c> in <module>()
----> 1 solve(A, b)
/home/john/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.pyc in solve(a, b, sym_pos, lower, overwrite_a, overwrite_b, debug, check_finite)
     97         return x
     98     if info > 0:
---> 99         raise LinAlgError("singular matrix")
    100     raise ValueError('illegal value in %d-th argument of internal gesv|posv'
LinAlgError: singular matrix
    \end{verbatim}

    What does this mean?  How can we fix it?

    Moral: We still need to understand the concepts

\end{frame}




\section{Vector Space}


\begin{frame}
    \frametitle{Vector Space}

    Recall that $\RR^N := $ set of all $N$-vectors

    \vspace{1em}

    An $N$-vector $\boldx$ is a tuple of $N$ real numbers:
    $$
    \boldx = (x_1, \ldots, x_N)
        \quad
        \text{ where } 
        \quad x_n \in \RR \text{ for each } n
    $$  

    \vspace{1em}

    We can also write $\boldx$ vertically, like so: 
    %
    \begin{equation*}
        \boldx = 
        \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            \vdots \\
            x_N
        \end{array}
        \right)
    \end{equation*}
    %




\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec.tex}}
        \caption{\label{f:vec} Visualization of vector $\boldx$ in $\RR^2$}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}

    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{vecs.pdf}}
        \caption{Three vectors in $\RR^2$ }
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    The vector of ones will be denoted $\boldone$ 
    
    %
    \begin{equation*}
        \boldone := 
        \left(
        \begin{array}{c}
            1 \\
            \vdots \\
            1
        \end{array}
        \right)
    \end{equation*}
    %

    Vector of zeros will be denoted $\boldzero$

    %
    \begin{equation*}
        \boldzero := 
        \left(
        \begin{array}{c}
            0 \\
            \vdots \\
            0
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}


\section{Linear Operations}

\begin{frame}
    \frametitle{Linear Operations}

    \vspace{1em}
    
    Two fundamental algebraic operations: 
    %
    \begin{enumerate}
        \item Vector addition 
        \item Scalar multiplication
    \end{enumerate}
    
    
    1. \navy{Sum} of $\boldx \in \RR^N$ and $\boldy \in \RR^N$ defined by
    
    \begin{equation*}
        \boldx + \boldy 
        :=: 
        \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            \vdots \\
            x_N
        \end{array}
        \right)
        +
        \left(
        \begin{array}{c}
             y_1 \\
             y_2 \\
            \vdots \\
             y_N
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{c}
            x_1 + y_1 \\
            x_2 + y_2 \\
            \vdots \\
            x_N + y_N
        \end{array}
        \right)
    \end{equation*}
    %


\end{frame}


\begin{frame}

    Example 1:

    \begin{equation*}
        \left(
        \begin{array}{c}
            1 \\
            2 \\
            3 \\
            4
        \end{array}
        \right)
        +
        \left(
        \begin{array}{c}
             2 \\
             4 \\
             6 \\
             8
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{c}
            3 \\
            6 \\
            9 \\
            12
        \end{array}
        \right)
    \end{equation*}
    %

    Example 2:

    \begin{equation*}
        \left(
        \begin{array}{c}
            1 \\
            2 \\
            3 \\
            4
        \end{array}
        \right)
        +
        \left(
        \begin{array}{c}
             1 \\
             1 \\
             1 \\
             1
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{c}
            2 \\
            3 \\
            4 \\
            5
        \end{array}
        \right)
    \end{equation*}
    %
\end{frame}

\begin{frame}
    

    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec_add.tex}}
        \caption{\label{f:vec_add} Vector addition}
       \end{center}
    \end{figure}



\end{frame}


\begin{frame}
    
    2. \navy{Scalar product} of $\alpha \in \RR$ and $\boldx \in \RR^N$ defined by
    
    \begin{equation*}
        \alpha \boldx 
        =
        \alpha \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            \vdots \\
            x_N
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{c}
            \alpha x_1 \\
            \alpha x_2 \\
            \vdots \\
            \alpha x_N
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}



\begin{frame}

    Example 1:

    \begin{equation*}
        0.5 
        \left(
        \begin{array}{c}
            1 \\
            2 \\
            3 \\
            4
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{c}
            0.5 \\
            1.0 \\
            1.5 \\
            2.0 
        \end{array}
        \right)
    \end{equation*}
    %

    Example 2:

    \begin{equation*}
        -1
        \left(
        \begin{array}{c}
            1 \\
            2 \\
            3 \\
            4
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{c}
            -1 \\
            -2 \\
            -3 \\
            -4
        \end{array}
        \right)
    \end{equation*}
    %
\end{frame}

\begin{frame}
    


    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec_scalar.tex}}
        \caption{\label{f:vec_scalar} Scalar multiplication}
       \end{center}
    \end{figure}


\end{frame}



\begin{frame}
    
    Subtraction performed element by element, analogous to addition 

    %
    \begin{equation*}
        \boldx - \boldy 
        :=
        \left(
        \begin{array}{c}
            x_1 - y_1 \\
            x_2 - y_2 \\
            \vdots \\
            x_N - y_N
        \end{array}
        \right)
    \end{equation*}
    %

    \vspace{2em}

    Def can be given in terms of addition and scalar multiplication:

    \begin{equation*}
      \boldx - \boldy := \boldx + (-1) \boldy      
    \end{equation*}

\end{frame}



\begin{frame}
    

    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec_minus.tex}}
        \caption{\label{f:vec_minus} Difference between vectors}
       \end{center}
    \end{figure}

\end{frame}




\begin{frame}[fragile]
    
    Incidentally, most high level numerical libraries treat vector addition
    and scalar multiplication in the same way --- elementwise

\begin{pythoncode}
    In [1]: import numpy as np

    In [2]: x = np.array((2, 4, 6))

    In [3]: y = np.array((10, 10, 10))

    In [4]: x + y  # Vector addition
    Out[4]: array([12, 14, 16])

    In [6]: 2 * x  # Scalar multiplication
    Out[6]: array([ 4,  8, 12])
\end{pythoncode}


\end{frame}



\begin{frame}

    A \navy{linear combination} of vectors $\boldx_1,\ldots, \boldx_K$ in $\RR^N$ 
    is a vector 
    %
    \begin{equation*}
        \boldy = \sum_{k=1}^K \alpha_k \boldx_k 
        =  \alpha_1 \boldx_1 + \cdots +  \alpha_K \boldx_K 
    \end{equation*}
    %
    where $\alpha_1,\ldots, \alpha_K$ are scalars

    \vspace{1em}

    \Eg

    %
    \begin{equation*}
        0.5 \left(
        \begin{array}{c}
            6.0 \\
            2.0 \\
            8.0
        \end{array}
        \right)
        +
        3.0 \left(
        \begin{array}{c}
             0 \\
             1.0 \\
             -1.0
        \end{array}
        \right)
        =
        \left(
        \begin{array}{c}
            3.0 \\
            4.0 \\
            1.0
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}






\begin{frame}
    \frametitle{Inner Product}
    
    The \navy{inner product} of two vectors $\boldx$ and $\boldy$ in $\RR^N$ is
    %
    \begin{equation*}
        \boldx'  \boldy :=
         \sum_{n=1}^N x_n y_n
    \end{equation*}
    %

    Example: $\boldx = (2, 3)$ and $\boldy = (-1, 1)$ implies that
    %
    \begin{equation*}
        \boldx'  \boldy 
        = 2 \times (-1) + 3 \times 1 
        = 1
    \end{equation*}
    %

    Example: $\boldx = (1/N) \boldone$ and $\boldy = (y_1, \ldots, y_N)$ implies 
    %
    \begin{equation*}
        \boldx'  \boldy 
        = \frac{1}{N} \sum_{n=1}^N y_n
    \end{equation*}
    %
\end{frame}


\begin{frame}[fragile]


    \begin{pythoncode}
In [1]: import numpy as np

In [2]: x = np.array((1, 2, 3, 4))

In [3]: y = np.array((2, 4, 6, 8))

In [6]: np.sum(x * y)  # Inner product
Out[6]: 60
    \end{pythoncode}
    
\end{frame}



\begin{frame}
    
    \Fact For any $\alpha, \beta \in \RR$ and any $\boldx, \boldy \in \RR^N$, the following 
    statements are true:
    %
    \begin{enumerate}
        \item $\boldx' \boldy = \boldy' \boldx$
        \item $(\alpha \boldx)' (\beta \boldy) =  \alpha \beta (\boldx' \boldy)$
        \item $\boldx' (\boldy + \boldz) =  \boldx' \boldy + \boldx' \boldz$
    \end{enumerate}
    %

    \vspace{1em}

    For example, item 2 is true because
    %
    \begin{equation*}
        (\alpha \boldx)' (\beta \boldy) 
        = \sum_{n=1}^N \alpha x_n \beta y_n
        = \alpha \beta \sum_{n=1}^N x_n y_n
        =  \alpha \beta (\boldx' \boldy)
    \end{equation*}

    \Ex Use above rules to show that 
    $(\alpha \boldy + \beta \boldz)'\boldx =  \alpha \boldx' \boldy + \beta \boldx' \boldz$

\end{frame}


\begin{frame}

    The next result is a generalization

    \vspace{1em}
    

    \Fact Inner products of linear combinations satisfy 
    %
    \begin{equation*}
      \left(
          \sum_{k=1}^K \alpha_k \boldx_k
      \right)' 
      \left(
          \sum_{j=1}^J \beta_j \boldy_j 
      \right)
      =
          \sum_{k=1}^K \sum_{j=1}^J \alpha_k \beta_j \boldx_k' \boldy_j 
    \end{equation*}
    %


\end{frame}







\section{Norms and Distance}

\begin{frame}
    \frametitle{Norms and Distance}

    The (Euclidean) \navy{norm} of $\boldx \in \RR^N$ is defined as
    %
    \begin{equation*}
        \| \boldx \| 
        := \sqrt{\boldx' \boldx } 
        = \left( \sum_{n=1}^N x_n^2 \right)^{1/2}
    \end{equation*}
    %

    Interpretation:
    %
    \begin{itemize}
        \item $\| \boldx \|$ represents the ``length'' of $\boldx$
            \vspace{0.5em}
        \item $\| \boldx - \boldy \|$ represents distance between $\boldx$ and $\boldy$
    \end{itemize}




\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.95}{\input{../tikzfigs/vec.tex}}
        \caption{\label{f:vec_rpt} Length of red line $= \sqrt{x_1^2 + x_2^2}
            =: \|\boldx\|$ }
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    $\| \boldx - \boldy \|$ represents distance between $\boldx$ and $\boldy$

    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec_minus.tex}}
       \end{center}
    \end{figure}

\end{frame}




\begin{frame}
    

    \Fact For any $\alpha \in \RR$ and any $\boldx, \boldy \in \RR^N$, the following 
    statements are true:
    %
    \begin{enumerate}
        \item $\| \boldx \| \geq 0$ and $\| \boldx \| = 0$ if and only if
            $\boldx = \boldzero$
            \vspace{1em}
        \item $\| \alpha \boldx \| = |\alpha| \| \boldx \|$
            \vspace{1em}
        \item $\| \boldx + \boldy \| \leq  \| \boldx \| + \| \boldy \|$
            (\navy{triangle inequality})
            \vspace{1em}
        \item $| \boldx' \boldy | \leq  \| \boldx \| \| \boldy \|$
            (\navy{Cauchy-Schwarz inequality})
    \end{enumerate}
    %

\end{frame}


\begin{frame}


    For example, let's show that $\| \boldx \| = 0 \iff \boldx = \boldzero$
    
    \vspace{1em}

    First let's assume that $\| \boldx \| = 0$ and show $\boldx = \boldzero$

    Since $\| \boldx \| = 0$ we have $\| \boldx \|^2 = 0$ and hence
    $\sum_{n=1}^N x^2_n = 0$

    That is $x_n = 0$ for all $n$, or, equivalently, $\boldx = \boldzero$

    \vspace{1em}
    
    Next let's assume that $\boldx = \boldzero$ and show $\| \boldx \| = 0$ 

    This is immediate from the definition of the norm

\end{frame}






\begin{frame}

    \Fact If $\boldx \in \RR^N$ is nonzero, then the solution to the optimization problem 
    %
    \begin{equation*}
        \max_{\boldy} \boldx'\boldy 
        \quad \text{ subject to }  \quad
        \boldy \in \RR^N \text{ and } \| \boldy \| = 1     
    \end{equation*}
    %
    is $\hboldx := \boldx / \| \boldx \|$

    \vspace{-1em}


    \begin{figure}
       \begin{center}
        \scalebox{.65}{\input{../tikzfigs/max_inner_prod.tex}}
       \end{center}
    \end{figure}


\end{frame}


\begin{frame}
    
    Proof: Fix nonzero $\boldx \in \RR^N$
    
    Let $\hboldx := \boldx / \| \boldx \| := \alpha \boldx$ when $\alpha := 1/\|\boldx\|$

    Evidently $\| \hboldx \| = 1$
    
    Pick any other $\boldy \in \RR^N$ satisfying $\| \boldy \| = 1$
    
    The Cauchy-Schwarz inequality yields
    %
    \begin{equation*}
        \boldy' \boldx
        \leq |\boldy' \boldx|
        \leq \| \boldy \| \| \boldx \|
        = \| \boldx \|
        = \frac{\boldx' \boldx}{ \| \boldx \| }
        = \hboldx' \boldx
    \end{equation*}
    %
    Hence $\hboldx$ is the maximizer, as claimed

\end{frame}




\section{Span}

\begin{frame}
    \frametitle{Span}

    
    Let $X \subset \RR^N$ be any nonempty set

    \vspace{1em}

    Set of all possible linear combinations of elements of $X$ is 
    called the \navy{span} of $X$, denoted by $\Span(X)$

    \vspace{1em}

    For finite $X := \{\boldx_1,\ldots, \boldx_K\}$ the span can be expressed
    as 
    % 
    \begin{equation*}
        \Span(X):= \left\{ \text{ all } \sum_{k=1}^K \alpha_k \boldx_k 
        \text{ such that }
         (\alpha_1,\ldots, \alpha_K) \in \RR^K \right\}
    \end{equation*}
    %

    We are mainly interested in the span of finite sets...

\end{frame}

\begin{frame}

    \begin{example}
        
    Let's start with the span of a singleton
    
    Let $X = \{ \boldone \} \subset \RR^2$, where $\boldone := (1,1)$  
    
    The span of $X$ is all vectors of the form 


    
    $$
    \alpha \boldone 
    =
    \left(
    \begin{array}{c}
        \alpha \\
        \alpha
    \end{array}
    \right)
    \quad \text{ with } \quad \alpha \in \RR  
    $$
    
    Constitutes a line in the plane that passes through

    \begin{itemize}
        \item the vector $\boldone$  (set $\alpha = 1$)
        \item the origin $\boldzero$ (set $\alpha = 0$)
    \end{itemize}

    \end{example}

\end{frame}


\begin{frame}

    \begin{figure}
       \centering
        \scalebox{.4}{\includegraphics{span_of_one_vec.pdf}}
        \caption{The span of $\boldone := (1,1)$ in $\RR^2$}
    \end{figure}

\end{frame}


\begin{frame}

    \begin{example}
        

    Let $\boldx_1 = (3, 4, 2)$ and let $\boldx_2 = (3, -4, 0.4)$

    By definition, the span is all vectors of the form


    \begin{equation*}
        \boldy = 
        \alpha \left(
        \begin{array}{c}
            3 \\
            4 \\
            2
        \end{array}
        \right)
        +
        \beta \left(
        \begin{array}{c}
             3 \\
             -4 \\
             0.4
        \end{array}
        \right)
        \quad \text{where} \quad
        \alpha, \beta \in \RR
    \end{equation*}
    %
    
    It turns out to be a plane that passes through

    \begin{itemize}
        \item the vector $\boldx_1$
        \item the vector $\boldx_2$
        \item the origin $\boldzero$
    \end{itemize}

    \end{example}

\end{frame}


\begin{frame}

   \begin{figure}
       \scalebox{.42}{\includegraphics[trim=110 0 0 0, clip]{span_plane.pdf}}
       \caption{\label{f:span_plane} Span of $\boldx_1, \boldx_2$}
   \end{figure}

\end{frame}


\begin{frame}
    
    \Fact If $X \subset Y$, then $\Span(X) \subset \Span(Y)$

    To see this, pick any nonempty $X \subset Y \subset \RR^N$

    Letting $\boldz \in \Span(X)$, we have

    \begin{equation*}
        \boldz = \sum_{k=1}^K \alpha_k \boldx_k 
        \text{ for some }
        \boldx_1, \ldots, \boldx_K \in X, \; 
        \alpha_1, \ldots, \alpha_K \in \RR
    \end{equation*}
    %

    Since $X \subset Y$, each $\boldx_k$ is also in $Y$, giving us

    \begin{equation*}
        \boldz = \sum_{k=1}^K \alpha_k \boldx_k 
        \text{ for some }
        \boldx_1, \ldots, \boldx_K \in Y, \; 
        \alpha_1, \ldots, \alpha_K \in \RR
    \end{equation*}
    %

    Hence $\boldz \in \Span(Y)$

\end{frame}

\begin{frame}
    
    Let $Y$ be any subset of $\RR^N$, and let $X:= \{\boldx_1,\ldots, \boldx_K\}$ 
    
    \vspace{1em}

    If $Y \subset \Span(X)$, we say that the vectors in $X$ \navy{span the set} $Y$
    
    \vspace{1em}

    Alternatively, we say that $X$ is a \navy{spanning set} for $Y$

    \vspace{1em}
    
    A nice situation: $Y$ is large but $X$ is small
    
    \vspace{1em}

    $\implies$ large set $Y$ ``described'' by the small number of vectors in $X$
    

\end{frame}


\begin{frame}
    
    \begin{example}
        
    
    Consider the vectors $\{\bolde_1, \ldots, \bolde_N\} \subset \RR^N$, where

    \begin{equation*}
        \bolde_1 := 
        \left(
        \begin{array}{c}
            1 \\
            0 \\
            \vdots \\
            0
        \end{array}
        \right),
        \quad 
        \bolde_2 := 
        \left(
        \begin{array}{c}
            0 \\
            1 \\
            \vdots \\
            0
        \end{array}
        \right),
        \; 
        \cdots,
        \;
        \bolde_N := 
        \left(
        \begin{array}{c}
            0 \\
            0 \\
            \vdots \\
            1
        \end{array}
        \right)
    \end{equation*}
    %

    \vspace{1em}

    That is, $\bolde_n$ has all zeros except for a $1$ as the $n$-th element
    
    \vspace{1em}

    Vectors $\bolde_1, \ldots, \bolde_N$ called the \navy{canonical basis vectors} of $\RR^N$

    \end{example}

\end{frame}

\begin{frame}
    

    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec_canon.tex}}
        \caption{\label{f:vec_canon} Canonical basis vectors in $\RR^2$}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \Fact The span of $\{\bolde_1, \ldots, \bolde_N\}$ is equal
    to all of $\RR^N$
    
    Proof for $N=2$: 
    
    Pick any $\boldy \in \RR^2$ 
    
    We have
    %
    \begin{multline*}
        \boldy 
        :=
        \left(
        \begin{array}{c}
            y_1 \\
            y_2
        \end{array}
        \right)
        =
        \left(
        \begin{array}{c}
            y_1 \\
            0
        \end{array}
        \right)
        +
        \left(
        \begin{array}{c}
            0 \\
            y_1
        \end{array}
        \right)
        \\
        =
        y_1
        \left(
        \begin{array}{c}
            1 \\
            0
        \end{array}
        \right)
        +
        y_2
        \left(
        \begin{array}{c}
            0 \\
            1
        \end{array}
        \right)
        = y_1 \bolde_1 + y_2 \bolde_2
    \end{multline*}
    %
    Thus, $\boldy \in \Span \{\bolde_1, \bolde_2\}$ 
    
    Since $\boldy$ arbitrary, we have shown that $\Span \{\bolde_1,
    \bolde_2\} = \RR^2$

\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.95}{\input{../tikzfigs/vec_canon_x.tex}}
        \caption{\label{f:vec_canon2} Canonical basis vectors in $\RR^2$}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}

    \Eg Consider the set 
    %
    \begin{equation*}
        P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}
    \end{equation*}
    %

    Graphically, $P =$ flat plane in $\RR^3$, where height
    coordinate $=0$

    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{flat_plane_no_vecs.pdf}}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}

    Let $\bolde_1$ and $\bolde_2$ be the canonical basis vectors in $\RR^3$

    \underline{Claim}: $\Span\{\bolde_1, \bolde_2\} = P$ 
    
    Proof:

    Let $\boldx = (x_1, x_2, 0)$ be any element of $P$
    
    We can write $\boldx$ as 
    
    %
    \begin{equation*}
        \boldx = 
        \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            0
        \end{array}
        \right)
        =
        x_1
        \left(
        \begin{array}{c}
            1 \\
            0 \\
            0
        \end{array}
        \right)
        + 
        x_2
        \left(
        \begin{array}{c}
            0 \\
            1 \\
            0
        \end{array}
        \right)
        = x_1 \bolde_1 + x_2 \bolde_2
    \end{equation*}
    %

    In other words, $P \subset \Span\{\bolde_1, \bolde_2\}$

    Conversely (check it) we have $\Span\{\bolde_1, \bolde_2\} \subset P$ 


\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{flat_plane_e_vecs.pdf}}
       \end{center}
       \caption{$\Span\{\bolde_1, \bolde_2\} = P$}
    \end{figure}


\end{frame}


\section{Linear Subspaces}





\begin{frame}
    \frametitle{Linear Subspaces}
    
    A nonempty $S \subset \RR^N$ called a \navy{linear
    subspace} of $\RR^N$ if
    %
    \begin{equation*}
        \boldx, \boldy \in S \; \text{ and }  \;\alpha, \beta \in \RR
        \quad \implies \quad
        \alpha \boldx + \beta \boldy \in S 
    \end{equation*}

    In other words, $S \subset \RR^N$ is ``closed" under vector addition
    and scalar multiplication

    \vspace{1em}

    Note: Sometimes we just say \navy{subspace}...

    \vspace{1em}

    \Eg $\RR^N$ itself is a linear subspace of $\RR^N$

    
\end{frame}





\begin{frame}

    \begin{example}

    Fix $\bolda \in \RR^N$ and let $A := \setntn{\boldx \in \RR^N}{\bolda' \boldx  = 0}$

    \vspace{1em}
    
    \Fact The set $A$ is a linear subspace of $\RR^N$

    \vspace{1em}

    Proof: Let $\boldx, \boldy \in A$ and let $\alpha, \beta \in \RR$
    
    We must show that $\boldz := \alpha \boldx + \beta \boldy \in A$ 
    
    Equivalently, that $\bolda' \boldz = 0$
    
    True because
    %
    $$
    \bolda' \boldz =
    \bolda' (\alpha \boldx + \beta \boldy) = \alpha
    \bolda' \boldx + \beta \bolda' \boldy = 0 + 0 = 0
    $$
        
    \end{example}
    
\end{frame}



\begin{frame}
    
    \Fact If $Z$ is a nonempty subset of $\RR^N$, then $\Span(Z)$ is a linear
    subspace

    Proof: If $\boldx, \boldy \in \Span(Z)$, then $\exists$ vectors $\boldz_k$ in $Z$ 
    and scalars $\gamma_k$ and $\delta_k$ such that 
    %
    \begin{equation*}
        \boldx = \sum_{k=1}^K \gamma_k \boldz_k
        \quad \text{and} \quad
        \boldy = \sum_{k=1}^K \delta_k \boldz_k
    \end{equation*}
    %
    \begin{equation*}
        \fore
        \alpha \boldx = \sum_{k=1}^K \alpha \gamma_k \boldz_k
        \quad \text{and} \quad
        \beta \boldy = \sum_{k=1}^K \beta \delta_k \boldz_k 
    \end{equation*}
    %
    \begin{equation*}
        \fore
        \alpha \boldx + \beta \boldy 
        = \sum_{k=1}^K (\alpha \gamma_k + \beta \delta_k) \boldz_k  
    \end{equation*}
    %

    This vector clearly lies in $\Span(Z)$

\end{frame}


\begin{frame}
    
    \Fact If $S$ and $S'$ are two linear subspaces of $\RR^N$, then $S
    \cap S'$ is also a linear subspace of $\RR^N$.

    \vspace{1em}

    Proof: Let $S$ and $S'$ be two linear subspaces of $\RR^N$

    Fix $\boldx, \boldy \in S \cap S'$ and $\alpha, \beta \in \RR$

    We claim that $\boldz := \alpha \boldx + \beta \boldy \in S \cap S'$

    \begin{itemize}
        \item Since $\boldx, \boldy \in S$ and $S$ is a linear subspace we have $\boldz \in S$
            \vspace{0.5em}
        \item Since $\boldx, \boldy \in S'$ and $S'$ is a linear subspace we have $\boldz \in S'$
    \end{itemize}

    Therefore $\boldz \in S \cap S'$
    
\end{frame}




\begin{frame}
    
    Other examples of linear subspaces
    %    
    \begin{itemize}
        \item The singleton $\{\boldzero\}$ in $\RR^N$
            \vspace{1em}
        \item Lines through the origin in $\RR^2$ and $\RR^3$
            \vspace{1em}
        \item Planes through the origin in $\RR^3$
    \end{itemize}

            \vspace{1em}

    \Ex Let $S$ be a linear subspace of $\RR^N$.  Show that

    \begin{enumerate}
        \item $\boldzero \in S$
        \item If $X \subset S$, then $\Span(X) \subset S$
        \item $\Span(S) = S$
    \end{enumerate}

\end{frame}



\section{Linear Independence}

\begin{frame}
    \frametitle{Linear Independence}

    Important applied questions
    \vspace{1em}

    \begin{itemize}
        \item When is a matrix invertible?
        \item When do regression arguments suffer from collinearity?
        \item When does a set of linear equations have a solution?
        \item When is that solution unique?
        \item How can we approximate complex functions parsimoniously?
        \item What is the rank of a matrix?
    \end{itemize}

    \vspace{1em}

    All of these questions closely related to linear independence 

\end{frame}





\begin{frame}
    \frametitle{Definition}

    A nonempty collection of vectors $X := \{\boldx_1,\ldots, \boldx_K\}
    \subset \RR^N$ is called \navy{linearly independent} if
    %
    \begin{equation*}
        \sum_{k=1}^K \alpha_k \boldx_k
         = \boldzero 
        \; \implies \;
        \alpha_1 = \cdots = \alpha_K = 0
    \end{equation*}

    \vspace{1em}

    As we'll see, linear independence of a set of vectors determines how large
    a space they span
    
    Loosely speaking, linearly independent sets span large spaces


\end{frame}


\begin{frame}
    
    \Eg Let $\boldx := (1, 2)$ and $\boldy := (-5, 3)$
        
    The set $X = \{\boldx, \boldy\}$ is linearly independent in $\RR^2$

        Indeed, suppose $\alpha_1$ and $\alpha_2$ are scalars with

        \begin{equation*}
            \alpha_1
            \left(
            \begin{array}{c}
                1 \\
                2
            \end{array}
            \right)
            + 
            \alpha_2
            \left(
            \begin{array}{c}
                -5 \\
                3
            \end{array}
            \right)
            =
            \boldzero
        \end{equation*}
        %
        Equivalently,
        %
        \begin{align*}
            \alpha_1 & = 5 \alpha_2
            \\
            2 \alpha_1 & = -3 \alpha_2
        \end{align*}

        Then $2(5\alpha_2) = 10 \alpha_2 = -3 \alpha_2$, implying $\alpha_2 = 0$
        and hence $\alpha_1 = 0$ 


\end{frame}

\begin{frame}
    
    \begin{example}
        
    The set of canonical basis vectors $\{\bolde_1, \ldots, \bolde_N\}$
    is linearly independent in $\RR^N$

    Proof: Let $\alpha_1, \ldots, \alpha_N$ be coefficients such that
    $\sum_{k=1}^N \alpha_k \bolde_k = \boldzero$

    \vspace{1em}

    Then
    %
    \begin{equation*}
        \left(
        \begin{array}{c}
            \alpha_1 \\
            \alpha_2 \\
            \vdots \\
            \alpha_N
        \end{array}
        \right)
        = \sum_{k=1}^N \alpha_k \bolde_k 
        = \boldzero
        =
        \left(
        \begin{array}{c}
            0 \\
            0 \\
            \vdots \\
            0
        \end{array}
        \right)
    \end{equation*}

    \vspace{1em}

    In particular, $\alpha_k = 0$ for all $k$
    
    Hence  $\{\bolde_1, \ldots, \bolde_N\}$ linearly independent

    \end{example}

\end{frame}





\begin{frame}

    As a first step to better understanding linear independence let's look at
    some equivalences

    \vspace{0.5em}

    Take $X := \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$


    \Fact For $K > 1$ all of following statements are equivalent
    %
    \begin{enumerate}
        \item $X$ is linearly independent
            \vspace{0.6em}
        \item No $\boldx_i \in X$ can be written as linear combination of the others
            \vspace{0.6em}
        \item $X_0 \subsetneq X \implies \Span(X_0) \subsetneq \Span(X)$
    \end{enumerate}
    %

    \vspace{1em}


    \begin{itemize}
        \item Here $X_0 \subsetneq X$ means $X_0 \subset X$ and $X_0 \not= X$
            \vspace{0.5em}
        \item We say that $X_0$ is a \navy{proper subset} of $X$ 
    \end{itemize}

\end{frame}



\begin{frame}

    As an exercise, let's show that the first two statements are equivalent

    The first is
    %
    \begin{equation}
        \label{eq:cli} 
        \tag{$\star$}
        \sum_{k=1}^K \alpha_k \boldx_k
         = \boldzero 
        \; \implies \;
        \alpha_1 = \cdots = \alpha_K = 0
    \end{equation}

    The second is
    %
    \begin{equation}
        \label{eq:cli2} 
        \tag{$\star\star$}
        \text{no $\boldx_i \in X$ can be written as linear combination of others}
    \end{equation}

    We now show that
    %
    \begin{itemize}
        \item \eqref{eq:cli} $\implies$ \eqref{eq:cli2}, and
        \item \eqref{eq:cli2} $\implies$ \eqref{eq:cli}
    \end{itemize}
        

\end{frame}



\begin{frame}

    To show that \eqref{eq:cli} $\implies$ \eqref{eq:cli2} let's suppose to the contrary that

    \begin{enumerate}
        \item $\sum_{k=1}^K \alpha_k \boldx_k = \boldzero \implies \alpha_1 = \cdots = \alpha_K = 0$ 
        \item and yet some $\boldx_i$ can be written as a linear combination
            of the other elements of $X$
    \end{enumerate}
    
    \vspace{1em}

    In particular, suppose that
    $$
        \boldx_i = \sum_{k \not= i} \alpha_k \boldx_k 
    $$
    Then, rearranging,
    %
    $$
        \alpha_1 \boldx_1 + \cdots + (-1) \boldx_i 
        + \cdots + \alpha_K \boldx_K = \boldzero
    $$

    This contradicts 1., and hence \eqref{eq:cli2} holds

\end{frame}


\begin{frame}

    To show that \eqref{eq:cli2} $\implies$ \eqref{eq:cli} let's suppose to
    the contrary that 
    
    \begin{enumerate}
        \item no $\boldx_i$ can be written as a linear combination of others
        \item and yet $\exists$ $\alpha_1, \ldots, \alpha_K$ not all zero with $\alpha_1 \boldx_1 + \cdots + \alpha_K \boldx_K = \boldzero$ 
    \end{enumerate}

    Suppose without loss of generality that $\alpha_1 \not= 0$

    (Similar argument works for any $\alpha_j$)

    \vspace{1em}

    Then
    %
    $$
        \boldx_1 = \frac{\alpha_2}{-\alpha_1} \boldx_2 
            + \cdots + \frac{\alpha_K}{-\alpha_1} \boldx_K 
    $$

    \vspace{1em}

    This contradicts 1., and hence \eqref{eq:cli} holds


\end{frame}


\begin{frame}

    Let's show one more part of the proof as an exercise:
    %
    $$
     X \text{ linearly independent } 
     \implies
     \text{ proper subsets of $X$ have smaller span}
    $$

    Proof: Suppose to the contrary that 
    %
    \begin{enumerate}
        \item $X$ is linearly independent,
        \item $X_0 \subsetneq X$ and yet
        \item $\Span(X_0) = \Span(X)$
    \end{enumerate}

    Let $\boldx_j$ be in $X$ but not $X_0$
    
    Since $\boldx_j \in \Span(X)$, we also have $\boldx_j \in \Span(X_0)$

    But then $\boldx_j$ can be written as a linear combination of the other
    elements of $X$
    
    This contradicts linear independence

\end{frame}







\begin{frame}

    \Eg Dropping any of the canonical basis vectors reduces span

    Consider the $N=2$ case
    
    We know that $\Span \{\bolde_1, \bolde_2\} =$ all of $\RR^2$

    Removing either element of $\Span \{\bolde_1, \bolde_2\}$ reduces the span to a line

    \begin{figure}
       \begin{center}
        \scalebox{.9}{\input{../tikzfigs/vec_h_axis.tex}}
        \caption{\label{f:vec_h_axis} The span of $\{\bolde_1\}$ alone is the horizonal axis}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}

    \Eg As another visual example of linear independence, consider the pair

    $$
    \boldx_1 =
        \begin{pmatrix}
            3 \\
            4 \\
            2
        \end{pmatrix}
    \quad \text{and} \quad
    \boldx_2 =
        \begin{pmatrix}
            3 \\
            -4 \\
            1
        \end{pmatrix}
    $$

    \vspace{2em}

    The span of this pair is a plane in $\RR^3$

    \vspace{2em}

    But if we drop either one the span reduces to a line

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{nonredundant1.pdf}}
           \caption{The span of $\{\boldx_1, \boldx_2\}$ is a plane}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{nonredundant2.pdf}}
           \caption{The span of $\{\boldx_1\}$ alone is a line}
       \end{center}
    \end{figure}

\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{nonredundant3.pdf}}
           \caption{The span of $\{\boldx_2\}$ alone is a line}
       \end{center}
    \end{figure}

\end{frame}






\begin{frame}
    \frametitle{Linear Dependence}

    If $X$ is not linearly independent then it is called \navy{linearly dependent}

    We saw above that 
    
    \begin{center}
    linear independence $\iff$ dropping any elements reduces span
    \end{center}

    Hence $X$ is linearly dependent when some
    elements can be removed without changing $\Span(X)$
    
    That is,
    %
    $$\exists \, X_0 \subsetneq X \; \st \; \Span(X_0) = \Span(X)$$
    


\end{frame}









\begin{frame}

    
    \Eg As an example with redundacy, consider $\{\boldx_1, \boldx_2\} \subset \RR^2$ where 
    %
    \begin{itemize}
        \item $\boldx_1 = \bolde_1 := (1, 0)$
        \item $\boldx_2 = (-2, 0)$
    \end{itemize}
    

    \begin{figure}
       \begin{center}
        \scalebox{.9}{\input{../tikzfigs/vec_noncanon.tex}}
        \caption{\label{f:vec_noncanon} The vectors $\boldx_1$ and $\boldx_2$}
       \end{center}
    \end{figure}

\end{frame}




\begin{frame}
    

    We claim that $\Span \{\boldx_1, \boldx_2\} = \Span\{\boldx_1\}$

    Proof: $\Span \{\boldx_1\} \subset \Span\{\boldx_1, \boldx_2\}$ is clear (why?)
    
    To see the reverse, pick any $\boldy \in \Span \{\boldx_1, \boldx_2\}$ 
    
    By definition, 
    $$
        \exists \;
        \alpha_1,\alpha_2 \; \st \;
        \boldy 
        = \alpha_1 \boldx_1 + \alpha_2 \boldx_2
        =
        \alpha_1 
        \begin{pmatrix}
            1 \\
            0
        \end{pmatrix}
        + \alpha_2 
        \begin{pmatrix}
            -2 \\
            0
        \end{pmatrix}
    $$
    %
    \begin{equation*}
        \fore 
        \boldy 
        = \alpha_1 
        \begin{pmatrix}
            1 \\
            0
        \end{pmatrix}
        - 2 \alpha_2 
        \begin{pmatrix}
            1 \\
            0
        \end{pmatrix}
        = (\alpha_1 - 2 \alpha_2)
        \begin{pmatrix}
            1 \\
            0
        \end{pmatrix}
        = (\alpha_1  -2 \alpha_2 ) \boldx_1 
    \end{equation*}

    The right hand side is clearly in $\Span \{\boldx_1\}$

    Hence $\Span \{\boldx_1, \boldx_2\} \subset \Span \{\boldx_1\}$ as claimed
    
\end{frame}




\section{Implications of Independence}

\begin{frame}
    \frametitle{Implications of Independence}

    Let $X := \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$

    \vspace{1em}

    \Fact If $X$ is linearly independent, then $X$ does not contain $\boldzero$

    \Ex Prove it
    
    \Fact If $X$ is linearly independent, then every subset of $X$ is linearly independent 
    \vspace{1em}
    
    Sketch of proof: Suppose for example that $\{\boldx_1,\ldots,
    \boldx_{K-1}\} \subset X$ is linearly dependent

    Then $\exists \; \alpha_1, \ldots, \alpha_{K-1}$ not all zero with 
    $\sum_{k=1}^{K-1} \alpha_k \boldx_k = \boldzero$
    
    Setting $\alpha_K =0$ we can write this as $\sum_{k=1}^K \alpha_k \boldx_k = \boldzero$
    
    Not all scalars zero so contradicts linear independence of $X$ 

\end{frame}







\begin{frame}
    
    \Fact If $X:= \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$ is linearly
    independent and $\boldz$ is an $N$-vector not in $\Span(X)$, then $X \cup
    \{ \boldz \}$ is linearly independent 

    \vspace{0.8em}

    Proof: Suppose to the contrary that $X \cup \{ \boldz \}$ is linearly
    dependent:
    %
    \begin{equation}
        \label{eq:m}
        \exists \; \alpha_1, \ldots, \alpha_K, \beta
        \text{ not all zero with }
        \sum_{k=1}^K \alpha_k \boldx_k + \beta \boldz = \boldzero
    \end{equation}

    If $\beta=0$, then by \eqref{eq:m} we have $\sum_{k=1}^K \alpha_k \boldx_k = \boldzero$ and 
    $\alpha_k \not= 0$ for some $k$, a contradiction

    
    If $\beta \not=0$, then by \eqref{eq:m} we have 
    %
    $$
    \boldz = \sum_{k=1}^K \frac{-\alpha_k}{\beta} \boldx_k  
    $$
    Hence $\boldz \in \Span(X)$ --- contradiction



\end{frame}



\begin{frame}
    \frametitle{Unique Representations}

    Let 
    % 
    \begin{itemize}
        \item $X := \{\boldx_1,\ldots,\boldx_K\} \subset \RR^N$
            \vspace{0.6em}
        \item $\boldy \in \RR^N$
    \end{itemize}
    

    We know that if $\boldy \in \Span(X)$, then exists representation
    %
    \begin{equation*}
        \boldy = \sum_{k=1}^K \alpha_k \boldx_k
    \end{equation*}

    But when is this representation unique?

    \vspace{1em}

    Answer: When $X$ is linearly independent
    
\end{frame}



\begin{frame}

        
    \Fact  If $X = \{\boldx_1,\ldots, \boldx_K\} \subset \RR^N$ is linearly independent and
    $\boldy \in \RR^N$, then there is at most one set of scalars $\alpha_1,\ldots,\alpha_K$ such
    that $\boldy = \sum_{k=1}^K \alpha_k \boldx_k$

    \vspace{1em}

    Proof: Suppose there are two such sets of scalars
    
    That is,
        %
        \begin{equation*}
            \exists \;
            \alpha_1, \ldots, \alpha_K
            \text{ and } \beta_1, \ldots, \beta_K
            \; \st \; 
            \boldy 
            = \sum_{k=1}^K \alpha_k \boldx_k
            = \sum_{k=1}^K \beta_k \boldx_k
        \end{equation*}
        %
        \begin{equation*}
            \fore \sum_{k=1}^K (\alpha_k - \beta_k) \boldx_k = \boldzero
        \end{equation*} 

        \begin{equation*}
            \fore
            \alpha_k = \beta_k 
            \quad \text{for all} \quad k
        \end{equation*}
        
\end{frame}



\section{Span and Independence}

\begin{frame}
    \frametitle{Exchange Lemma}

    Here's one of the most fundamental results in linear algebra

    \Fact (Exchange lemma) If 
    %
    \begin{enumerate}
        \item $S$ is a linear subspace of $\RR^N$
        \item $S$ is spanned by $K$ vectors,
    \end{enumerate}
    %
    then any linearly independent subset of $S$ has at most $K$
        vectors

    \vspace{1em}

    Proof: Omitted

    \vspace{1em}

    \Eg If $X := \{\boldx_1, \boldx_2, \boldx_3\} \subset \RR^2$
         then $X$ is linearly dependent


     \begin{itemize}
         \item because $\RR^2$ is spanned by the two vectors $\bolde_1, \bolde_2$
     \end{itemize}

\end{frame}


\begin{frame}

    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{vecs.pdf}}
        \caption{Must be linearly dependent}
       \end{center}
    \end{figure}

\end{frame}




\begin{frame}

    \begin{example}
        

    Recall the plane
    %
    \begin{equation*}
        P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}
    \end{equation*}
    %

    \begin{itemize}
        \item flat plane in $\RR^3$ where height coordinate $=$ zero  
    \end{itemize}

    We showed before that $\Span\{\bolde_1, \bolde_2\} = P$ for 
    
    \begin{equation*}
        \bolde_1 := 
        \left(
        \begin{array}{c}
            1 \\
            0 \\
            0
        \end{array}
        \right),
        \quad 
        \bolde_2 := 
        \left(
        \begin{array}{c}
            0 \\
            1 \\
            0
        \end{array}
        \right)
    \end{equation*}
    %
    
    Therefore any three vectors lying in $P$ are linearly dependent

    \end{example}

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.36}{\includegraphics{flat_plane.pdf}}
           \caption{Any three vectors in $P$ are linearly dependent}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}
    \frametitle{When Do $N$ Vectors Span $\RR^N$?}

    In general, linearly independent vectors have a relatively ``large" span

    \begin{itemize}
        \item No vector is redundant, so each contributes to the span
    \end{itemize}

    This helps explain the following fact:

    Let $X := \{ \boldx_1, \ldots, \boldx_N \}$ be any $N$ vectors in $\RR^N$

    \Fact $\Span(X) = \RR^N$ if and only if $X$ is linearly independent
            
    \vspace{2em}
    
    \Eg The vectors $\boldx = (1, 2)$ and $\boldy = (-5, 3)$ span $\RR^2$

    \begin{itemize}
        \item We already showed $\{\boldx, \boldy\}$ is linearly independent
    \end{itemize}
    


\end{frame}

\begin{frame}


    Let's start with the proof that 

    \begin{center}
        $X= \{ \boldx_1, \ldots, \boldx_N \}$ linearly independent $\implies$ $\Span(X) = \RR^N$
    \end{center}
    
    Seeking a contradiction, suppose that 
    
    \begin{enumerate}
        \item $X $ is linearly independent 
        \item and yet $\exists \, \boldz \in \RR^N$ with $\boldz \notin \Span(X)$ 
    \end{enumerate}
    

    But then $X \cup \{\boldz\} \subset \RR^N$ is linearly independent (why?)

    This set has $N+1$ elements 

    And yet $\RR^N$ is spanned by the $N$ canonical basis vectors
    
    Contradiction (of what?)

\end{frame}


\begin{frame}


    Next let's show the converse

    \begin{center}
    $\Span(X) = \RR^N$
    $\implies$ 
    $X= \{ \boldx_1, \ldots, \boldx_N \}$ linearly independent
    \end{center}
    
    Seeking a contradiction, suppose that 
    
    \begin{enumerate}
        \item $\Span(X) = \RR^N$ 
        \item and yet $X$ is linearly dependent 
    \end{enumerate}
    
    Since $X$ not independent, $\exists X_0 \subsetneq X$ with $\Span(X_0) =
    \Span(X)$

    But by 1 this implies that $\RR^N$ is spanned by $K < N$ vectors

    But then the $N$ canonical basis vectors must be linearly dependent
    
    Contradiction 

\end{frame}



\section{Bases}


\begin{frame}
    \frametitle{Bases}
    
    Let $S$ be a linear subspace of $\RR^N$ 
    
    A set of vectors $B := \{\boldb_1, \ldots, \boldb_K\} \subset S$ is
    called a \navy{basis of $S$} if
    %
    \begin{enumerate}
        \item $B$ is linearly independent
        \item $\Span(B) = S$
    \end{enumerate}

    \vspace{1em}

    \Eg Canonical basis vectors form a basis of $\RR^N$

    Indeed, if $E := \{\bolde_1, \ldots, \bolde_N\} \subset \RR^N$, then

    \begin{itemize}
        \item $E$ is linearly independent -- we showed this earlier
            \vspace{0.3em}
        \item $\Span(E) = \RR^N$ -- we showed this earlier
    \end{itemize}

\end{frame}



\begin{frame}

    \begin{example}
        

    Recall the plane
    %
    \begin{equation*}
        P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}
    \end{equation*}
    %

    We showed before that $\Span\{\bolde_1, \bolde_2\} = P$ for 
    
    \begin{equation*}
        \bolde_1 := 
        \left(
        \begin{array}{c}
            1 \\
            0 \\
            0
        \end{array}
        \right),
        \quad 
        \bolde_2 := 
        \left(
        \begin{array}{c}
            0 \\
            1 \\
            0
        \end{array}
        \right)
    \end{equation*}
    %
    
    Moreover, $\{\bolde_1, \bolde_2\}$ is linearly independent (why?)

    \vspace{1em}

    Hence $\{\bolde_1, \bolde_2\}$ is a basis for $P$

    \end{example}

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{flat_plane_e_vecs.pdf}}
       \end{center}
       \caption{The pair $\{\bolde_1, \bolde_2\}$ form a basis for $P$}
    \end{figure}


\end{frame}




\begin{frame}
    
    What are the implications of $B$ being a basis of $S$?

    In short, every element of $S$ can be represented uniquely from the
    smaller set $B$

    \vspace{1em}

    In more detail:

    \begin{itemize}
        \item $B$ spans $S$ and, by linear independence, every element is
            needed to span $S$ --- a ``minimal" spanning set
            \vspace{0.5em}
        \item Since $B$ spans $S$, every $\boldy$ in $S$ can be represented as
            a linear combination of the basis vectors
            \vspace{0.5em}
        \item By independence, this representation is unique
    \end{itemize}

\end{frame}


\begin{frame}

    It's obvious given the definition that

    \Fact If $B \subset \RR^N$ is linearly independent, then $B$ is a basis of
    $\Span(B)$

    \Eg Let $B := \{\boldx_1, \boldx_2\}$ where 

    $$
    \boldx_1 =
        \begin{pmatrix}
            3 \\
            4 \\
            2
        \end{pmatrix}
    \quad \text{and} \quad
    \boldx_2 =
        \begin{pmatrix}
            3 \\
            -4 \\
            1
        \end{pmatrix}
    $$

    We saw earlier that 

    \begin{itemize}
        \item $S := \Span(B)$ is the plane in $\RR^3$ passing through
            $\boldx_1$, $\boldx_2$ and $\boldzero$
        \item $B$ is linearly independent in $\RR^3$  (dropping either reduces span)
    \end{itemize}

    Hence $B$ is a basis for the plane $S$ 

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{nonredundant1.pdf}}
           \caption{The pair $\{\boldx_1, \boldx_2\}$ is a basis of its span}
       \end{center}
    \end{figure}

\end{frame}





\begin{frame}
    \frametitle{Fundamental Properties of Bases}
    
    \Fact If $S$ is a linear subspace of $\RR^N$ distinct from $\{\boldzero\}$, then 
        %
        \begin{enumerate}
            \item $S$ has at least one basis, and
            \item every basis of $S$ has the same number of elements
        \end{enumerate}

    \vspace{1em}


    Proof of part 2: Let $B_i$ be a basis of $S$ with $K_i$ elements, $i=1, 2$

    By definition, $B_2$ is a linearly independent subset of $S$
     
    Moreover, $S$ is spanned by the set $B_1$, which has $K_1$ elements
     
     Hence $K_2 \leq K_1$ 
     
     Reversing the roles of $B_1$ and $B_2$ gives $K_1 \leq K_2$

\end{frame}


\begin{frame}
    \frametitle{Dimension}

    Let $S$ be a linear subspace of $\RR^N$

    We now know that every basis of $S$ has the same number of elements

    This common number is called the \navy{dimension} of $S$

    \vspace{1em}

    \Eg $\RR^N$ is $N$ dimensional because the $N$ canonical basis vectors
    form a basis

    \Eg $P := \setntn{(x_1, x_2, 0) \in \RR^3}{ x_1, x_2 \in \RR}$ is
    two dimensional because the first two canonical basis vectors
    of $\RR^3$ form a basis 
    
    \Eg In $\RR^3$, a line through the origin is one-dimensional, while a
    plane through the origin is two-dimensional

\end{frame}


\begin{frame}
    \frametitle{Dimension of Spans}

    \Fact  Let $X := \{\boldx_1,\ldots,\boldx_K\} \subset \RR^N$ 

    The following statements are true:
    %
    \begin{enumerate}
        \item $\dimension(\Span(X)) \leq K$
        \item $\dimension(\Span(X)) = K$ $\;\iff\;$ $X$ is linearly independent
    \end{enumerate}
    %

    \vspace{1.5em}

    Proof that $\dimension(\Span(X)) \leq K$

    If not then $\Span(X)$ has a basis with $M > K$ elements

    Hence $\Span(X)$ contains $M > K$ linearly independent vectors

    This is impossible, given that $\Span(X)$ is spanned by $K$ vectors

\end{frame}


\begin{frame}
    
    Now consider the second claim: 

    \begin{itemize}
        \item[1.] $X$ is linearly independent $\implies$ $\dim(\Span(X)) = K$  
    \end{itemize}

    Proof: True because the vectors $\boldx_1,\ldots,\boldx_K$ form
    a basis of $\Span(X)$

    \vspace{1em}

    \begin{itemize}
        \item[2.] $\dimension(\Span(X)) = K$ $\implies$ $X$ linearly independent
    \end{itemize}
    
    Proof: If not then $\exists \, X_0 \subsetneq X$ such that $\Span(X_0) = \Span(X)$

    By this equality and part 1 of the theorem, 
    %
    $$\dim(\Span(X)) = \dim(\Span(X_0)) \leq \# X_0 \leq K - 1$$
    
    Contradiction
    
\end{frame}


\begin{frame}
    
    \Fact If $S$ a linear subspace of $\RR^N$, then 
    $$
    \dim(S) = N \; \iff \; S = \RR^N
    $$

    \vspace{1em}

    Useful implications

    \begin{itemize}
        \item The only $N$-dimensional subspace of $\RR^N$ is $\RR^N$
        \item To show $S = \RR^N$ just need to show that $\dim(S) = N$ 
    \end{itemize}


    \vspace{1em}


    Proof: See course notes
    
    

\end{frame}




\begin{frame}
    \frametitle{Linear Maps}

    In this section we investigate one of the most important classes of
    functions

    These are the so-called linear functions

    Linear functions play a fundamental role in  all fields of science

    \begin{itemize}
        \item In one-to-one correspondence with matrices
    \end{itemize}

    Even nonlinear functions can often be rewritten as partially linear

    The properties of linear functions are closely tied to notions such as 

    \begin{itemize}
        \item linear combinations, span
        \item linear independence, bases, etc.
    \end{itemize}

\end{frame}



\begin{frame}
    \frametitle{Linearity}

    A function $T \colon \RR^K \to \RR^N$ is called
    \navy{linear} if 
    %
    \begin{equation*}
            T(\alpha \boldx + \beta \boldy) = \alpha T\boldx + \beta T\boldy
            \qquad
            \forall \, 
            \boldx, \boldy \in \RR^K, \;
            \forall \,
            \alpha, \beta \in \RR
    \end{equation*}
    %

    \vspace{1em}
    Notation: 
    
    \begin{itemize}
        \item Linear functions often written with upper case letters
            \vspace{0.5em}
        \item Typically omit parenthesis around arguments when convenient
    \end{itemize}



\end{frame}



\begin{frame}
    
    \Eg    $T \colon \RR \to \RR$ defined by $Tx = 2x$ is linear 

    Proof: Take any $\alpha, \beta, x, y$ in $\RR$ and observe that
    %
    \begin{equation*}
      T(\alpha x + \beta y)
      = 2(\alpha x + \beta y)
      = \alpha 2 x + \beta 2 y
      = \alpha Tx + \beta Ty  
    \end{equation*}

    \vspace{1em}

        
    \Eg The function $f \colon \RR \to \RR$ defined by $f(x) = x^2$ is
    \underline{non}linear
    
    Proof: Set $\alpha = \beta = x = y = 1$
    
    Then 
    %
    \begin{itemize}
        \item $f(\alpha x + \beta y) = f(2) = 4$
        \item But $\alpha f(x) + \beta f(y) = 1 + 1 = 2$
    \end{itemize}
    

\end{frame}



\begin{frame}

    \begin{example}
        
    Given constants $c_1$ and $c_2$, the 
    function $T \colon \RR^2 \to \RR$ defined by 
    %
    \begin{equation*}
        T \boldx = T (x_1, x_2) = c_1 x_1 + c_2 x_2   
    \end{equation*}
    %
    is linear 
    
    \end{example}

    \vspace{1em}

    Proof: If we take any $\alpha, \beta$ in $\RR$ and
    $\boldx, \boldy$ in $\RR^2$, then 
    %
    \begin{align*}
      T(\alpha \boldx + \beta \boldy)
      & = c_1 [\alpha x_1 + \beta y_1] + c_2 [\alpha x_2 + \beta y_2]
      \\
      & = \alpha [c_1 x_1 + c_2 x_2] + \beta [c_1 y_1 + c_2 y_2]
      \\
      & = \alpha T \boldx + \beta T \boldy  
    \end{align*}

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{linfunc.pdf}}
           \caption{The graph of $T \boldx = c_1 x_1 + c_2 x_2$ is a plane
           through the origin}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}
    
    Remark: Thinking of linear functions as those whose graph is a straight
    line is not correct

    \begin{example}
        
    Function $f \colon \RR \to \RR$ defined by $f(x) = 1 + 2x$ is
    \underline{non}linear

    Proof: Take $\alpha = \beta = x = y = 1$
    
    Then 
    
    \begin{itemize}
        \item $f(\alpha x + \beta y) = f(2) = 5$
        \item But $\alpha f(x) + \beta f(y) = 3 + 3 = 6$
    \end{itemize}
    
    \end{example}

    \vspace{1em}
    
    This kind of function is called an \navy{affine} function

\end{frame}


\begin{frame}
    
    Let $\bolda_1, \ldots, \bolda_K$ be vectors in $\RR^N$

    Let $T \colon \RR^K \to \RR^N$ be defined by 
    %
    $$
    T\boldx 
    =
    T
    \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_K
    \end{pmatrix}
    =
    x_1 \bolda_1 + \ldots + x_K \bolda_K
    $$

    \Ex Show that this function is linear

    Remarks
    %
    \begin{itemize}
        \item This is a generalization of the previous linear examples 
        \item In a sense it is the most general representation of a linear map
            from $\RR^K$ to $\RR^N$
        \item It is also ``the same" as the $N \times K$ matrix with
            columns $\bolda_1, \ldots, \bolda_K$ --- more on this later
    \end{itemize}


\end{frame}


\begin{frame}
    \frametitle{Implications of Linearity} 

    \Fact  If $T \colon \RR^K \to \RR^N$ is a linear map and $\boldx_1,
    \ldots, \boldx_J$ are vectors in $\RR^K$, then
    for any linear combination we have
    %
    \begin{equation*}
        T
         \left[ \alpha_1 \boldx_1 + \cdots + \alpha_J \boldx_J \right]
        = \alpha_1 T \boldx_1 + \cdots + \alpha_J T \boldx_J
    \end{equation*}

    Proof for $J=3$:  Applying the def of linearity twice, 
    %
    \begin{align*}
        T
         \left[ \alpha_1 \boldx_1 + \alpha_2 \boldx_2 + \alpha_3 \boldx_3 \right]
         & = T\left[ (\alpha_1 \boldx_1 + \alpha_2 \boldx_2) + \alpha_3 \boldx_3 \right]
         \\
         & = T\left[ \alpha_1 \boldx_1 + \alpha_2 \boldx_2 \right] + \alpha_3 T \boldx_3 
         \\
         & = \alpha_1 T \boldx_1 + \alpha_2 T \boldx_2  + \alpha_3 T \boldx_3 
    \end{align*}

    \vspace{1em}

    \Ex Show that if $T$ is any linear function then $T\boldzero = \boldzero$


\end{frame}

\begin{frame}
    

    \Fact If $T \colon \RR^K \to \RR^N$ is a linear map, then 
    %
    \begin{equation*}
        \range(T) = \Span(V) 
        \quad \text{where} \quad
        V := \{T\bolde_1, \ldots, T\bolde_K\}
    \end{equation*}


    \begin{itemize}
        \item Here $\bolde_k$ is the $k$-th canonical basis vector in $\RR^K$
    \end{itemize}

    \vspace{1em}

    Proof: Any $\boldx \in \RR^K$ can be expressed as $\sum_{k=1}^K \alpha_k \bolde_k$

    Hence $\range(T)$ is the set of all points of the form
    %
    \begin{equation*}
        T\boldx
        = T \left[ \sum_{k=1}^K \alpha_k \bolde_k \right]
        = \sum_{k=1}^K \alpha_k T \bolde_k 
    \end{equation*}
    %
    as we vary $\alpha_1, \ldots, \alpha_K$ over all combinations

    This coincides with the definition of $\Span(V)$


\end{frame}


\begin{frame}
    
    \begin{example}
    Let $T \colon \RR^2 \to \RR^2$ be defined by 
    %
    $$
        T\boldx 
        =
        T(x_1, x_2)
        =
        x_1 
        \begin{pmatrix}
            1 \\
            2
        \end{pmatrix}
        +
        x_2 
        \begin{pmatrix}
            0 \\
            -2
        \end{pmatrix}
    $$

    Then 
    $$
        T\bolde_1
        =
        \begin{pmatrix}
            1 \\
            2
        \end{pmatrix}
        \quad \text{and} \quad
        T\bolde_2
        =
        \begin{pmatrix}
            0 \\
            -2
        \end{pmatrix}
    $$

    \vspace{1em}

    \Ex Show that $V := \{T\bolde_1, T\bolde_2\}$ is linearly independent
        

    \vspace{1em}

    We conclude that the range of $T$ is all of $\RR^2$  (why?)

    \end{example}



\end{frame}

\begin{frame}

    The \navy{null space} or \navy{kernel} of linear map $T \colon \RR^K \to
    \RR^N$ is
    %
    \begin{equation*}
        \kernel(T) := \setntn{\boldx \in \RR^K}{T\boldx = \boldzero}
    \end{equation*}
    %

    \Ex Show that $\kernel(T)$ is a linear subspace of $\RR^K$
            \vspace{0.5em}

            \vspace{0.5em}

    \Fact $\kernel(T) = \{\boldzero\}$ if and only if $T$ is one-to-one

            \vspace{0.5em}

    Proof of $\implies$: Suppose that $T\boldx = T\boldy$ for arbitrary $\boldx, \boldy \in \RR^K$
    
    Then $\boldzero = T\boldx - T\boldy  = T(\boldx - \boldy)$
    
    In other words, $\boldx - \boldy \in \kernel(T)$
    
    Hence $\kernel(T) = \{\boldzero\}$ $\implies$ $\boldx = \boldy$

\end{frame}


\begin{frame}
    
    \frametitle{Linearity and Bijections}

    Many scientific and practical problems are ``inverse" problems

    \begin{itemize}
        \item We observe outcomes but not what caused them
        \item How can we work backwards from outcomes to causes?
    \end{itemize}
    
    Examples

    \begin{itemize}
        \item What consumer preferences generated observed market behavior?
        \item What kinds of expectations led to given shift in exchange rates?
    \end{itemize}

\end{frame}

\begin{frame}

    Loosely, we can express an inverse problem as
    
    \begin{figure}
       \begin{center}
        \scalebox{.5}{\input{inverse_prob.pdf_t}}
       \end{center}
    \end{figure}

    \begin{itemize}
        \item Does this problem have a solution?
        \item Is it unique?
    \end{itemize}

    Answers depend on whether $F$ is one-to-one, onto, etc.

    The best case is a bijection

    But other situations also arise

\end{frame}

\begin{frame}
    
    Recall that an arbitrary function can be 

    \begin{itemize}
        \item one-to-one
        \item onto
        \item both (a bijection)
        \item neither 
    \end{itemize}

    For linear functions from $\RR^N$ to $\RR^N$, the first three are all
    equivalent!

    In particular, 
    $$
    \text{onto } \iff \text{ one-to-one } \iff \text{ bijection}
    $$

    The next theorem summarizes

\end{frame}


\begin{frame}
   
    \Fact     If $T$ is a linear function from $\RR^N$ to $\RR^N$ then all of the
    following are equivalent:
    %
    \begin{enumerate}
        \item $T$ is a bijection
        \item $T$ is onto
        \item $T$ is one-to-one
        \item $\kernel(T) = \{ \boldzero \}$
        \item The set of vectors $V := \{T\bolde_1, \ldots, T\bolde_N\}$ is
            linearly independent
    \end{enumerate}

    If any one of these equivalent conditions is true, then $T$ is called
    \navy{nonsingular}

    \vspace{1em}

    \begin{itemize}
        \item Don't forget: We are talking about $\RR^N$ to $\RR^N$ here
    \end{itemize}

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{linbijec.pdf}}
        \caption{The case of $N=1$, nonsingular and singular}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}
    
    Proof that $T$ onto $\iff$ $V := \{T\bolde_1, \ldots, T\bolde_N\}$ is
            linearly independent

    \vspace{1em}

    Recall that for any linear map $T$ we have $\range(T) =  \Span(V)$

    Using this fact and the definitions,
    %
    \begin{align*}
        T \text{ onto } 
        & \iff \range(T) = \RR^N
        \\
        & \iff \Span(V)  = \RR^N
        \\
        & \iff V \text{ is linearly indepenent}
    \end{align*}

    (We saw that $N$ vectors span $\RR^N$ iff linearly indepenent)

    \vspace{1em}

    Rest of proof: Solved exercises

\end{frame}


\begin{frame}
    
    \Fact If $T \colon \RR^N \to \RR^N$ is nonsingular then so is $T^{-1}$.  

    \vspace{1em}

    What is the implication here?

    If $T$ is a bijection then so is $T^{-1}$

    Hence the only real claim is that $T^{-1}$ is also linear

    The proof is an exercise...

\end{frame}



\begin{frame}
    \frametitle{Maps Across Different Dimensions} 

    Remember that these results apply to maps from $\RR^N$ to $\RR^N$

    Things change when we look at linear maps across dimensions

    \vspace{1em}

    The general rules for linear maps are 

    \begin{itemize}
        \item Maps from lower to higher dimensions cannot be onto
        \item Maps from higher to lower dimensions cannot be one-to-one
    \end{itemize}

    In either case they cannot be bijections

    \vspace{1em}

    The next fact summarizes

\end{frame}


\begin{frame}



    \Fact For a linear map $T$ from $\RR^K \to \RR^N$, the following statements are
    true:
    %
    \begin{enumerate}
        \item If $K < N$ then $T$ is not onto
        \item If $K > N$ then $T$ is not one-to-one
    \end{enumerate}
    %
    \vspace{1em}

    Proof of part 1: Let $K < N$ and let $T \colon \RR^K \to \RR^N$ be linear  
    
    Letting $V := \{T\bolde_1, \ldots, T\bolde_K\}$, we have
    %
    $$
    \dim(\range(T)) = \dim(\Span(V)) \leq K < N
    $$
    %
    $$
    \fore 
        \range(T) \not= \RR^N
    $$

    Hence $T$ is not onto 
    
\end{frame}


\begin{frame}
    
    Proof of part 2: $K > N$ $\implies$ $T$ is not one-to-one

    \vspace{1em}

    Suppose to the contrary that $T$ is one-to-one

    Let $\alpha_1, \ldots, \alpha_K$ be a collection of vectors such that 
    %
    $$
    \alpha_1 T \bolde_1 + \cdots + \alpha_K T \bolde_K = \boldzero
    $$
    %
    $$
    \fore 
    T (\alpha_1 \bolde_1 + \cdots + \alpha_K \bolde_K) = \boldzero
    \qquad (\text{by linearity})
    $$
    %
    $$
    \fore
    \alpha_1 \bolde_1 + \cdots + \alpha_K \bolde_K = \boldzero
    \qquad (\text{since $\ker(T) = \{\boldzero\}$})
    $$
    %
    $$
    \fore 
    \alpha_1 = \cdots = \alpha_K = 0
    \qquad (\text{by independence of $\{\bolde_1, \ldots \bolde_K\}$)}
    $$

    We have shown that $\{T\bolde_1, \ldots, T\bolde_K\}$ is linearly
    independent

    But then $\RR^N$ contains a linearly independent set
    with $K > N$ vectors --- contradiction

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{cost_min_2.pdf}}
       \end{center}
    \end{figure}

   \Eg Cost function $c(k, \ell) = rk + w\ell$ cannot be one-to-one

\end{frame}




\section{Matrices}


\begin{frame}
    \frametitle{Matrices and Linear Equations}

    We now begin our study of matrices

    As we'll see, there's an isomorphic relationship between 
    %
    \begin{enumerate}
        \item matrices 
        \item linear maps
    \end{enumerate}
    

    Often properties of matrices are best understood via those of linear maps
    
\end{frame}






\begin{frame}
    \frametitle{Matrices}

    Typical \navy{$N \times K$ matrix}: 
    
    \begin{equation*}
        \boldA = 
        \left(
        \begin{array}{cccc}
            a_{11} & a_{12} & \cdots & a_{1K} \\
            a_{21} & a_{22} & \cdots & a_{2K} \\
            \vdots & \vdots &  & \vdots \\
            a_{N1} & a_{N2} & \cdots & a_{NK} 
        \end{array}
        \right)
    \end{equation*}
    %

    \vspace{2em}

    Symbol $a_{nk}$ stands for element in the 
    %
    \begin{itemize}
    \item $n$-th row 
    \item $k$-th column
    \end{itemize}

\end{frame}


\begin{frame}

    Often matrices correspond to coefficients of a linear equation
    
    \begin{equation}
        \begin{array}{c}
            a_{11} x_1 + a_{12} x_2 + \cdots + a_{1K} x_K = b_1 \\
            a_{21} x_1 + a_{22} x_2 + \cdots + a_{2K} x_K = b_2 \\
            \vdots  \\
            a_{N1} x_1 + a_{N2} x_2 + \cdots + a_{NK} x_K = b_N 
        \end{array}
    \end{equation}

    \vspace{1em}

    Given the $a_{nm}$ and $b_n$, what values of $x_1, \ldots, x_K$ solve this
    system?

    \vspace{1em}

    We now investigate this and other related questions

    But first some background on matrices...

\end{frame}



\begin{frame}

    An $N \times K$ matrix also called a 
    %
    \begin{itemize}
        \item \navy{row vector} if $N = 1$
            \vspace{1em}
        \item \navy{column vector} if $K = 1$
    \end{itemize}
    
    \vspace{1em}

    \Egs

    $$
    \boldb 
    = 
    \begin{pmatrix}
        b_1 \\
        \vdots \\
        b_N 
    \end{pmatrix}
    \text{ is }\; N \times 1,
    \qquad
    \boldc 
    = 
    \begin{pmatrix}
        c_1 \cdots c_K 
    \end{pmatrix}
    \text{ is }\; 
    1 \times K
    $$

    If $N = K$, then $\boldA$ is called \navy{square}
    

\end{frame}


\begin{frame}
    
    We use 
    
    \begin{itemize}
        \item $\col_k(\boldA)$ to denote the $k$-th column of $\boldA$
        \item $\row_n(\boldA)$ to denote the $n$-th row of $\boldA$
    \end{itemize}

    Example

    %
    \begin{equation*}
        \col_1(\boldA)
        =
        \col_1
        \left(
        \begin{array}{ccc}
            \red{a_{11}} & \cdots & a_{1K} \\
            \red{a_{21}} & \cdots & a_{2K} \\
            \vdots & \vdots & \vdots \\
            \red{a_{N1}} & \cdots & a_{NK} 
        \end{array}
        \right)
        =
        \left(
        \begin{array}{c}
            a_{11} \\
            a_{21} \\
            \vdots \\
            a_{N1} 
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}


\begin{frame}
    
    The \navy{zero matrix} is
    %
    \begin{equation*}
        \boldzero := 
        \left(
        \begin{array}{cccc}
            0 & 0 & \cdots & 0 \\
            0 & 0 & \cdots & 0 \\
            \vdots & \vdots &  & \vdots \\
            0 & 0 & \cdots & 0 \\
        \end{array}
        \right)
    \end{equation*}
    %

    The \navy{identity matrix} is
    %
    \begin{equation*}
        \boldI := 
        \left(
        \begin{array}{cccc}
            1 & 0 & \cdots & 0 \\
            0 & 1 & \cdots & 0 \\
            \vdots & \vdots &  & \vdots \\
            0 & 0 & \cdots & 1 \\
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}



\begin{frame}
    \frametitle{Algebraic Operations for Matrices}

    Addition and scalar multiplication are also defined for matrices
    
   Both are element by element, as in the vector case

   Scalar multiplication:
    
    \begin{equation*}
        \gamma 
        \left(
        \begin{array}{cccc}
            a_{11} & a_{12} & \cdots & a_{1K} \\
            a_{21} & a_{22} & \cdots & a_{2K} \\
            \vdots & \vdots &  & \vdots \\
            a_{N1} & a_{N2} & \cdots & a_{NK} \\
        \end{array}
        \right)
        :=
        \left(
        \begin{array}{cccc}
            \gamma a_{11} & \gamma a_{12} & \cdots & \gamma a_{1K} \\
            \gamma a_{21} & \gamma a_{22} & \cdots & \gamma a_{2K} \\
            \vdots & \vdots &  & \vdots \\
            \gamma a_{N1} & \gamma a_{N2} & \cdots & \gamma a_{NK} \\
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}



\begin{frame}
    
    Addition:
    % 
    \begin{multline*}
        \left(
        \begin{array}{ccc}
            a_{11} & \cdots & a_{1K} \\
            a_{21} & \cdots & a_{2K} \\
            \vdots & \vdots & \vdots \\
            a_{N1} & \cdots & a_{NK} \\
        \end{array}
        \right)
        +
        \left(
        \begin{array}{ccc}
            b_{11} & \cdots & b_{1K} \\
            b_{21} & \cdots & b_{2K} \\
            \vdots & \vdots & \vdots \\
            b_{N1} & \cdots & b_{NK} \\
        \end{array}
        \right)
        \\
        :=
        \left(
        \begin{array}{ccc}
            a_{11} + b_{11} &  \cdots & a_{1K} + b_{1K} \\
            a_{21} + b_{21} &  \cdots & a_{2K} + b_{2K} \\
            \vdots & \vdots & \vdots \\
            a_{N1} + b_{N1} &  \cdots & a_{NK} + b_{NK} \\
        \end{array}
        \right)
    \end{multline*}
    %

    Note that matrices must be same dimension

\end{frame}

\begin{frame}
    
    Multiplication of matrices:  
    
    Product $\boldA \boldB$: 
    $i,j$-th element is inner product of $i$-th row of $\boldA$ and 
    $j$-th column of $\boldB$  

    %
    \begin{equation*}
        \left(
        \begin{array}{ccc}
            \red{a_{11}} & \red{\cdots} & \red{a_{1K}} \\
            a_{21} & \cdots & a_{2K} \\
            \vdots & \vdots & \vdots \\
            a_{N1} & \cdots & a_{NK} \\
        \end{array}
        \right)
        \left(
        \begin{array}{ccc}
            \red{b_{11}} & \cdots & b_{1J} \\
            \red{b_{21}} & \cdots & b_{2J} \\
            \red{\vdots} & \vdots & \vdots \\
            \red{b_{K1}} & \cdots & b_{KJ} \\
        \end{array}
        \right)
        =
        \left(
        \begin{array}{ccc}
            \red{c_{11}} & \cdots & c_{1J} \\
            c_{21} & \cdots & c_{2J} \\
            \vdots & \vdots & \vdots \\
            c_{N1} & \cdots & c_{NJ} \\
        \end{array}
        \right)
    \end{equation*}
    %

    In this display, 
    %
    \begin{equation*}
        c_{11} = \sum_{k=1}^K a_{1k} b_{k1}
    \end{equation*}



\end{frame}


\begin{frame}
    
    Suppose $\boldA$ is $N \times K$ and $\boldB$ is $J \times M$
    
    \begin{itemize}
        \item $\boldA \boldB$ defined only if $K = J$
        \item Resulting matrix $\boldA \boldB$ is $N \times M$
    \end{itemize}
    

    The rule to remember:
    %
    \begin{equation*}
        \text{product of } N \times K \text{ and } K \times M
        \text{ is }  N \times M
    \end{equation*}
    %

    Important: Multiplication is not commutative
    
    In particular, it is not in general true that $\boldA \boldB = \boldB \boldA$ 

    \begin{itemize}
        \item In fact $\boldB \boldA$ is not well-defined unless $N = M$ also holds 
    \end{itemize}
    

\end{frame}


\begin{frame}

    Useful observation:

    \begin{align*}
        \boldA \boldx
        & = 
        \left(
        \begin{array}{cccc}
            a_{11} & a_{12} & \cdots & a_{1K} \\
            a_{21} & a_{22} & \cdots & a_{2K} \\
            \vdots & \vdots &  & \vdots \\
            a_{N1} & a_{N2} & \cdots & a_{NK} 
        \end{array}
        \right)
        \left(
        \begin{array}{c}
            x_1 \\
            x_2 \\
            \vdots \\
            x_K
        \end{array}
        \right)
        \\
        & =
        x_1 \left(
        \begin{array}{c}
            a_{11} \\
            a_{21} \\
            \vdots \\
            a_{N1} 
        \end{array}
        \right)
        +
        x_2 \left(
        \begin{array}{c}
            a_{12} \\
            a_{22} \\
            \vdots \\
            a_{N2} 
        \end{array}
        \right)
        + \cdots + 
        x_K \left(
        \begin{array}{c}
            a_{1K} \\
            a_{2K} \\
            \vdots \\
            a_{NK} 
        \end{array}
        \right)
        \\
        & = 
        \sum_{k=1}^K x_k \col_k(\boldA)
    \end{align*}





\end{frame}

\begin{frame}
    
    Rules for multiplication:

    \Fact Given scalar $\alpha$ and conformable $\boldA$, $\boldB$ and $\boldC$, we have

    \begin{enumerate}
        \item $\boldA (\boldB \boldC) = (\boldA \boldB) \boldC$
            \vspace{0.4em}
        \item $\boldA (\boldB + \boldC) = \boldA \boldB + \boldA \boldC$
            \vspace{0.4em}
        \item $(\boldA + \boldB) \boldC = \boldA \boldC + \boldB \boldC$
            \vspace{0.4em}
        \item $\boldA \alpha \boldB = \alpha \boldA \boldB$
    \end{enumerate}

    \vspace{0.4em}

    (Here ``conformable'' means operation makes sense)


\end{frame}


\begin{frame}
    
    The \navy{$k$-th power} of a square matrix $\boldA$ is 
    
    $$ \boldA^k := \underbrace{\boldA \cdots \boldA}_{k \text{ terms}} $$

    \vspace{1em}

    If it exists, the \navy{square root} of $\boldA$ is written $\boldA^{1/2}$ 

    Defined as the matrix $\boldB$ such that $\boldB^2$ is $\boldA$

    \vspace{1em}

    More on these later...

\end{frame}


\begin{frame}
    
    In matrix multiplication, $\boldI$ is the multiplicative unit

    That is,  assuming conformability, we always have
    %
    \begin{equation*}
        \boldA \boldI = \boldI \boldA = \boldA
    \end{equation*}
    
    \Ex Check it using the definition of matrix multiplication

    \vspace{1em}

    Note: If $\boldI$ is $K \times K$, then
    $$
    \col_k(\boldI)
    = \bolde_k
    = \text{ $k$-th canonical basis vector in } \RR^K
    $$

\end{frame}

\begin{frame}[fragile]
    

    \begin{pythoncode}
In [1]: import numpy as np

In [2]: A = [[2, 4],
   ...:      [4, 2]]

In [3]: A = np.array(A)  # Convert A to array

In [4]: B = np.identity(2)

In [5]: B
Out[5]: 
array([[ 1.,  0.],
       [ 0.,  1.]])
    \end{pythoncode}

\end{frame}




\begin{frame}[fragile]

    \begin{pythoncode}
In [6]: A + B        # Matrix addition
Out[6]: 
array([[ 3.,  4.],
       [ 4.,  3.]])

In [7]: np.dot(A, B)  # Matrix multiplication
Out[7]: 
array([[ 2.,  4.],
       [ 4.,  2.]])
    \end{pythoncode}


\end{frame}





\section{Matrices as Maps}

\begin{frame}
    \frametitle{Matrices as Maps}
    
    Any $N \times K$ matrix $\boldA$ can be thought of as a function
    $\boldx \mapsto \boldA \boldx$
    %
    \begin{itemize}
        \item In $\boldA \boldx$ the $\boldx$ is understood to be
            a column vector
    \end{itemize}

    It turns out that every such map is linear

    To see this fix $N \times K$ matrix $\boldA$ and let $T$ be defined by
    \begin{equation*}
        T \colon \RR^K \to \RR^N, 
        \qquad
        T\boldx = \boldA \boldx
    \end{equation*}

    Pick any $\boldx$, $\boldy$ in $\RR^K$, and any scalars $\alpha$ and $\beta$
     
     The rules of matrix arithmetic tell us that
    %
    \begin{equation*}
        T(\alpha \boldx + \beta \boldy) 
        := \boldA (\alpha \boldx + \beta \boldy)
        = \alpha \boldA \boldx  + \beta \boldA \boldy
        =: \alpha T\boldx + \beta T\boldy 
    \end{equation*}
    
\end{frame}

\begin{frame}
    
    So matrices make linear functions
    
    How about examples of linear functions that don't involve matrices?  
    
    Actually there are none!

    \vspace{1em}

    \Fact If $T \colon \RR^K \to \RR^N$ then 
    %
    $$
    T \text{ is linear }
    \; \iff \;
    \exists \; N \times K \text{ matrix } \boldA  \st T\boldx = \boldA \boldx, 
    \;\forall \, \boldx \in \RR^K
    $$
    %

    \vspace{1em}

    \begin{itemize}
        \item On the last slide we showed the $\Longleftarrow$ part
            \vspace{1em}
        \item On the next slide we show the $\implies$ part
    \end{itemize}

\end{frame}


\begin{frame}
    
    Let $T \colon \RR^K \to \RR^N$ be linear
    
    We aim to construct an $N \times K$ matrix $\boldA$ such that
    %
    $$ T\boldx = \boldA \boldx, \qquad \forall \, \boldx \in \RR^K $$
    
    As usual, let
    $\bolde_k$ be the $k$-th canonical basis vector in $\RR^K$
    
    Define a matrix $\boldA$ by $\col_k(\boldA) = T\bolde_k$
    
    Pick any $\boldx = (x_1, \ldots, x_K) \in \RR^K$
    
    By linearity we have 
    %
    \begin{equation*}
        T\boldx 
        = T \left[\sum_{k=1}^K x_k \bolde_k \right]
        = \sum_{k=1}^K x_k T \bolde_k
        = \sum_{k=1}^K x_k \col_k(\boldA)
        = \boldA \boldx
    \end{equation*}
    %

\end{frame}


\begin{frame}
    \frametitle{Matrix Product as Composition}

    Let

    \begin{itemize}
        \item $\boldA$ be $N \times K$ and $\boldB$ be $K \times M$
        \item $T \colon \RR^K \to \RR^N$ be the linear map $T\boldx = \boldA\boldx$ 
        \item $U \colon \RR^M \to \RR^K$ be the linear map $U\boldx = \boldB\boldx$ 
    \end{itemize}

    The matrix product $\boldA \boldB$ corresponds exactly to the
    \underline{composition} of $T$ and $U$ 
    
    Proof:
    $$
    (T \circ U) (\boldx)
    = T( U\boldx)
    = T( \boldB \boldx)
    = \boldA \boldB \boldx
    $$

\end{frame}


\begin{frame}
    
    This helps us understand a few things

    For example, let
    %
    \begin{itemize}
        \item $\boldA$ be $N \times K$ and $\boldB$ be $J \times M$
        \item $T \colon \RR^K \to \RR^N$ be the linear map $T\boldx = \boldA\boldx$ 
        \item $U \colon \RR^M \to \RR^J$ be the linear map $U\boldx = \boldB\boldx$ 
    \end{itemize}

    Then $\boldA \boldB$ is only defined when $K = J$

    This is because $\boldA \boldB$ corresponds to $T \circ U$

    But for $T \circ U$ to be well defined we need $K = J$

    Then $U$ maps $\RR^M$ to $\RR^K$ and $T$ maps $\RR^K$ to $\RR^N$

\end{frame}




\section{Rank}


\begin{frame}
    \frametitle{Column Space}

    Let $\boldA$ be an $N \times K$ matrix

    The \navy{column space} of $\boldA$ is defined as the span of its columns
    %
    \begin{align*}
    \Span(\boldA) 
        & = \Span \{ \col_1 (\boldA), \ldots, \col_K(\boldA) \}
        \\
        & = \text{all vectors of the form } \sum_{k=1}^K x_k \col_k(\boldA)
    \end{align*}

    Equivalently,
    %
    \begin{equation*}
        \Span(\boldA) := \setntn{\boldA \boldx}{ \boldx \in \RR^K}
    \end{equation*}
    %
    
    This is exactly the range of the associated linear map
    %
    \begin{center}
    $T \colon \RR^K \to \RR^N$ defined by $T \boldx = \boldA \boldx$
    \end{center}

\end{frame}


\begin{frame}
    
    \Eg If
    %
    $$
    \boldA =
    \begin{pmatrix}
       1 & -5 \\
       2 & 3
    \end{pmatrix}
    $$
    %

    then the span is all linear combinations
    %
    \begin{equation*}
        x_1
        \left(
        \begin{array}{c}
            1 \\
            2
        \end{array}
        \right)
        + 
        x_2
        \left(
        \begin{array}{c}
            -5 \\
            3
        \end{array}
        \right)
        \quad
        \text{where}
        \quad
        (x_1, x_2) \in \RR^2
    \end{equation*}
    %

    These columns are linearly independent (shown earlier)
    
    Hence the column space is all of $\RR^2$ (why?)

    \vspace{1em}

    \Ex Show that the column space of any $N \times K$ matrix is a linear
    subspace of $\RR^N$

\end{frame}



\begin{frame}
    \frametitle{Rank}

    Equivalent questions
    %
    \begin{itemize}
        \item How large is the range of the linear map $T \boldx = \boldA \boldx$?
        \item How large is the column space of $\boldA$?
    \end{itemize}

    \vspace{1em}

    The obvious measure of size for a linear subspace is its dimension
    
    The dimension of $\Span(\boldA)$ is known as the \navy{rank} of $\boldA$  
    %
    \begin{equation*}
        \rank(\boldA) := \dimension(\Span(\boldA))
    \end{equation*}
    %

    Because $\Span(\boldA)$ is the span of $K$ vectors, we have
    %
    \begin{equation*}
        \rank(\boldA) = \dimension(\Span(\boldA)) \leq K
    \end{equation*}


\end{frame}


\begin{frame}
    
    $\boldA$ is said to have \navy{full column rank} if 
    %
    \begin{equation*}
        \rank(\boldA) = \text{ number of columns of } \boldA
    \end{equation*}

    
    \Fact For any matrix $\boldA$,  the following statements are equivalent:
    %
    \begin{enumerate}
        \item $\boldA$ is of full column rank
            \vspace{0.3em} 
        \item The columns of $\boldA$ are linearly independent
            \vspace{0.3em} 
        \item If $\boldA \boldx = \boldzero$, then $\boldx = \boldzero$
    \end{enumerate}
    %

    \vspace{1em}

    \Ex Check this, recalling that
    %
    $$
    \dim(\Span\{\bolda_1, \ldots, \bolda_K\}) = K
    \, \iff \,
    \{\bolda_1, \ldots, \bolda_K\} \text{ linearly indepenent}
    $$

\end{frame}


\begin{frame}[fragile]

    \begin{pythoncode}
In [1]: import numpy as np

In [2]: A = [[2.0, 1.0],
   ...:      [6.3, 3.0]]

In [3]: np.linalg.matrix_rank(A)
Out[3]: 2

In [4]: A = [[2.0, 1.0],  # Col 2 is half col 1
   ...:      [6.0, 3.0]]

In [5]: np.linalg.matrix_rank(A)
Out[5]: 1
    \end{pythoncode}
    
\end{frame}



\begin{frame}
    \frametitle{Reminder I}
    
    Suppose we want to find the $x$ that solves $f(x) = y$

    The ideal case is when $f$ is a bijection

    \begin{figure}
       \begin{center}
           \scalebox{0.6}{\input{../tikzfigs/function2.tex}}
       \end{center}
    \end{figure}

    \vspace{-1em}

    Equivalent:
    %
    \begin{itemize}
        \item $f$ is a bijection
        \item each $y \in B$ has a unique preimage
        \item $f(x) = y$ has a unique solution $x$ for each $y$
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Reminder II}
    
    Let $T$ be a linear function from $\RR^N$ to $\RR^N$ 

    \vspace{1em}
    
    We saw that in this case all of the following are equivalent:
    %
    \begin{enumerate}
        \item $T$ is a bijection
        \item $T$ is onto
        \item $T$ is one-to-one
        \item $\kernel(T) = \{ \boldzero \}$
        \item $V := \{T\bolde_1, \ldots, T\bolde_N\}$ is linearly independent
    \end{enumerate}

    \vspace{1em}

    We then say that $T$ is nonsingular ($=$ linear bijection)

\end{frame}


\section{$N \times N$ Linear Equations}

\begin{frame}
    \frametitle{Linear Equations}
    
    Let's look at solving linear equations such as $\boldA \boldx = \boldb$ 

    We start with the ``best" case: 
    
    \begin{center}
    number of equations $=$ number of unknowns
    \end{center}

    Thus, 

    \begin{itemize}
        \item Take $N \times N$ matrix $\boldA$ and $N \times 1$ vector $\boldb$ as given 
        \item Search for an $N \times 1$ solution $\boldx$
    \end{itemize}
    

    But does such a solution exist?  If so is it unique?


\end{frame}

\begin{frame}

    The best way to think about this is to consider 
    the corresponding linear map
    %
    $$
    T \colon \RR^N \to \RR^N,
    \qquad T\boldx = \boldA \boldx
    $$

    \begin{figure}
       \begin{center}
           \scalebox{0.6}{\input{../tikzfigs/linbijec.tex}}
       \end{center}
    \end{figure}


    Equivalent:
    %
    \begin{enumerate}
        \item $\boldA \boldx = \boldb$ has a unique solution $\boldx$ for any
            given $\boldb$
        \item $T \boldx = \boldb$ has a unique solution $\boldx$ for any
            given $\boldb$
        \item $T$ is a bijection
    \end{enumerate}
    

\end{frame}


\begin{frame}

    We already have conditions for linear maps to be bijections

    Just need to translate these into the matrix setting

    \vspace{1em}

    Recall that $T$ called nonsingular if $T$ is a linear bijection

    We say that $\boldA$ is \navy{nonsingular} if $T$ is nonsingular

    Equivalent:

    \begin{itemize}
        \item $\boldx \mapsto \boldA \boldx$ is a bijection from $\RR^N$ to $\RR^N$
    \end{itemize}

    \vspace{1em}

    We now list equivalent conditions for nonsingularity

\end{frame}

\begin{frame}

    Let $\boldA$ be an $N \times N$ matrix
    
    \Fact All of the following conditions are equivalent
    %
    \begin{enumerate}
        \item $\boldA$ is nonsingular
            \vspace{0.3em}
        \item The columns of $\boldA$ are linearly independent
            \vspace{0.3em}
        \item $\rank(\boldA) = N$
            \vspace{0.3em}
        \item $\Span(\boldA) = \RR^N$
            \vspace{0.3em}
        \item If $\boldA \boldx = \boldA \boldy$, then $\boldx = \boldy$
            \vspace{0.3em}
        \item If $\boldA \boldx = \boldzero$, then $\boldx = \boldzero$
            \vspace{0.3em}
        \item For each $\boldb \in \RR^N$, the equation $\boldA \boldx = \boldb$
            has a solution
            \vspace{0.3em}
        \item For each $\boldb \in \RR^N$, the equation $\boldA \boldx = \boldb$
            has a unique solution
    \end{enumerate}
    %


\end{frame}



\begin{frame}

    All equivalent ways of saying that $T\boldx = \boldA \boldx$ is a bijection!

    \vspace{1em} 

    \Eg For condition 5 the equivalence is
    % 
    \begin{align*}
        \text{ if $\boldA \boldx = \boldA \boldy$,} & \text{ then $\boldx = \boldy$ }
         \\
         & \iff \text{ if $T \boldx = T \boldy$, then $\boldx = \boldy$ }
         \\
         &\iff  \text{ $T$ is one-to-one }
         \\
 \text{Since $T$ is a linear map from} & \text{ $\RR^N$ to $\RR^N$, }
         \\
         &\iff \text{ $T$ is a bijection }
    \end{align*}


\end{frame}

\begin{frame}
    
    \Eg For condition 6 the equivalence is
    % 
    \begin{align*}
        \text{ if $\boldA \boldx = \boldzero$,} & \text{ then $\boldx = \boldzero$ }
         \\
         &\iff \setntn{\boldx}{\boldA \boldx = \boldzero} =\{ \boldzero \}
         \\
         &\iff \setntn{\boldx}{T \boldx = \boldzero} =\{ \boldzero \}
         \\
         &\iff \ker(T) = \{ \boldzero \} 
         \\
 \text{Since $T$ is a linear map from} & \text{ $\RR^N$ to $\RR^N$, }
         \\
         &\iff \text{ $T$ is a bijection }
    \end{align*}


\end{frame}

\begin{frame}
    
    
    \Eg For condition 7 the equivalence is 
    
    \begin{align*}
        \text{for each $\boldb \in \RR^N$, } & \text{the equation $\boldA \boldx = \boldb$
            has a solution}
         \\
         &\iff \text{ every $\boldb \in \RR^N$ has an $\boldx$ such that $\boldA\boldx = \boldb$ }
         \\
         &\iff \text{ every $\boldb \in \RR^N$ has an $\boldx$ such that $T\boldx = \boldb$ }
         \\
         &\iff \text{ $T$ is onto}
         \\
 \text{Since $T$ is a linear } & \text{map from $\RR^N$ to $\RR^N$, }
         \\
         &\iff \text{ $T$ is a bijection }
    \end{align*}


\end{frame}



\begin{frame}
    
    Now consider condition 2:
    
    \begin{center}
        The columns of $\boldA$ are linearly independent
    \end{center}

    Let $\bolde_n$ be the $n$-th canonical basis vector in $\RR^N$

    Observe that $\boldA \bolde_n = \col_n (\boldA)$
    %
    \vspace{0.5em}
    $$
    \fore T\bolde_n = \col_n (\boldA)
    $$
    \vspace{-0.5em}
    $$
    \fore 
    V := \{T \bolde_1, \ldots, T\bolde_N\}
    = \text{ columns of $\boldA$}
    $$

    And $V$ is linearly independent if and only if $T$ is a bijection

\end{frame}


\begin{frame}

    \Eg Consider a one good linear market system 
    %
    \begin{align*}
        q & = a - b p \qquad (\text{demand}) \\
        q & = c + d p  \qquad (\text{supply})
    \end{align*}

    Treating $q$ and $p$ as the unknowns, let's write in matrix form as 
    %
    $$
    \begin{pmatrix}
        1 & b \\
        1 & -d
    \end{pmatrix}
    \begin{pmatrix}
        q\\
        p
    \end{pmatrix}
    =
    \begin{pmatrix}
        a \\
        c 
    \end{pmatrix}
    $$

    A unique solution exists whenever the columns are linearly independent

    \begin{itemize}
        \item means that $(b, -d)$ is not a scalar multiple of $\boldone$
        \item means that $b \not= -d$ 
    \end{itemize}

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{not_multiple_of_one.pdf}}
        \caption{$(b, -d)$ is not a scalar multiple of $\boldone$}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}[fragile]
    
    \Eg Recall when we try to solve the system $\boldA \boldx = \boldb$
    of this form

\begin{pythoncode}
In [1]: import numpy as np
In [2]: from scipy.linalg import solve

In [3]: A = [[0, 2, 4],
   ...:      [1, 4, 8],
   ...:      [0, 3, 6]]

In [4]: b = (1, 2, 0)

In [5]: A, b = np.asarray(A), np.asarray(b)

In [6]: solve(A, b)
\end{pythoncode}


\end{frame}


\begin{frame}[fragile]
    
    This is the output that we got

    \begin{verbatim}
LinAlgError        Traceback (most recent call last)
<ipython-input-8-4fb5f41eaf7c> in <module>()
----> 1 solve(A, b)
/home/john/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.pyc in solve(a, b, sym_pos, lower, overwrite_a, overwrite_b, debug, check_finite)
     97         return x
     98     if info > 0:
---> 99         raise LinAlgError("singular matrix")
    100     raise ValueError('illegal value in %d-th argument of internal gesv|posv'
LinAlgError: singular matrix
    \end{verbatim}

    The problem is that $\boldA$ is singular (not nonsingular)

    \begin{itemize}
        \item In particular, $\col_3(\boldA) = 2 \col_2(\boldA)$
    \end{itemize}


\end{frame}








\begin{frame}
    \frametitle{Inverse Matrices}
    

    Given square matrix $\boldA$, suppose $\exists$ square matrix $\boldB$ such
    that 
    $$\boldA \boldB = \boldB \boldA = \boldI$$  
    %
    Then
    %
    \begin{itemize}
        \item $\boldB$ is called the \navy{inverse} of $\boldA$, and written $\boldA^{-1}$
        \item $\boldA$ is called \navy{invertible}
    \end{itemize}

            \vspace{0.3em} 

    \vspace{1em}

    \Fact A square matrix $\boldA$ is nonsingular if and only if it is
    invertible

    Remark
    %
    \begin{itemize}
        \item $\boldA^{-1}$ is just the matrix corresponding to the linear map $T^{-1}$
    \end{itemize}
    
\end{frame}

\begin{frame}

    \Fact Given nonsingular $N \times N$ matrix $\boldA$ and $\boldb \in \RR^N$, the unique
    solution to $\boldA \boldx = \boldb$ is given by 
    %
    $$    \boldx_b := \boldA^{-1} \boldb $$
    %

    \vspace{0.4em}
    
    Proof: Since $\boldA$ is nonsingular we already know any solution is
    unique

    \begin{itemize}
        \item $T$ is a bijection, and hence one-to-one
        \item if $\boldA \boldx = \boldA \boldy = \boldb$ then $\boldx = \boldy$
    \end{itemize}
    
    To show that $\boldx_b$ is indeed a solution we need to show that 
        $\boldA \boldx_b = \boldb$ 

    To see this, observe that
    %
    $$
    \boldA \boldx_b  = \boldA \boldA^{-1} \boldb = \boldI \boldb = \boldb
    $$

    
\end{frame}


\begin{frame}

    \Eg Recall the one good linear market system 
    %
    $$
    \begin{array}{c}
        q = a - b p  \\
        q = c + d p
    \end{array}
    \quad \iff \quad
    \begin{pmatrix}
        1 & b \\
        1 & -d
    \end{pmatrix}
    \begin{pmatrix}
        q\\
        p
    \end{pmatrix}
    =
    \begin{pmatrix}
        a \\
        c 
    \end{pmatrix}
    $$

    Suppose that $a=5$, $b=2$, $c=1$, $d=1.5$
    
    The matrix system is $\boldA \boldx = \boldb$ where
    %
    $$
    \boldA :=
    \begin{pmatrix}
        1 & 2 \\
        1 & -1.5
    \end{pmatrix},
    \;
    \boldx :=
    \begin{pmatrix}
        q\\
        p
    \end{pmatrix},
    \;
    \boldb :=
    \begin{pmatrix}
        5 \\
        1 
    \end{pmatrix}
    $$

    Since $b \not= -d$ we can solve for the unique solution

    Easy by hand but let's try on the computer

\end{frame}


\begin{frame}[fragile]
    
    \begin{pythoncode}
In [1]: import numpy as np
In [2]: from scipy.linalg import inv

In [3]: A = [[1, 2],
    ...:     [1, -1.5]]

In [4]: b = [5, 1]

In [5]: q, p = np.dot(inv(A), b)  # A^{-1} b

In [6]: q
Out[6]: 2.7142857142857144
In [7]: p
Out[7]: 1.1428571428571428
    \end{pythoncode}

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{simple_mkt.pdf}}
        \caption{Equilibrium $(p^*, q^*)$ in the one good case}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \Fact In the $2 \times 2$ case, the inverse has the form

    $$
        \left(
        \begin{array}{cc}
            a & b  \\
            c & d  \\
        \end{array}
        \right)^{-1} = 
        \frac{1}{ad - bc}
        \left(
        \begin{array}{cc}
            d & -b  \\
            -c & a  \\
        \end{array}
        \right)
    $$
    %

    \vspace{1em}

    \Eg 

    $$
    \boldA = 
        \left(
        \begin{array}{cc}
            1 & 2  \\
            1 & -1.5  \\
        \end{array}
        \right)
        \quad \implies \quad
        \boldA^{-1} = 
        \frac{1}{-3.5}
        \left(
        \begin{array}{cc}
            -1.5 & -2  \\
            -1 & 1  \\
        \end{array}
        \right)
    $$
    %

\end{frame}



\begin{frame}
    
    \Eg Consider the $N$ good linear demand system
    %
    \begin{equation}
        \label{eq:ds}
        q_n = \sum_{k=1}^N a_{nk} p_k + b_n,
        \quad n = 1, \ldots N
    \end{equation}

    Task: take quantities $q_1, \ldots, q_N$ as given and find
    corresponding prices $p_1, \ldots, p_N$ --- the ``inverse demand curves"

    We can write \eqref{eq:ds} as 
    %
    $$
    \boldq = \boldA \boldp + \boldb
    $$
    where vectors are $N$-vectors and $\boldA$ is $N \times N$

    If the columns of $\boldA$ are linearly independent then a unique solution
    exists for each fixed $\boldq$ and $\boldb$, and is given by
    %
    $$
    \boldp = \boldA^{-1} (\boldq - \boldb)
    $$
    



\end{frame}



\begin{frame}
    \frametitle{Left and Right Inverses}
    


    \vspace{0.5em}

    Given square matrix $\boldA$, a matrix $\boldB$ is called 
    % 
    \begin{itemize}
        \item a \navy{left inverse} of $\boldA$ if $\boldB \boldA = \boldI$
        \item a \navy{right inverse} of $\boldA$ if $\boldA \boldB = \boldI$
    \end{itemize}

    By definition, a matrix that is both an left inverse and a right inverse is an inverse

    \vspace{0.5em}

    \Fact  If square matrix $\boldB$ is either a left or right inverse for
    $\boldA$, then $\boldA$ is nonsingular and $\boldA^{-1} = \boldB$

    \vspace{1em}

    In other words, for square matrices,
    %
    \vspace{-0.4em}
    \begin{center}
        left inverse $\iff$ right inverse $\iff$ inverse
    \end{center}


\end{frame}



\begin{frame}
    \frametitle{Rules for Inverses}
    
    \Fact If $\boldA$ is nonsingular and $\alpha \not= 0$, then
    %
    \begin{enumerate}
        \item $\boldA^{-1}$ is nonsingular and $(\boldA^{-1})^{-1} = \boldA$
        \item $\alpha \boldA$ is nonsingular and $(\alpha \boldA)^{-1} = \alpha^{-1} \boldA^{-1}$
    \end{enumerate}

    \vspace{1em}

    Proof of part 2: 
    
    It suffices to show that $\alpha^{-1} \boldA^{-1}$ is the right inverse of 
    $\alpha \boldA$

    \vspace{1em}

    This is true because
    %
    $$
        \alpha \boldA \alpha^{-1} \boldA^{-1} 
        =
        \alpha \alpha^{-1} \boldA \boldA^{-1} 
        = \boldI
    $$
    

\end{frame}




\begin{frame}

    \Fact If $\boldA$ and $\boldB$ are $N \times N$ and nonsingular then
    %
    \begin{enumerate}
        \item $\boldA \boldB$ is also nonsingular
            \vspace{0.5em}
        \item $(\boldA \boldB)^{-1} = \boldB^{-1} \boldA^{-1}$
    \end{enumerate}

    Proof I: Let $T$ and $U$ be the linear maps corresponding to $\boldA$ and
    $\boldB$

    Recall that
    
    \begin{itemize}
        \item $T \circ U$ is the linear map corresponding to $\boldA \boldB$
        \item Compositions of linear maps are linear
        \item Compositions of bijections are bijections
    \end{itemize}

    Hence $T \circ U$ is a linear bijection with $(T \circ U)^{-1} = U^{-1} \circ T^{-1}$

    That is, $\boldA\boldB$ is nonsingular with inverse $\boldB^{-1} \boldA^{-1}$

\end{frame}

\begin{frame}
    
    Proof II: 
    
    A different proof that $\boldA \boldB$ is nonsingular with inverse 
    $\boldB^{-1} \boldA^{-1}$

    Suffices to show that $\boldB^{-1} \boldA^{-1}$ is the right inverse of $\boldA \boldB$

    To see this, observe that 
    %
    $$
        \boldA \boldB \boldB^{-1} \boldA^{-1}
        = \boldA  \boldA^{-1}
        = \boldI
    $$

    Hence $\boldB^{-1} \boldA^{-1}$ is a right inverse as claimed

\end{frame}






\section{The Singular Case}

\begin{frame}
    \frametitle{When the Conditions Fail}

    Suppose as before we have 
    %
    \begin{itemize}
        \item an $N \times N$ matrix $\boldA$ 
        \item an $N \times 1$ vector $\boldb$ 
    \end{itemize}

    \vspace{0.3em}

    We seek a solution $\boldx$ to the equation $\boldA \boldx = \boldb$

    \vspace{0.3em}

    What if $\boldA$ is \underline{singular}?

    \vspace{0.3em}

    Then $T\boldx = \boldA \boldx$ is not a bijection, and in fact
    %
    \begin{itemize}
        \item $T$ cannot be onto (otherwise it's a bijection)
        \vspace{0.3em}
        \item $T$ cannot be one-to-one (otherwise it's a bijection)
    \end{itemize}

        \vspace{0.3em}

    Hence neither existence nor uniqueness is guaranteed

\end{frame}


\begin{frame}
    
    \Eg The matrix $\boldA$ with columns
    %
    $$
    \bolda_1 :=
        \begin{pmatrix}
            3 \\
            4 \\
            2
        \end{pmatrix},
        \quad
    \bolda_2 :=
        \begin{pmatrix}
            3 \\
            -4 \\
            1
        \end{pmatrix}
    \quad \text{and} \quad
    \bolda_3 :=
        \begin{pmatrix}
            -3 \\
            4 \\
            -1
        \end{pmatrix}
    $$
    %
    is singular ($\bolda_3 = - \bolda_2$)

    \vspace{1em}

    Its column space $\Span(\boldA)$ is just a plane in $\RR^2$

    Recall $\boldb \in \Span(\boldA)$ 
    %
    \begin{itemize}
        \item[] $\iff$ $\exists \, x_1, \ldots, x_N$ such that $\sum_{k=1}^N
            x_k \col_k(\boldA) = \boldb$
            \vspace{0.5em}
        \item[] $\iff$ $\exists \, \boldx$ such that $\boldA \boldx = \boldb$
    \end{itemize}
    

    \vspace{1em}

    Thus if $\boldb$ is not in this plane then $\boldA \boldx = \boldb$ has no
    solution


\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{not_in_span.pdf}}
           \caption{The vector $\boldb$ is not in $\Span(\boldA)$}
       \end{center}
    \end{figure}

\end{frame}

\begin{frame}

    When $\boldA$ is $N \times N$ and singular how rare is scenario $\boldb \in
    \Span(\boldA)$?
    
    Answer: In a sense, very rare

    We know that $\dim(\Span(\boldA)) < N$
    
    Such sets are always ``very small" subset of $\RR^N$ in terms of ``volume"

    \begin{itemize}
        \item A $K < N$ dimensional subspace has ``measure zero" in $\RR^N$
        \item A ``randomly chosen" $\boldb$ has zero probability of being in such
            a set
    \end{itemize}
    
    \Eg Consider the case where $N = 3$ and $K=2$
    
    A two-dimensional linear subspace is a 2D plane in $\RR^3$

    This set has no volume because planes have no ``thickness''

\end{frame}


\begin{frame}

    All this means that if $\boldA$ is singular then existence
    of a solution to $\boldA \boldx = \boldb$ typically fails

    In fact the problem is worse --- uniqueness fails as well

    \vspace{1em}

    \Fact If $\boldA$ is a singular matrix and $\boldA \boldx = \boldb$ has a
    solution then it has an infinity (in fact a continuum) of solutions

    Proof: Let $\boldA$ be singular and let $\boldx$ be a solution

    Since $\boldA$ is singular there exists a nonzero $\boldy$ with $\boldA
    \boldy = \boldzero$

    But then $\alpha \boldy + \boldx$ is also a solution for any $\alpha \in
    \RR$ because
    %
    $$ 
        \boldA(\alpha \boldy + \boldx) 
        = \alpha \boldA \boldy + \boldA \boldx
        = \boldA \boldx = \boldb
    $$

\end{frame}



\section{Determinants}


\begin{frame}
    \frametitle{Determinants}
    
    Let $S(N)$ be set of all bijections from $\{1, \ldots, N\}$ to itself

    For $\pi \in S(N)$ we define the \navy{signature} of $\pi$ as 
    %
    \begin{equation*}
        \sgn(\pi) := \prod_{m < n} \frac{\pi(m) - \pi(n)}{m - n}
    \end{equation*}
    %

    The \navy{determinant} of $N \times N$ matrix $\boldA$ is then given as
    %
    \begin{equation*}
        \det(\boldA) 
        := \sum_{\pi \in S(N)} \sgn(\pi) \prod_{n=1}^N a_{\pi(n) n}
    \end{equation*}

    \begin{itemize}
        \item You don't need to understand or remember this for our course
    \end{itemize}
    
\end{frame}


\begin{frame}
    
    \Fact In the $N = 2$ case this definition reduces to
    
    $$
    \det 
        \left(
        \begin{array}{cc}
            a & b  \\
            c & d  \\
        \end{array}
        \right)
        = ad - bc
    $$
    %

    \vspace{1em}

    \begin{itemize}
        \item Remark: But you do need to remember this $2 \times 2$ case
    \end{itemize}

    \vspace{1em}

    \begin{example}
        
    %
    \begin{equation*}
        \det 
        \left(
        \begin{array}{cc}
            2 & 0  \\
            7 & -1  \\
        \end{array}
        \right)
        = (2 \times -1) - (7 \times 0) = -2
    \end{equation*}
    %

    \end{example}

\end{frame}


\begin{frame}

    Important facts concerning the determinant

    \vspace{1em}
    
    \Fact If $\boldI$ is the $N \times N$ identity, $\boldA$ and $\boldB$ are $N
    \times N$ matrices and $\alpha \in \RR$, then
    %
    \begin{enumerate}
        \item $\det(\boldI) = 1$  
            \vspace{0.6em}
        \item $\boldA$ is nonsingular if and only if $\det(\boldA)
            \not= 0$
            \vspace{0.6em}
        \item $\det(\boldA\boldB) = \det(\boldA)
            \det(\boldB)$
            \vspace{0.6em}
        \item $\det(\alpha \boldA) = \alpha^N \det(\boldA)$
            \vspace{0.6em}
        \item $\det(\boldA^{-1}) = (\det(\boldA))^{-1}$
    \end{enumerate}


\end{frame}




\begin{frame}

    \Eg Thus singularity in the $2 \times 2$ case is equivalent to
    %
    $$
    \det(\boldA)
    =
    \det 
        \left(
        \begin{array}{cc}
            a_{11} & a_{12}  \\
            a_{21} & a_{22}  \\
        \end{array}
        \right)
        = a_{11}a_{22} - a_{12} a_{21} = 0
    $$
    %

    \vspace{1.5em}

    \Ex Let $\bolda_i := \col_i(\boldA)$ and assume that $a_{ij} \not= 0$ for each $i, j$

    Show the following are equivalent:
    %
    \begin{enumerate}
        \item $a_{11}a_{22} = a_{12} a_{21}$
        \item $\bolda_1 = \lambda \bolda_2$ for some $\lambda \in \RR$
    \end{enumerate}
    %
    
\end{frame}




\begin{frame}[fragile]
    
    \begin{pythoncode}
In [1]: import numpy as np

In [2]: A = np.random.randn(2, 2)  # Random matrix

In [3]: A
Out[3]: 
array([[-0.70120551,  0.57088203],
       [ 0.40757074, -0.72769741]])

In [4]: np.linalg.det(A)
Out[4]: 0.27759063032043652

In [5]: 1.0 / np.linalg.det(np.linalg.inv(A))
Out[5]: 0.27759063032043652
    \end{pythoncode}

\end{frame}


\begin{frame}
    
    As an exercise, let's now show that any right inverse is an inverse

    Fix square $\boldA$ and suppose $\boldB$ is a right inverse:
    %
    \begin{equation}
        \label{eq:ud}
        \boldA \boldB = \boldI
    \end{equation}

    Applying the determinant to both sides gives $\det(\boldA) \det(\boldB) = 1$

    Hence $\boldB$ is nonsingular (why?) and we can
    %
    \begin{enumerate}
        \item multiply \eqref{eq:ud} by $\boldB$ to get $\boldB \boldA \boldB = \boldB$
        \item then postmultiply by $\boldB^{-1}$ to get $\boldB \boldA = \boldI$
    \end{enumerate}

    We see that $\boldB$ is also left inverse, and therefore an inverse of $\boldA$ 
    
    \Ex Do the left inverse case 

\end{frame}








\section{Other Linear Equations}

\begin{frame}
    \frametitle{Other Linear Equations}

    So far we have considered the nice $N \times N$ case for equations

    \begin{itemize}
        \item number of equations $=$ number of unknowns
    \end{itemize}


    \vspace{0.51em}

    We have to deal with other cases too

    Underdetermined systems: 

    \begin{itemize}
        \item eqs $<$ unknowns
    \end{itemize}

    Overdetermined systems: 

    \begin{itemize}
        \item eqs $>$ unknowns
    \end{itemize}

\end{frame}




\begin{frame}
    \frametitle{Overdetermined Systems}

    Consider the system $\boldA \boldx = \boldb$ where $\boldA$ is $N \times K$ and $K < N$

    \vspace{1em}
    
    \begin{itemize}
        \item The elements of $\boldx$ are the unknowns 
            \vspace{0.5em}
        \item More equations than unknowns ($N > K$)
    \end{itemize}

    \vspace{1em}

    May not be able to find an $\boldx$ that satisfies all $N$ equations  

    \vspace{1em}

    Let's look at this in more detail...

\end{frame}


\begin{frame}

    Fix $N \times K$ matrix $\boldA$ with $K < N$

    Let $T \colon \RR^K \to \RR^N$ be defined by $T \boldx = \boldA \boldx$

    \vspace{1em}

    We know these to be equivalent:
    %
    \begin{enumerate}
        \item there exists an $\boldx \in \RR^K$ with $\boldA \boldx = \boldb$
            \vspace{0.3em}
        \item $\boldb$ has a preimage under $T$
            \vspace{0.3em}
        \item $\boldb$ is in $\range(T)$
            \vspace{0.3em}
        \item $\boldb$ is in $\Span(\boldA)$
    \end{enumerate}

    \vspace{1em}

    We also know $T$ \underline{cannot} be onto (maps small to big space)

    Hence $\boldb \in \Span(\boldA)$ will not always hold

\end{frame}


\begin{frame}

    Given our assumption that $K < N$, how rare is the scenario $\boldb \in
    \Span(\boldA)$?

    \vspace{1em}
    
    Answer: We talked about this before --- it's very rare

    \vspace{1em}
    We know that $\dim(\range(T)) = \dim(\Span(\boldA)) \leq K < N$
    
    \vspace{1em}

    A $K < N$ dimensional subspace has ``measure zero" in $\RR^N$

\end{frame}


\begin{frame}

    So should we give up on solving $\boldA \boldx = \boldb$ in the
    overdetermined case?

    What's typically done is we try to find a best approximation

    To define ``best'' we need a way of ranking approximations

    The standard way is in terms of Euclidean norm

    In particular, we search for the $\boldx$ that solves
    
    $$ \min_{\boldx \in \RR^K} \| \boldA \boldx - \boldb\|$$

    Details later

\end{frame}

\begin{frame}
    \frametitle{Underdetermined Systems}

    Now consider $\boldA \boldx = \boldb$ when $\boldA$ is $N \times K$ and $K > N$

    Let $T \colon \RR^K \to \RR^N$ be defined by $T \boldx = \boldA \boldx$

    Now $T$ maps from a larger to a smaller place
    
    This tells us that $T$ is not one-to-one

    Hence solutions are not in general unique

    In fact the following is true

    \Ex Show that $\boldA \boldx =
    \boldb$ has a solution and $K > N$, then the same equation has an infinity of solutions

    \vspace{0.3em}

    Remark: Working with underdetermined systems is relatively rare in economics / elsewhere

\end{frame}


\begin{frame}
    \frametitle{Transpose}

    The \navy{transpose} of $\boldA$ is the matrix $\boldA'$ defined by 
    $$\col_n(\boldA') = \row_n(\boldA)$$

    
    \Egs If
    %
    \begin{equation*}
        \label{eq:aandb}
        \boldA := 
        \left(
        \begin{array}{cc}
            10 & 40  \\
            20 & 50  \\
            30 & 60
        \end{array}
        \right)
        \quad \text{then} \quad
        \boldA' = 
        \left(
        \begin{array}{ccc}
            10 & 20 & 30 \\
            40 & 50 & 60 
        \end{array}
        \right)
    \end{equation*}
    %

    If
    %
    \begin{equation*}
        \boldB := 
        \left(
        \begin{array}{ccc}
            1 & 3 & 5 \\
            2 & 4 & 6 \\
        \end{array}
        \right)
        \quad \text{then} \quad
        \boldB' := 
        \left(
        \begin{array}{cc}
            1 & 2  \\
            3 & 4  \\
            5 & 6 
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}



\begin{frame}
    
    \Fact For conformable matrices $\boldA$ and $\boldB$, transposition satisfies
    %
    \begin{enumerate}
        \item $(\boldA')' = \boldA$
            \vspace{0.4em}
        \item $(\boldA \boldB)' = \boldB' \boldA'$
            \vspace{0.4em}
        \item $(\boldA + \boldB)' = \boldA' +  \boldB'$
            \vspace{0.4em}
        \item $(c \boldA)' = c \boldA'$ for any constant $c$
    \end{enumerate}
    %

    \vspace{1em}


    For each square matrix $\boldA$, 
    %
    \begin{enumerate}
        \item $\det(\boldA') = \det(\boldA)$
            \vspace{0.4em}
        \item If $\boldA$ is nonsingular then so is $\boldA'$, and $(\boldA')^{-1}= (\boldA^{-1})'$
    \end{enumerate}
    %

\end{frame}


\begin{frame}[fragile]

    \begin{pythoncode}
In [1]: import numpy as np

In [2]: A = np.random.randn(2, 2)

In [3]: np.linalg.inv(A.transpose())
Out[3]: 
array([[ 4.52767206, -1.83628665],
       [ 0.90504942,  1.5014984 ]])

In [4]: np.linalg.inv(A).transpose()
Out[4]: 
array([[ 4.52767206, -1.83628665],
      [ 0.90504942,  1.5014984 ]])
    \end{pythoncode}
    
\end{frame}


\begin{frame}
    
    A square matrix $\boldA$ is called \navy{symmetric} if $\boldA' = \boldA$

    \vspace{1em}

    Equivalent: $a_{nk} = a_{kn}$ for all $n, k$

    \vspace{1em}

    \Egs
    
    \begin{equation*}
        \boldA 
        := 
        \left(
        \begin{array}{cc}
            10 & 20  \\
            20 & 50  
        \end{array}
        \right),
        \qquad
        \boldB 
        := 
        \left(
        \begin{array}{ccc}
            1 & 2 & 3  \\
            2 & 0 & 0 \\ 
            3 & 0 & 2 
        \end{array}
        \right)
    \end{equation*}
    %

    \vspace{1em}
    
    \Ex For any matrix $\boldA$, show that $\boldA' \boldA$ and $\boldA \boldA'$ are always
    %
    \begin{enumerate}
        \item well-defined (multiplication makes sense)
        \item symmetric
    \end{enumerate}
    

\end{frame}




\begin{frame}
    
    The \navy{trace} of a square matrix is defined by 
    %
    \vspace{0.5em}
    \begin{equation*}
        \trace \left(
        \begin{array}{ccc}
            a_{11} &  \cdots & a_{1N} \\
            \vdots & &  \vdots \\
            a_{N1} &  \cdots & a_{NN} \\
        \end{array}
        \right)
        = 
        \sum_{n=1}^N a_{nn}
    \end{equation*}
    %

    \vspace{0.5em}

    \Fact $\trace(\boldA) = \trace(\boldA')$

    \vspace{0.5em}

    \Fact If $\boldA$ and $\boldB$ are square matrices and
    $\alpha, \beta \in \RR$, then 
    %
    \begin{equation*}
        \trace(\alpha \boldA + \beta \boldB) 
        = \alpha \trace(\boldA) + \beta \trace(\boldB)
    \end{equation*}
    %

    \vspace{0.5em}

    \Fact When conformable, $\trace(\boldA \boldB) = \trace(\boldB \boldA)$


\end{frame}


\begin{frame}
    
    A square matrix $\boldA$ is called \navy{idempotent} if $\boldA \boldA = \boldA$

    \vspace{1em}

    \Egs
    
    \begin{equation*}
        \boldA 
        := 
        \left(
        \begin{array}{cc}
            1 & 1  \\
            0 & 0  
        \end{array}
        \right),
        \qquad
        \boldI 
        := 
        \left(
        \begin{array}{ccc}
            1 & 0 & 0  \\
            0 & 1 & 0 \\ 
            0 & 0 & 1 
        \end{array}
        \right)
    \end{equation*}
    %

    \vspace{1em}

    The next result is often used in statistics / econometrics:

    \vspace{1em}

    \Fact If $\boldA$ is idempotent, then $\rank(\boldA) = \trace(\boldA)$

\end{frame}




\section{Diagonal Matrices}

\begin{frame}
    \frametitle{Diagonal Matrices}

    Consider a square $N \times N$ matrix $\boldA$

    \vspace{1em}

    The $N$ elements of the form $a_{nn}$ are called the \navy{principal diagonal}


    \vspace{1em}

    
    \begin{equation*}
        \left(
        \begin{array}{cccc}
            \red{a_{11}} & a_{12} & \cdots & a_{1N} \\
            a_{21} & \red{a_{22}} & \cdots & a_{2N} \\
            \vdots & \vdots &  & \vdots \\
            a_{N1} & a_{N2} & \cdots & \red{a_{NN}} \\
        \end{array}
        \right)
    \end{equation*}
    %

\end{frame}


\begin{frame}
    
    A square matrix $\boldD$ is called \navy{diagonal} if all entries off the
    principal diagonal are zero

    \begin{equation*}
        \boldD = 
        \left(
        \begin{array}{cccc}
            d_1 & 0 & \cdots & 0 \\
            0 & d_2 & \cdots & 0 \\
            \vdots & \vdots &  & \vdots \\
            0 & 0 & \cdots & d_N \\
        \end{array}
        \right)
    \end{equation*}

    \vspace{1em}

    Often written as
    
    \begin{equation*}
        \boldD = \diag(d_1, \ldots, d_N) 
    \end{equation*}

\end{frame}


\begin{frame}[fragile]

    Incidentally, the same notation works in Python
    
    \begin{pythoncode}
In [1]: import numpy as np

In [2]: D = np.diag((2, 4, 6, 8, 10))

In [3]: D
Out[3]: 
array([[ 2,  0,  0,  0,  0],
       [ 0,  4,  0,  0,  0],
       [ 0,  0,  6,  0,  0],
       [ 0,  0,  0,  8,  0],
       [ 0,  0,  0,  0, 10]])
    \end{pythoncode}

\end{frame}

\begin{frame}
    
    Diagonal systems are very easy to solve

    \vspace{0.5em}

    \Eg

    \begin{equation*}
        \begin{pmatrix}
            d_1 & 0  & 0 \\
            0 & d_2 & 0 \\
            0 & 0 & d_3 
        \end{pmatrix}
        \begin{pmatrix}
            x_1  \\
            x_2  \\
            x_3 
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_1  \\
            b_2  \\
            b_3 
        \end{pmatrix}
    \end{equation*}

    is equivalent to

    \begin{equation*}
        \begin{array}{c}
            d_1 x_1  = b_1  \\
            d_2x_2 = b_2 \\
            d_3 x_3 = b_3
        \end{array}
    \end{equation*}

\end{frame}



\begin{frame}
    
    \Fact If $\boldC = \diag(c_1, \ldots, c_N)$ and $\boldD = \diag(d_1, \ldots,
    d_N)$ then
    %
    \begin{enumerate}
        \item $\boldC + \boldD = \diag(c_1 + d_1, \ldots, c_N + d_N)$
            \vspace{0.4em}
        \item $\boldC \boldD = \diag(c_1 d_1, \ldots, c_N d_N)$
            \vspace{0.4em}
        \item $\boldD^k = \diag(d^k_1, \ldots, d^k_N)$ for any $k \in \NN$
            \vspace{0.4em}
        \item $d_n \geq 0$ for all $n$ $\implies$ $\boldD^{1/2}$ exists and equals
            %
            $$\diag(\sqrt{d_1}, \ldots, \sqrt{d_N})$$
            \vspace{-1.5em}
        \item $d_n \not= 0$ for all $n$ $\implies$ $\boldD$ is nonsingular and 
            %
            $$\boldD^{-1} = \diag(d_1^{-1}, \ldots, d_N^{-1})$$
    \end{enumerate}
    %

    \vspace{1em}

    Proofs: Check 1 and 2 directly, other parts follow

\end{frame}


\begin{frame}[fragile]
    
    \begin{pythoncode}
In [1]: import numpy as np

In [2]: D = np.diag((2, 4, 10, 100))

In [3]: np.linalg.inv(D)
Out[3]: 
array([[ 0.5 ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.25,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.1 ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.01]])
   \end{pythoncode}

\end{frame}

\begin{frame}

    A square matrix is called \navy{lower triangular} if every element strictly above the
    principle diagonal is zero

    \Eg
    %
    \begin{equation*}
        \label{eq:ltute}
        \boldL :=
        \left(
        \begin{array}{ccc}
            1 & 0 & 0  \\
            2 & 5 & 0 \\
            3 & 6 & 1
        \end{array}
        \right)
    \end{equation*}
    %

    A square matrix is called \navy{upper triangular} if every element
    strictly below the principle diagonal is zero

    \Eg
    %
    \begin{equation*}
        \boldU :=
        \left(
        \begin{array}{ccc}
            1 & 2 & 3  \\
            0 & 5 & 6 \\
            0 & 0 & 1
        \end{array}
        \right)
    \end{equation*}
    %

    Called \navy{triangular} if either upper or lower triangular

\end{frame}


\begin{frame}
    
  Associated linear equations also simple to solve 

  \vspace{0.7em}

    \Eg
    % 
    \begin{equation*}
        \left(
        \begin{array}{ccc}
            4 & 0 & 0  \\
            2 & 5 & 0 \\
            3 & 6 & 1
        \end{array}
        \right)
        \left(
        \begin{array}{ccc}
            x_1 \\
            x_2 \\
            x_3 
        \end{array}
        \right)
        =
        \left(
        \begin{array}{c}
            b_1 \\
            b_2 \\
            b_3 \\
        \end{array}
        \right)
    \end{equation*}
    
    becomes

    \begin{equation*}
        \begin{array}{c}
            4x_1  = b_1  \\
            2x_1 + 5x_2 = b_2 \\
            3x_1 + 6x_2 + x_3 = b_3
        \end{array}
    \end{equation*}
    %

    Top equation involves only $x_1$, so can solve for it directly

    Plug that value into second equation, solve out for $x_2$, etc.

\end{frame}



\section{Eigenvalues}

\begin{frame}
    \frametitle{Eigenvalues and Eigenvectors}

    Let $\boldA$ be $N \times N$  
    
    In general $\boldA$ maps $\boldx$ to some arbitrary new location $\boldA \boldx$

    But sometimes $\boldx$ will only be \underline{scaled}:
    %
    \begin{equation}
        \label{eq:eiei}
        \boldA \boldx = \lambda \boldx
        \quad \text{for some scalar $\lambda$}
    \end{equation}
    %

    If \eqref{eq:eiei} holds and $\boldx$ is nonzero, then 
    %
    \begin{enumerate}
        \item $\boldx$ is called an \navy{eigenvector} of $\boldA$
            and $\lambda$ is called an \navy{eigenvalue}
            \vspace{0.3em}
        \item $(\boldx, \lambda)$ is called an \navy{eigenpair}
    \end{enumerate}

    Clearly $(\boldx, \lambda)$ is an eigenpair of $\boldA$ $\implies$
    $(\alpha \boldx, \lambda)$ is an eigenpair of $\boldA$ for any nonzero $\alpha$

\end{frame}


\begin{frame}
    
    \Eg Let
    %
    $$
    \boldA :=
    \begin{pmatrix}
        1 & -1 \\
        3 & 5
    \end{pmatrix}
    $$

    Then
    
    $$
    \lambda = 2 
    \quad \text{ and } \quad
    \boldx
    =
    \begin{pmatrix}
        1 \\
        -1
    \end{pmatrix}
    $$

    form an eigenpair because $\boldx \not= \boldzero$ and

    $$
    \boldA \boldx =
    \begin{pmatrix}
        1 & -1 \\
        3 & 5
    \end{pmatrix}
    \begin{pmatrix}
        1 \\
        -1
    \end{pmatrix}
    =
    \begin{pmatrix}
        2 \\
        -2
    \end{pmatrix}
    = 2
    \begin{pmatrix}
        1 \\
        -1
    \end{pmatrix}
    =
    \lambda \boldx 
    $$

\end{frame}


\begin{frame}[fragile]

    \Eg
    
\begin{pythoncode}
In [3]: import numpy as np
In [4]: A = [[1, 2],
   ...:      [2, 1]]

In [5]: eigvals, eigvecs = np.linalg.eig(A)

In [6]: x = eigvecs[:,0]  # Let x = first eigenvector
In [7]: lm = eigvals[0]   # Let lm = first eigenvalue

In [8]: np.dot(A, x)      # Compute Ax
Out[8]: array([ 2.12132034,  2.12132034])
In [9]: lm * x            # Compute lm x 
Out[9]: array([ 2.12132034,  2.12132034])
\end{pythoncode}


\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{eigenvecs.pdf}}
        \caption{The eigenvectors of $\boldA$}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}

    Consider the matrix 
    %
    \begin{equation*}
        \boldR := 
        \left(
        \begin{array}{cc}
            0 & -1  \\
            1 & 0  \\
        \end{array}
        \right)
    \end{equation*}
    %

    Induces counter-clockwise rotation on any point by $90^{\circ}$

    Hence no point $\boldx$ is scaled
    
    Hence there exists \underline{no} pair $\lambda \in \RR$ and $\boldx \not=
    \boldzero$
    such that
    %
    $$\boldR \boldx = \lambda \boldx$$  
    %
    
    \begin{itemize}
        \item In other words, no \underline{real-valued} eigenpairs exist
    \end{itemize}


\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{rotation_1.pdf}}
        \caption{The matrix $\boldR$ rotates points by $90^{\circ}$}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{rotation_2.pdf}}
        \caption{The matrix $\boldR$ rotates points by $90^{\circ}$}
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}
    
    But $\boldR \boldx = \lambda \boldx$ can hold \underline{if} we allow
    complex values

    \vspace{1em}

    \Eg 
    %
    \begin{equation*}
        \left(
        \begin{array}{cc}
            0 & -1  \\
            1 & 0  \\
        \end{array}
        \right)
        \begin{pmatrix}
            1 \\
            -i
        \end{pmatrix}
        =
        \begin{pmatrix}
            i \\
            1
        \end{pmatrix}
        =
        i
        \begin{pmatrix}
            1 \\
            -i
        \end{pmatrix}
    \end{equation*}

    That is,
    %
    \begin{equation*}
        \boldR \boldx = \lambda \boldx
        \quad \text{for} \quad
        \lambda := i
        \quad \text{and} \quad
        \boldx := 
        \begin{pmatrix}
            1 \\
            -i
        \end{pmatrix}
    \end{equation*}


    Hence $(\boldx, \lambda)$ is an eigenpair provided we admit complex values 
    
    We do, since this is standard

\end{frame}



\begin{frame}
    
    \Fact For any square matrix $\boldA$ 
    %
    \begin{equation*}
        \lambda \text{ is an eigenvalue of } \boldA \; \iff \;
        \det(\boldA - \lambda \boldI) = 0
    \end{equation*}
    %

    \vspace{1em}

    Proof:  Let $\boldA$ by $N \times N$ and let $\boldI$ be the $N \times N$
    identity

    We have
    %
    \begin{align*}
        \det(\boldA - \lambda \boldI) = 0
        & \iff \boldA - \lambda \boldI \text{ is singular}
        \\
        & \iff \exists \, \boldx \not= \boldzero \st
            (\boldA - \lambda \boldI) \boldx = \boldzero
        \\
        & \iff \exists \, \boldx \not= \boldzero \st
        \boldA \boldx = \lambda \boldx
        \\
        & \iff \lambda 
        \text{ is an eigenvalue of } \boldA
    \end{align*}

\end{frame}


\begin{frame}
    
    \Eg In the $2 \times 2$ case,
    %
    \begin{equation*}
        \boldA =
        \left(
        \begin{array}{cc}
            a & b  \\
            c & d  \\
        \end{array}
        \right)
        \quad \implies \quad
        \boldA - \lambda \boldI =
        \left(
        \begin{array}{cc}
            a - \lambda & b  \\
            c & d - \lambda 
        \end{array}
        \right)
    \end{equation*}
    %
    %
    \begin{align*}
        \fore
    \det(\boldA - \lambda \boldI) 
    & = (a - \lambda)(d - \lambda) - bc
    \\
    & = \lambda^2 - (a + d) \lambda + (ad - bc)
    \end{align*}
    
    Hence the eigenvalues of $\boldA$ are given by the two roots of 
    %
    \begin{equation*}
        \lambda^2 - (a + d) \lambda + (ad - bc) = 0
    \end{equation*}
    
    Equivalently,
    %
    \begin{equation*}
        \lambda^2 - \trace(\boldA) \lambda + \det(\boldA) = 0
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Existence of Eigenvalues}

    Fix $N \times N$ matrix $\boldA$ 

    \vspace{0.5em}
    
    \Fact There exist complex numbers $\lambda_1, \ldots, \lambda_N$ such that
    %
    \begin{equation*}
        \det(\boldA - \lambda \boldI) = \prod_{n=1}^N (\lambda_n - \lambda)
    \end{equation*}
    %

    Each such $\lambda_i$  is an eigenvalue of $\boldA$ because
    %
    \begin{equation*}
        \det(\boldA - \lambda_i \boldI) 
        = \prod_{n=1}^N (\lambda_n - \lambda_i) 
        = 0
    \end{equation*}
    %
    
    Important: Not all are necessarily distinct --- there can be repeats

\end{frame}


\begin{frame}
    

    \Fact Given $N \times N$ matrix $\boldA$ with eigenvalues $\lambda_1, \ldots, \lambda_N$
    we have
    %
    \begin{enumerate}
            \vspace{-0.6em}
        \item $\det(\boldA) = \prod_{n=1}^N \lambda_n$
            \vspace{0.4em}
        \item $\trace(\boldA) = \sum_{n=1}^N \lambda_n$
            \vspace{0.4em}
        \item If $\boldA$ is symmetric, then $\lambda_n \in \RR$ for all $n$
            \vspace{0.4em}
        \item If $\boldA = \diag(d_1, \ldots, d_N)$, then $\lambda_n = d_n$ for all $n$
    \end{enumerate}

    \vspace{0.8em}

    Hence $\boldA$ is nonsingular $\iff$ all eigenvalues are nonzero (why?)

    \vspace{0.4em}

    \Fact If $\boldA$ is nonsingular, then 
    %
    $$
    \text{eigenvalues of } \boldA^{-1}
    = 1/\lambda_1, \ldots, 1/\lambda_N
    $$

\end{frame}







\begin{frame}
    \frametitle{Diagonalization}

    Square matrix $\boldA$ is said to be \navy{similar} to square matrix $\boldB$ if 
    %
    $$
    \exists \text{ invertible matrix $\boldP$ such that }
    \boldA = \boldP \boldB \boldP^{-1}
    $$  


    \begin{figure}
       \begin{center}
           \scalebox{0.6}{\input{../tikzfigs/diagonalize.tex}}
       \end{center}
    \end{figure}



\end{frame}


\begin{frame}
    
    \Fact If $\boldA$ is similar to $\boldB$, then $\boldA^t$ is similar to
    $\boldB^t$ for all $t \in \NN$

    \vspace{1em}

    Proof for case $t=2$: 
    %
    \begin{align*}
    \boldA^2
    & = \boldA \boldA \\
    & = \boldP \boldB \boldP^{-1} \boldP \boldB \boldP^{-1} \\
    & = \boldP \boldB \boldB \boldP^{-1} \\
    & = \boldP \boldB^2 \boldP^{-1} 
    \end{align*}

\end{frame}


\begin{frame}
    
    If $\boldA$ is similar to a diagonal matrix, then $\boldA$ is
    called \navy{diagonalizable}

    \vspace{1em}

    \Fact  Let $\boldA$ be diagonalizable with $\boldA = \boldP \boldD
    \boldP^{-1}$ and let
    %
    \begin{enumerate}
        \item $\boldD = \diag(\lambda_1, \ldots, \lambda_N)$
        \item $\boldp_n := \col_n(\boldP)$
    \end{enumerate}
    %
    Then $(\boldp_n, \lambda_n)$ is an eigenpair of $\boldA$ for each $n$

    \vspace{1em}

    Proof: From $\boldA = \boldP \boldD \boldP^{-1}$ we get $\boldA \boldP = \boldP \boldD$
    
    Equating $n$-th column on each side gives 
    %
    $$
    \boldA \boldp_n = \lambda_n \boldp_n
    $$
    
    Moreover $\boldp_n \not= \boldzero$ because $\boldP$ is invertible (which
    facts?)

\end{frame}


\begin{frame}
    
    \Fact  If $N \times N$ matrix $\boldA$ has $N$ distinct eigenvalues
    $\lambda_1, \ldots, \lambda_N$, then
    $\boldA$ is diagonalizable as $\boldA = \boldP \boldD \boldP^{-1}$ where
    %
    \begin{enumerate}
        \item $\boldD = \diag(\lambda_1, \ldots, \lambda_N)$
        \item $\col_n(\boldP)$ is an eigenvector for $\lambda_n$
    \end{enumerate}

    \Eg Let
    $$
    \boldA :=
    \begin{pmatrix}
        1 & -1 \\
        3 & 5
    \end{pmatrix}
    $$

    The eigenvalues of $\boldA$ are 2 and 4, while the eigenvectors are
    %
    $$
    \boldp_1 :=
    \begin{pmatrix}
        1 \\
        -1
    \end{pmatrix}
    \quad \text{and} \quad
    \boldp_2 :=
    \begin{pmatrix}
        1 \\
        -3
    \end{pmatrix}
    $$
    
    Hence
    %
    \begin{equation*}
        \boldA = \boldP \diag(2, 4) \boldP^{-1}
    \end{equation*}

\end{frame}



\begin{frame}[fragile]
    
    \begin{pythoncode}
In [1]: import numpy as np
In [2]: from numpy.linalg import inv

In [3]: A = [[1, -1],
   ...:      [3, 5]]

In [4]: D = np.diag((2, 4))

In [5]: P = [[1, 1],  # Matrix of eigenvectors
   ...:     [-1, -3]]

In [6]: np.dot(P, np.dot(D, inv(P)))  # PDP^{-1} = A?
Out[6]: 
array([[ 1., -1.],
       [ 3.,  5.]])
    \end{pythoncode}

\end{frame}




\section{Matrix Norm}



\begin{frame}
    \frametitle{The Euclidean Matrix Norm}

    The concept of norm is very helpful for working with vectors

    \begin{itemize}
        \item provides notions of distance, similarity, convergence
    \end{itemize}

    How about an analogous concept for matrices?

    \vspace{1em}
    
    Given $N \times K$ matrix $\boldA$, we define 
    %
    \begin{equation*}
        \| \boldA \| :=
        \max \left\{ 
            \frac{\| \boldA \boldx \|}{ \| \boldx \|} \,: \, 
            \boldx \in \RR^K, \; \boldx \not= \boldzero
            \right\}
    \end{equation*}
    %

    \begin{itemize}
        \item LHS is the \navy{matrix norm} of $\boldA$
        \item RHS is ordinary Euclidean vector norms
    \end{itemize}

\end{frame}

\begin{frame}

    In the maximization we can restrict attention to $\boldx$ s.t. $\| \boldx \| = 1$

    To see this let
    %
    \begin{equation*}
        a :=
        \max_{\boldx \not= \boldzero} \frac{\| \boldA \boldx \|}{ \| \boldx \|} 
        \qquad \text{and} \qquad
        b :=
        \max_{\| \boldx \| = 1} \frac{\| \boldA \boldx \|}{ \| \boldx \|} 
        = \max_{\| \boldx \| = 1} \| \boldA \boldx \|
    \end{equation*}
    %

    Evidently $a \geq b$ because max is over a larger domain

    To see the reverse let 
    
    \begin{itemize}
        \item $\boldx_a$ be the maximizer over $\boldx \not= \boldzero$ and
            let $\alpha := 1 / \| \boldx_a\|$
        \item $\boldx_b := \alpha \boldx_a $ 
    \end{itemize}

    Then 
    $$
        b 
        \geq \frac{ \| \boldA \boldx_b \| }{ \| \boldx_b \|}
        = \frac{ \| \alpha \boldA \boldx_a \| }{ \| \alpha \boldx_a \|}
        = \frac{\alpha}{\alpha} \frac{\| \boldA \boldx_a \|}{ \| \boldx_a \|}
        = a
    $$

\end{frame}


\begin{frame}

    \Ex Show that for any $\boldx$ we have $\|\boldA \boldx\| \leq \| \boldA \| \| \boldx \|$

    If $\| \boldA \| < 1$ then $\boldA$ is called \navy{contractive} --- it
    shrinks the norm
    
    \begin{figure}
       \begin{center}
           \scalebox{0.85}{\input{../tikzfigs/contractive.tex}}
       \end{center}
    \end{figure}


\end{frame}



\begin{frame}
    
    The matrix norm has similar properties to the Euclidean norm

    \vspace{1em}

    \Fact For conformable matrices $\boldA$ and $\boldB$, we have
    %
    \begin{enumerate}
        \item $\| \boldA \| = \boldzero$ if and only if all entries of
            $\boldA$ are zero
            \vspace{1em}
        \item $\| \alpha \boldA \| = |\alpha| \| \boldA \|$ for any scalar
            $\alpha$
            \vspace{1em}
        \item $\| \boldA + \boldB \| \leq \| \boldA \| + \| \boldB \|$
            \vspace{1em}
        \item $\| \boldA \boldB \| \leq \| \boldA \| \| \boldB \|$
    \end{enumerate}
    
            \vspace{1em}
    The last inequality is called the submultiplicative property of the matrix
    norm

    For square $\boldA$ it implies that $\|\boldA^k\| \leq \|\boldA \|^k$ for any $k \in \NN$

\end{frame}



\begin{frame}
    
    \Fact For the diagonal matrix
    %
    \begin{equation*}
        \boldD = 
        \diag(d_1, \ldots, d_N) 
        =
        \left(
        \begin{array}{cccc}
            d_1 & 0 & \cdots & 0 \\
            0 & d_2 & \cdots & 0 \\
            \vdots & \vdots &  & \vdots \\
            0 & 0 & \cdots & d_N \\
        \end{array}
        \right)
    \end{equation*}


    we have

    $$ 
        \| \boldD \| = \max_n |d_n|
    $$

\end{frame}



\begin{frame}
    
    Let $\{ \boldA_j \}$ and $\boldA$ be $N \times K$ matrices
    
    \begin{itemize}
        \item If $\| \boldA_j - \boldA \| \to 0$ then we say that $\boldA_j$
            \navy{converges} to $\boldA$
            \vspace{1em}
        \item If $\sum_{j=1}^J \boldA_j$ converges to some matrix
        $\boldB_{\infty}$ as $J \to \infty$ we write
        %
        \begin{equation*}
            \sum_{j=1}^{\infty} \boldA_j = \boldB_{\infty}
        \end{equation*}
    \end{itemize}

    
    \vspace{1em}

    In other words,
    %
    \begin{equation*}
        \boldB_{\infty} = \sum_{j=1}^{\infty} \boldA_j 
        \quad \iff \quad
        \lim_{J \to \infty}
        \;
        \left\| \sum_{j=1}^J \boldA_j  - \boldB_{\infty} \right\|
        \to 0
    \end{equation*}

\end{frame}





\section{Neumann Series}


\begin{frame}
    \frametitle{Neumann Series}

    Consider the difference equation $\boldx_{t+1} = \boldA \boldx_t + \boldb$, where 
    %
    \begin{itemize}
        \item $\boldx_t \in \RR^N$ represents the values of some variables at time $t$
            \vspace{0.4em}
        \item $\boldA$ and $\boldb$ form the parameters in the law of motion for $\boldx_t$
    \end{itemize}
    
    Question of interest: is there an $\boldx$ such that 
    $$
        \boldx_t = \boldx  \; \implies \;  \boldx_{t+1} = \boldx
    $$
    
    In other words, we seek an $\boldx \in \RR^N$ that solves the system of equations
    %
    \begin{equation*}
        \boldx = \boldA \boldx + \boldb, 
        \quad \text{where} \quad \boldA \text{ is } N \times N
        \text{ and } \boldb \text{ is } N \times 1
    \end{equation*}

\end{frame}


\begin{frame}

    We can get some insight from the scalar case $x = a x + b$

    \vspace{1em}
    
    If $|a| < 1$, then this equation has the solution
    %
    \begin{equation*}
        \bar x 
        = \frac{b}{1-a} 
         = b \sum_{k=0}^{\infty} a^k 
    \end{equation*}

    \vspace{1em}
    Does an analogous result hold in the vector case $\boldx = \boldA \boldx + \boldb$?
    
    \vspace{1em}
    Yes, if we replace condition $|a| < 1$ with $\| \boldA \| < 1$ 

\end{frame}


\begin{frame}

    Let $\boldb$ be any vector in $\RR^N$ and $\boldA$ be an $N \times N$ matrix

    The next result is called the \navy{Neumann series lemma} 

    \vspace{1em}
    
    \Fact  If $\| \boldA^k \| < 1$ for some $k \in \NN$, then $\boldI - \boldA$
    is invertible and
    %
    \begin{equation*}
         (\boldI - \boldA)^{-1} = \sum_{j=0}^{\infty} \boldA^j
    \end{equation*}
    %

    \vspace{1em}

    In this case $\boldx = \boldA \boldx + \boldb$ has the unique solution
    %
    \begin{equation*}
        \bar \boldx = \sum_{j=0}^{\infty} \boldA^j  \boldb 
    \end{equation*}
    %

\end{frame}


\begin{frame}

    Sketch of proof that $(\boldI - \boldA)^{-1} = \sum_{j=0}^{\infty} \boldA^j$ 
    for case $\|\boldA \| < 1$
    
    We have $(\boldI - \boldA) \sum_{j=0}^{\infty} \boldA^j =
    \boldI $ because
    %
    \begin{align*}
       \left\|
        (\boldI - \boldA) \sum_{j=0}^{\infty} \boldA^j - \boldI
       \right\|
       & 
       =
       \left\|
        (\boldI - \boldA) \lim_{J \to \infty}\sum_{j=0}^J \boldA^j - \boldI
       \right\|
       \\
       & 
       =
       \lim_{J \to \infty}
       \left\|
        (\boldI - \boldA) \sum_{j=0}^J \boldA^j - \boldI
       \right\|
       \\
       & =
       \lim_{J \to \infty}
       \left\|
        \boldA^J 
       \right\|
       \\
       & \leq
       \lim_{J \to \infty}
       \left\| \boldA \right\|^J = 0
    \end{align*}


\end{frame}


\begin{frame}

    How to test the hypotheses of the Neumann series lemma?

    \vspace{1em}
    
    The \navy{spectral radius} of square matrix $\boldA$ is
    %
    \begin{equation*}
        \rho(\boldA) := \max \setntn{ |\lambda| }
        { \lambda \text{ is an eigenvalue of } \boldA}
    \end{equation*}
    
    \vspace{1em}

    Here $|\lambda|$ is the \navy{modulus} of the possibly complex number $\lambda$

    \vspace{1em}

    \Eg  If $\lambda = a + ib$, then 
    $$|\lambda| = (a^2 + b^2)^{1/2}$$

    \Eg  If $\lambda \in \RR$, then $|\lambda|$ is the absolute value
    
\end{frame}


\begin{frame}
    
    \Fact If $\rho(\boldA) < 1$, then $\| \boldA^j \| < 1$ for some $j \in \NN$    

    \vspace{1em}

    Proof, for diagonalizable $\boldA$:

    \vspace{0.3em}

    We have $\boldA^j = \boldP \boldD^j \boldP^{-1}$ where
    %
    $$
    \boldD = \diag(\lambda_1, \ldots, \lambda_N)
    \quad \text{and hence} \quad
    \boldD^j = \diag(\lambda_1^j, \ldots, \lambda_N^j)
    $$

    Hence
    %
    $$
        \| \boldA^j \|
        = \| \boldP \boldD^j \boldP^{-1} \|
        \leq \| \boldP \|  \| \boldD^j \| \| \boldP^{-1} \|
    $$

    In particular, when $C := \| \boldP \|\| \boldP^{-1} \|$,
    $$
        \| \boldA^j \| 
        \leq C \max_n |\lambda_n^j| 
        = C \max_n |\lambda_n|^j
        = C \rho(\boldA)^j
    $$

    This is $<1$ for large enough $j$ because $\rho(\boldA) < 1$

\end{frame}


\section{Quadratic Forms}

\begin{frame}
    \frametitle{Quadratic Forms}

    Up till now we have studied linear functions extensively

    \vspace{1em}

    Next level of complexity is quadratic maps

    \vspace{1em}
    
    Let $\boldA$ be $N \times N$ and symmetric, and let $\boldx$ be $N \times 1$
    
    \vspace{1em}

    The \navy{quadratic function} on $\RR^N$ associated with $\boldA$ is the
    map 
    %
    \begin{equation*}
        Q \colon \RR^N \to \RR, \qquad
        Q(\boldx) := \boldx' \boldA \boldx = \sum_{j=1}^N \sum_{i=1}^N a_{ij} x_i x_j
    \end{equation*}
    %

    The properties of $Q$ depend on $\boldA$

\end{frame}


\begin{frame}
    
    An $N \times N$ symmetric matrix $\boldA$ is called
    
    \vspace{1em}

    \begin{enumerate}
        \item \navy{nonnegative definite} if $\boldx' \boldA \boldx \geq 0$
            for all $\boldx \in \RR^N$ 
            \vspace{1em}
        \item \navy{positive definite} if $\boldx' \boldA \boldx > 0$ for all $\boldx
            \in \RR^N$ with $\boldx \not= \boldzero$
            \vspace{1em}
        \item \navy{nonpositive definite} if $\boldx' \boldA \boldx \leq 0$
            for all $\boldx \in \RR^N$
            \vspace{1em}
        \item \navy{negative definite} if $\boldx' \boldA \boldx < 0$ for all $\boldx
            \in \RR^N$ with $\boldx \not= \boldzero$
    \end{enumerate}
    %

\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{qform_pd.pdf}}
        \caption{\label{f:qform_pd} A positive definite case: $Q(\boldx) = \boldx' \boldI \boldx$ }
       \end{center}
    \end{figure}

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{qform_nd.pdf}}
        \caption{\label{f:qform_nd} A negative definite case: $Q(\boldx) =
        \boldx'(-\boldI)\boldx$ }
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    Note that some matrices have none of these properties

            \vspace{1em}

    \begin{itemize}
        \item $\boldx' \boldA \boldx < 0$ for some $\boldx$
            \vspace{1em}
        \item $\boldx' \boldA \boldx > 0$ for other $\boldx$
    \end{itemize}

            \vspace{1em}

    In this case $\boldA$ is called \navy{indefinite}

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{qform_indef.pdf}}
        \caption{\label{f:qform_indef} Indefinite quadratic function $Q(\boldx) = x_1^2/2 +
            8 x_1 x_2 + x_2^2/2$ }
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    

    \Fact A symmetric matrix $\boldA$ is 
    %
    \begin{enumerate}
        \item positive definite $\iff$ all eigenvalues are strictly positive
            \vspace{0.4em}
        \item negative definite $\iff$ all eigenvalues are strictly negative
            \vspace{0.4em}
        \item nonpositive definite $\iff$ all eigenvalues are nonpositive
            \vspace{0.4em}
        \item nonnegative definite $\iff$ all eigenvalues are nonnegative
    \end{enumerate}
    %

    \vspace{1em}

    It follows that 
    %
    \begin{itemize}
        \item $\boldA$ is positive definite $\implies$ $\det(\boldA) > 0$ 
    \end{itemize}

    \vspace{1em}
    
    In particular, $\boldA$ is nonsingular

\end{frame}




