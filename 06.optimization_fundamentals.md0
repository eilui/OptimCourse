---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Fundamentals of optimization
**ECON2125/6012 Lecture 2**
Fedor Iskhakov

## Announcements & Reminders

- *Test 1* results and discussion
- *Test 2* due date extended to Sept 6th (23:59)

## Plan for this lecture

1. Open and closed sets (review)
2. Continuity of functions
3. Suprema and infima
4. Existence of optima
5. Multivariate calculus
6. Uniqueness of optima

**Supplementary reading:**
- Simon & Blume: 
- Sundaram: 


## Sequences and limits in $\mathbb{R}^K$

```{admonition} Definition: sequence
:class: caution

A ***sequence*** $\{{\bf x}_n\}$ in $\mathbb{R}^K$ is a function from $\mathbb{N}$ to $\mathbb{R}^K$

```

```{admonition} Definition: Euclidean norm
:class: caution

The (Euclidean) ***norm*** of ${\bf x} \in \mathbb{R}^N$ is defined as
%
$$
\| {\bf x} \| 
:= \sqrt{{\bf x}' {\bf x} } 
= \left( \sum_{n=1}^N x_n^2 \right)^{1/2}
$$
%
```

Recall the properties of the norm from the lecture on analysis, and the corresponding tutorial

Interpretation:
%
- $\| {\bf x} \|$ represents the _length_ of ${\bf x}$
- $\| {\bf x} - {\bf y} \|$ represents distance between ${\bf x}$ and ${\bf y}$

When $K=1$, the norm $\| \cdot \|$ reduces to $|\cdot|$

```{admonition} Definition
:class: caution

A set $A \subset \mathbb{R}^K$ called ***bounded*** if 
%
$$
\exists \, M \in \mathbb{R} 
\; \mathrm{such\;that} \;
\|{\bf x}\| \leq M, \quad \forall \; {\bf x} \in A
$$
%
```

```{admonition} Definition
:class: caution

For $\epsilon > 0$, the $\epsilon$-ball $B_{\epsilon}({\bf a})$ around
${\bf a} \in \mathbb{R}^K$ is all ${\bf x} \in \mathbb{R}^K$ such that $\|{\bf a} - {\bf x}\|
< \epsilon$
```

```{figure} _static/plots/eps_ball2D.png
:name: eps_ball2D
:scale: 50%
```

```{admonition} Fact
:class: important

If ${\bf x}$ is in every $\epsilon$-ball around ${\bf a}$ then
${\bf x}={\bf a}$
```

```{admonition} Fact
:class: important

If ${\bf a} \ne {\bf b}$, then $\exists \, \epsilon > 0$ such that 
$B_\epsilon({\bf a}) \cap B_\epsilon({\bf b}) = \emptyset$
```


```{admonition} Definition
:class: caution

Sequence $\{{\bf x}_n\}$ is said to ***converge*** to ${\bf a} \in \mathbb{R}^K$ if
%
$$
\forall \epsilon > 0, 
\;
\exists \, N \in \mathbb{N}
\; 
\text{ such that }  n \geq N \implies {\bf x}_n \in B_{\epsilon}({\bf a})
$$

```

We say: "$\{{\bf x}_n\}$ is eventually in any $\epsilon$-neighborhood of ${\bf a}$"

In this case ${\bf a}$ is called the ***limit*** of the sequence, and we write
%
$$ 
{\bf x}_n \to {\bf a} \; \text{ as } \; n \to \infty
\quad \text{or} \quad
\lim_{n \to \infty} {\bf x}_n = {\bf a}
$$
%

```{admonition} Definition
:class: caution

We call $\{ {\bf x}_n \}$ ***convergent*** if it converges to some limit in $\mathbb{R}^K$

```


```{figure} _static/plots/convergence.png
:name: convergence
:scale: 80%
```

```{figure} _static/plots/convergence2.png
:name: convergence2
:scale: 80%
```

```{figure} _static/plots/convergence3.png
:name: convergence3
:scale: 80%
```

```{admonition} Fact
:class: important

A sequence $\{{\bf x}_n\}$ in $\mathbb{R}^K$ converges to ${\bf a} \in \mathbb{R}^K$
if and only if each component sequence converges in $\mathbb{R}$ 

That is,
%
$$
\begin{pmatrix}
x^1_n \\
\vdots \\
x^K_n 
\end{pmatrix}
\to
\begin{pmatrix}
a^1 \\
\vdots \\
a^K 
\end{pmatrix}
\quad \text{in } \mathbb{R}^K
\quad \iff \quad
\begin{array}{cc}
x^1_n \to a^1 &  \quad \text{in } \mathbb{R} \\
\vdots        &  \\
x^K_n \to a^K &  \quad \text{in } \mathbb{R} 
\end{array}
$$
%
% Equivalent:
% $$
% {\bf x}_n \to {\bf a} \; \text{ in } \mathbb{R}^K
% \quad \iff\quad  
% {\bf e}_k' {\bf x}_n \to {\bf e}_k' {\bf a} \text{ in $\mathbb{R}$ for all } k
$$
%
```

```{figure} _static/plots/norm_and_pointwise.png
:name: norm_and_pointwise
```

## Open and Closed Sets

Let $G \subset \mathbb{R}^K$

```{admonition} Definition
:class: caution

We call ${\bf x} \in G$ ***interior*** to $G$ if 
$\exists \; \epsilon > 0$ with $B_\epsilon({\bf x}) \subset G$

```

Loosely speaking, interior means "not on the boundary"

```{figure} _static/plots/interior.png
:name: interior
:scale: 50%
```

```{admonition} Example
:class: tip

If $G = (a, b)$ for some $a < b$, then any $x \in (a, b)$ is interior 
```
```{figure} _static/plots/x_interior.png
:name: x_interior
```

````{admonition} Proof
:class: dropdown

Fix any $a < b$ and any $x \in (a, b)$

Let $\epsilon := \min\{x - a, b - x\}$

If $y \in B_\epsilon(x)$ then $y < b$ because 
%
$$
y 
= y + x - x
\leq |y - x| + x
< \epsilon + x 
\leq b - x + x = b
$$
%

````

```{admonition} Example
:class: tip

If $G = [-1, 1]$, then $1$ is not interior 
```

```{figure} _static/plots/not_interior.png
:name: not_interior
```

Intuitively, any $\epsilon$-ball centered on $1$ will contain points $> 1$

More formally, pick any $\epsilon  > 0$ and consider $B_\epsilon(1)$

There exists a $y \in B_\epsilon(1)$ such that $y \notin [-1, 1]$

For example, consider the point $y := 1 + \epsilon/2$

**Exercise:** Check this point: lies in $B_\epsilon(1)$ but not in $[-1, 1]$

```{admonition} Definition
:class: caution

A set $G\subset \mathbb{R}^K$ is called ***open*** if all of its points are interior 

```

```{admonition} Example
:class: tip

Open sets:

- any "open" interval $(a,b) \subset \mathbb{R}$, since we showed all points are interior
- any "open" ball $B_\epsilon({\bf a}) = {\bf x} \in
\mathbb{R}^K : \|{\bf x} - {\bf a} \| < \epsilon$
- $\mathbb{R}^K$ itself
```

```{admonition} Example
:class: tip

Sets that are not open

- $(a,b]$ because $b$ is not interior 
- $[a,b)$ because $a$ is not interior 
```

### Closed Sets

```{admonition} Definition
:class: caution

A set $F \subset \mathbb{R}^K$ is called ***closed*** if every convergent sequence in $F$ converges to a point in $F$

```

Rephrased: If $\{{\bf x}_n\} \subset F$ and ${\bf x}_n \to {\bf x}$ for some
${\bf x} \in \mathbb{R}^K$, then ${\bf x} \in F$

```{admonition} Example
:class: tip

All of $\mathbb{R}^K$ is closed because every sequence converging to a point in $\mathbb{R}^K$ converges to a point in $\mathbb{R}^K$... right?
```

```{admonition} Example
:class: tip

If $(-1, 1) \subset \mathbb{R}$ is {\bf not} closed
```

````{admonition} Proof
:class: dropdown

True because
%
1. $x_n := 1-1/n$ is a sequence in $(-1, 1)$ converging to $1$,
9. and yet $1 \notin (-1, 1)$

````

```{admonition} Example
:class: tip

If $F = [a, b] \subset \mathbb{R}$ then $F$ is closed in $\mathbb{R}$
```

````{admonition} Proof
:class: dropdown

Take any sequence $\{x_n\}$ such that
%
- $x_n \in F$ for all $n$
- $x_n \to x$ for some $x \in \mathbb{R}$

We claim that $x \in F$

Recall that (weak) inequalities are preserved under limits:

- $x_n \leq b$ for all $n$ and $x_n \to x$, so $x \leq b$
- $x_n \geq a$ for all $n$ and $x_n \to x$, so $x \geq a$
%
therefore $x \in [a, b] =: F$

````

```{admonition} Example
:class: tip

Any "hyperplane" of the form 
%
$$
H = \{ {\bf x} \in \mathbb{R}^K : {\bf x}' {\bf a} = c \}
$$ 
%
is closed 
```

````{admonition} Proof
:class: dropdown

Fix ${\bf a} \in \mathbb{R}^K$ and $c \in \mathbb{R}$ and let $H$ be as above

Let $\{{\bf x}_n\} \subset H$ with ${\bf x}_n \to {\bf x} \in \mathbb{R}^K$

We claim that ${\bf x} \in H$

Since ${\bf x}_n \in H$ and ${\bf x}_n \to {\bf x}$ we have 
%
$$
{\bf x}_n ' {\bf a} \to {\bf x}' {\bf a} \text{ in } \mathbb{R}
\quad \text{and} \quad
{\bf x}_n' {\bf a} = c \text{ for all } n
$$
$$
\text{therefore } 
{\bf x}' {\bf a} = \lim_{n} {\bf x}_n' {\bf a} 
= \lim_n c
= c
$$
$$
\text{therefore } 
{\bf x} \in H
$$
%
````

### Properties of Open and Closed Sets

```{admonition} Fact
:class: important

$G \subset \mathbb{R}^K$ is open $\iff \; G^c$ is closed
```

````{admonition} Proof
:class: dropdown

$\implies$ 

First prove necessity

Pick any $G$ and let $F := G^c$

Suppose to the contrary that $G$ is open but $F$ is not closed, so 
%
$\exists$ a sequence $\{{\bf x}_n\} \subset F$ with limit ${\bf x} \notin F$ 

Then ${\bf x} \in G$, and since $G$ open, $\exists \, \epsilon > 0$ such
that $B_\epsilon({\bf x}) \subset G$

Since ${\bf x}_n \to {\bf x}$ we can choose an $N \in \mathbb{N}$ with ${\bf x}_N \in
B_\epsilon({\bf x})$

This contradicts ${\bf x}_n \in F$ for all $n$

$\Longleftarrow$ 

Next prove sufficiency

Pick any closed $F$ and let $G := F^c$, need to prove that $G$ is open

Suppose to the contrary that $G$ is not open

Then exists some non-interior ${\bf x} \in G$, that is no $\epsilon$-ball around $x$ lies entirely in $G$

Then it is possible to find a sequence $\{{\bf x}_n\}$ which converges to $x \in G$, but every element of which lies in the $B_{1/n}({\bf x}) \cap F$

This contradicts the fact that $F$ is closed
````


```{admonition} Example
:class: tip

Any singleton $\{ {\bf x} \} \subset \mathbb{R}^K$ is closed
```

````{admonition} Proof
:class: dropdown

Let's prove this by showing that $\{{\bf x}\}^c$ is open

Pick any ${\bf y} \in \{{\bf x}\}^c$

We claim that ${\bf y}$ is interior to $\{{\bf x}\}^c$

Since ${\bf y} \in \{{\bf x}\}^c$ it must be that ${\bf y} \ne {\bf x}$

Therefore, exists $\epsilon > 0$ such that $B_\epsilon({\bf y}) \cap B_\epsilon({\bf x}) = \emptyset$

In particular, ${\bf x} \notin B_\epsilon({\bf y})$, and hence $B_\epsilon({\bf y}) \subset \{{\bf x}\}^c$

Therefore ${\bf y}$ is interior as claimed

Since ${\bf y}$ was arbitrary it follows that $\{{\bf x}\}^c$ is open and $\{{\bf x}\}$ is closed

````


```{admonition} Fact
:class: important

1. Any *union* of open sets is open
9. Any *intersection* of closed sets is closed
```

````{admonition} Proof
:class: dropdown


*Proof of first fact:*

Let $G := \cup_{\lambda \in \Lambda} G_\lambda$, where each $G_\lambda$ is
open

We claim that any given ${\bf x} \in G$ is interior to $G$

Pick any ${\bf x} \in G$

By definition, ${\bf x} \in G_\lambda$ for some $\lambda$

Since $G_\lambda$ is open, $\exists \, \epsilon > 0$ such that $B_\epsilon({\bf x})
\subset G_\lambda$

But $G_\lambda \subset G$, so $B_\epsilon({\bf x}) \subset G$ also holds

In other words, ${\bf x}$ is interior to $G$ 

````

But be careful:  

- An **infinite** intersection of open sets is not necessarily open
- An **infinite** union of closed sets is not necessarily closed

For example, if $G_n := (-1/n, 1/n)$,  then $\cap_{n \in \mathbb{N}} G_n = \{0\} $  

To see this, suppose that $x \in \cap_n G_n$

Then
%
$$
-1/n < x < 1/n, \quad \forall n \in \mathbb{N}
$$
%
Therefore $x = 0$, and hence $x \in \{0\}$  

On the other hand, if $x \in \{0\}$ then $x \in \cap_n G_n$

```{admonition} Fact
:class: important

If $A$ is closed and bounded then every sequence in
$A$ has a subsequence which converges to a point of $A$
``` 


Take any sequence $\{{\bf x}_n\}$ contained in $A$

Since $A$ is bounded, $\{{\bf x}_n\}$ is bounded

Therefore it has a convergent subsequence

Since the subsequence is also contained in $A$, 
and $A$ is closed, the limit must lie in $A$.

```{admonition} Definition
:class: caution

Bounded and closed sets are called **compact sets** or **compacts**

```


## Continuity

One of the most fundamental properties of functions

Related to existence of 

- optima
- roots
- fixed points
- etc

as well as a variety of other useful concepts

### Reminder on functions >>

```{admonition} Definition
:class: caution

A ***function*** $f \colon A \rightarrow B$ from set $A$ to set $B$ is a rule that
associates to *each* element of $A$ a *uniquely determined* element of $B$
```

```{figure} _static/plots/function.png
:name: function
```

$A$ is called the ***domain*** of $f$ and $B$ is called the ***codomain***

```{figure} _static/plots/allfunctions.png
:name: function_non_function
:scale: 50%

```
Lower panel: functions have to map *all* elements in domain to a *uniquely determined* element in codomain.

```{admonition} Definition
:class: caution

The smallest possible codomain is called the ***range*** of $f \colon A \to B$:

```
$$
\mathrm{rng}(f) := \{ b \in B : f(a) = b \text{ for some } a \in A \} 
$$

```{figure} _static/plots/range.png
:name: range
:scale: 50%
```

```{admonition} Definition
:class: caution

A function $f \colon A \to B$ is called ***onto*** (or surjection) if every element of $B$
is the image under $f$ of at least one point in $A$.  

A function $f \colon A \to B$ is called ***one-to-one*** (or injection) if distinct
elements of $A$ are always mapped into distinct elements of $B$.

A function that is both one-to-one (injection) and onto (surjection) is called a ***bijection*** or ***one-to-one correspondence***
```

```{admonition} Fact
:class: important

 If $f \colon A \to B$ is one-to-one, then $f \colon A \to \mathrm{rng}(f)$ is a bijection

```

```{admonition} Fact
:class: important

If $f \colon A \to B$ a bijection, then there exists a unique
function $\phi \colon B \to A$ such that 
%
$$
\phi(f(a)) = a, \quad \forall \; a \in A
$$

That function $\phi$ is called the ***inverse*** of $f$ and written $f^{-1}$
```


```{figure} _static/plots/bijec.png
:name: bijec

```

### Bounded functions

```{admonition} Definition
:class: caution

A function $F$ is called ***bounded*** if its range is a bounded set.

```

```{admonition} Fact
:class: important

If $F$ and $G$ are bounded, then so are $F+G$, $F \cdot G$ and $\alpha F$ for any finite $\alpha$
```

````{admonition} Proof
:class: dropdown


Proof for the case $F + G$:

Let $F$ and $G$ be bounded functions 

$\exists$ $M_F$ and $M_G$ s.t.
$\| F({\bf x}) \| \leq M_F$ and $\| G({\bf x}) \| \leq M_G$
for all ${\bf x}$

Fix any ${\bf x}$ and let $M := M_F + M_G$ 

Applying the triangle inequality gives
%
$$
\| (F + G)({\bf x}) \|
:= \| F({\bf x}) + G({\bf x}) \|
\leq \| F({\bf x}) \| + \| G({\bf x}) \| 
\leq M
$$
%
Since ${\bf x}$ was arbitrary this bound holds for all ${\bf x}$

````

### Continuous functions

```{admonition} Definition
:class: caution

Let $F \colon A \to \mathbb{R}^J$ where $A$ is a subset of $\mathbb{R}^K$.
$F$ is called ***continuous at*** ${\bf x} \in A$ if as $n \to \infty$
%
$$
{\bf x}_n \to {\bf x}
\quad \implies \quad
F({\bf x}_n) \to F({\bf x}) 
$$
%
```

Requires that 

- $F({\bf x}_n)$ converges for each choice of ${\bf x}_n \to {\bf x}$,
- The limit is always the same, and that limit is $F({\bf x})$

```{admonition} Definition
:class: caution

$F$ is called ***continuous*** if it is continuous at every ${\bf x} \in
A$

```

```{figure} _static/plots/cont_func.png
:name: cont_func

Continuity
```

```{figure} _static/plots/discont_func.png
:name: discont_func

Discontinuity at $x$
```

```{admonition} Example
:class: tip

Let ${\bf A}$ be an $J \times K$ matrix and let $F({\bf x}) = {\bf A}
{\bf x}$

The function $F$ is continuous at every ${\bf x} \in \mathbb{R}^K$
```

To see this take 
%
- any ${\bf x} \in \mathbb{R}^K$ 
- any ${\bf x}_n \to {\bf x}$

By the definition of the matrix norm $\| {\bf A} \|$, we have
%
$$
\| {\bf A} {\bf x}_n - {\bf A} {\bf x} \|
= \| {\bf A} ({\bf x}_n - {\bf x}) \|
\leq \| {\bf A} \| \| {\bf x}_n - {\bf x} \|
$$
%
$$
\text{therefore }
{\bf x}_n \to {\bf x} \implies
{\bf A} {\bf x}_n \to {\bf A} {\bf x} 
$$
%

***Exercise:*** Exactly what rules are we using here?

```{admonition} Fact
:class: important

If $f \colon \mathbb{R} \to \mathbb{R}$ is differentiable at $x$, then $f$ is continuous at $x$
```

%
%**Proof:**  Suppose to the contrary that $f$ discontinuous at $x$

%Then exists $x_n \to x$ with $f(x_n) \nrightarrow f(x)$

%We can and do choose $\{x_n\}$ such that $x_n \ne x$ for all $n$

%Since $f(x_n) \nrightarrow f(x)$, exists $\epsilon > 0$ s.t. $|f(x_n) - f(x)| \geq \epsilon$
%infinitely often

%Letting $h_n := x_n - x$, we have
%%
%$$
%\frac{f(x + h_n) - f(x)}{h_n} 
%= 
%\frac{f(x_n) - f(x)}{x_n - x} 
%$$

%This sequence fails to converge to any constant (why?)

%

```{admonition} Fact
:class: important

Some functions known to be continuous on their domains:

%
- $x \mapsto x^\alpha$
- $x \mapsto |x|$
- $x \mapsto \log(x)$
- $x \mapsto \exp(x)$
- $x \mapsto \sin(x)$
- $x \mapsto \cos(x)$
%
```

```{admonition} Example
:class: tip

Discontinuous at zero: $x \mapsto \mathbb{1}\{x > 0\}$.

```


```{admonition} Fact
:class: important

Let $F$ and $G$ be functions and let $\alpha \in \mathbb{R}$

%
1. If $F$ and $G$ are continuous at ${\bf x}$ then so is $F + G$,
where
%
$$
(F + G)({\bf x}) := F({\bf x}) + G({\bf x})
$$
%

2. If $F$ is continuous at ${\bf x}$ then so is $\alpha F$, where
%
$$
(\alpha F)({\bf x}) := \alpha F({\bf x})
$$
%

3. If $F$ and $G$ are continuous at ${\bf x}$ and real valued then so is
$FG$, where
%
$$
(FG)({\bf x}) := F({\bf x}) \cdot G({\bf x})
$$ 
%
In the latter case, if in addition $G({\bf x}) \ne 0$, then $F/G$ is also continuous.

```


As a result, set of continuous functions is "closed" under elementary
arithmetic operations

```{admonition} Example
:class: tip

The function $F \colon \mathbb{R} \to \mathbb{R}$ defined by
%
$$
F(x) = \frac{\exp(x) + \sin(x)}{2 + \cos(x)} + \frac{x^4}{2}
- \frac{\cos^3(x)}{8!}
$$
%
is continuous
```

````{admonition} Proof
:class: dropdown

Just repeatedly apply the rules on the previous slide

Let's just check that 
%
$$
\text{$F$ and $G$ continuous at ${\bf x}$}
\implies 
\text{$F + G$ continuous at ${\bf x}$}
$$
%

Let $F$ and $G$ be continuous at ${\bf x}$

Pick any ${\bf x}_n \to {\bf x}$

We claim that 
$F({\bf x}_n) + G({\bf x}_n) \to F({\bf x}) + G({\bf x})$

By assumption, $F({\bf x}_n) \to F({\bf x})$ and $G({\bf x}_n) \to G({\bf x})$

From this and the triangle inequality we get
%
$$
\| F({\bf x}_n) + G({\bf x}_n) - (F({\bf x}) + G({\bf x})) \|
\leq 
$$
$$
\leq 
\| F({\bf x}_n) - F({\bf x}) \|
+
\| G({\bf x}_n) - G({\bf x}) \|
\to 0
$$
%
````


## Suprema and Infima



```{code-cell} python3
:tags: [hide-cell]

from myst_nb import glue
import matplotlib.pyplot as plt
import numpy as np

def subplots():
    "Custom subplots with axes through the origin"
    fig, ax = plt.subplots()
    # Set the axes through the origin
    for spine in ['left', 'bottom']:
        ax.spines[spine].set_position('zero')
    for spine in ['right', 'top']:
        ax.spines[spine].set_color('none')
    return fig, ax

xmin, xmax = 0, 1
xgrid1 = np.linspace(xmin, xmax, 100)
xgrid2 = np.linspace(xmax, 2, 10)

fig, ax = subplots()
ax.set_ylim(0, 1.1)
ax.set_xlim(-0.0, 2)
func_string = r'$f(x) = x^2$ if $x < 1$ else $f(x) = 0.5$'
ax.plot(xgrid1, xgrid1**2, 'b-', lw=3, alpha=0.6, label=func_string)
ax.plot(xgrid2, 0 * xgrid2 + 0.5, 'b-', lw=3, alpha=0.6)
#ax.legend(frameon=False, loc='lower right', fontsize=16)
glue("fig_none", fig, display=False)
```

````{admonition} Example: no maximizers
:class: tip

The following function has no maximizers on $[0, 2]$

$$
f(x) = 
\begin{cases}
x^2 &  \text{ if } x < 1
\\
1/2 &  \text{ otherwise}
\end{cases}
$$

```{glue:figure} fig_none
:width: 80%
:align: center

No maximizer on $[0, 2]$
```
````




\begin{frame}
    \frametitle{Fundamentals of Optimization}
    

    In elementary econ / finance courses we get well behaved, prepackaged problems 
    
    Usually they 
    %
    \begin{itemize}
        \item have a solution
            \vspace{0.5em}
        \item the solution is unique and not hard to find
    \end{itemize}
    
            \vspace{0.5em}
    We discussed such problems in the first few lectures
    
            \vspace{0.5em}
    However, when we tackle new proplems such properties aren't guaranteed

            \vspace{0.5em}
    We need some idea of how to check these things 

\end{frame}






\section{Sup and Inf}


    

\begin{frame}
    \frametitle{Suprema and Infima}

    Consider the problem of finding 
    the ``maximum" or ``minimum" of a function

    \vspace{1em}

    A first issue is that such values might not be well defined

            \vspace{0.6em}
    This leads us to start with ``suprema" and ``infima"

    \begin{itemize}
        \item Always well defined
            \vspace{0.6em}
        \item Agree with max and min when the latter exist
    \end{itemize}


\end{frame}



\begin{frame}

    Let $A \subset \RR$
    
    A number $u \in \RR$ is called an \navy{upper bound} of $A$ if 
    %
    \begin{equation*}
        a \leq u  \quad \text{for all} \quad a \in A
    \end{equation*}

    \vspace{0.6em}

    \Eg If $A = (0, 1)$ then 10 is an upper bound of $A$

    \vspace{0.2em}

    \hspace{1em} $\because \quad$ Every element of $(0, 1)$ is $\leq 10$


    \Eg If $A = (0, 1)$ then 1 is an upper bound of $A$

    \vspace{0.2em}

    \hspace{1em} $\because \quad$ Every element of $(0, 1)$ is $\leq 1$

    \Eg If $A = (0, 1)$ then $0.5$ is \underline{not} an upper bound of $A$

    \vspace{0.2em}

    \hspace{1em} $\because \quad$ $0.6 \in (0, 1)$ and $0.5 < 0.6$

\end{frame}



\begin{frame}
    
    Let $U(A) :=$ set of all upper bounds of $A$


    \begin{figure}[h]
       \begin{center}
        \scalebox{.4}{\input{upper_bounds.pdf_t}}
       \end{center}
    \end{figure}


    \Egs 
    
    \begin{itemize}
        \item If $A = [0, 1]$, then $U(A) = [1, \infty)$
            \vspace{0.5em}
        \item If $A = (0, 1)$, then $U(A) = [1, \infty)$
            \vspace{0.5em}
        \item If $A = (0, 1) \cup (2, 3)$, then $U(A) = [3, \infty)$
            \vspace{0.5em}
        \item If $A = \NN$, then $U(A) = \emptyset$
    \end{itemize}

\end{frame}


\begin{frame}
    
    If $s$ is a number satisfying
    %
    \begin{equation*}
      s \in U(A)
      \qquad \text{and} \qquad
      s \leq u, \; \forall \,   u \in U(A)
    \end{equation*}
    % 
    then $s$ is called the \navy{supremum} of $A$ and we write $s = \sup A$
    
    \begin{figure}[h]
       \begin{center}
        \scalebox{.4}{\input{sup.pdf_t}}
       \end{center}
    \end{figure}

    Also called the \navy{least upper bound} of $A$
    \vspace{0.5em}

    \Eg If $A = (0, 1]$, then $U(A) = [1, \infty)$, so $\sup A = 1$
    \vspace{0.5em}

    \Eg If $A = (0, 1)$, then $U(A) = [1, \infty)$, so $\sup A = 1$

\end{frame}




\begin{frame}

    A set $A \subset \RR$ is called \navy{bounded above} if $U(A)$ is not empty

        \vspace{1em}

    \Fact If $A$ is nonempty and bounded above then $A$
        has a supremum in $\RR$

        \vspace{0.8em}

    \begin{itemize}
        \item Equivalent to the fact that all Cauchy sequences converge 
        \vspace{0.3em}
        \item Same principle: $\RR$ has no ``gaps" or ``holes"
    \end{itemize}

        \vspace{1em}

    What if $A$ is not bounded above, so that $U(A) = \emptyset$?

    We follow the convention that $\sup A := \infty$ in this case

    Now the supremum of a nonempty subset of $\RR$ {\bf always} exists

\end{frame}


%\begin{frame}
    
    %Aside: Conventions for dealing with symbols ``$\infty$'' and ``$-\infty$"

    %If $a \in \RR$, then
    %%
    %\begin{itemize}
        %\item $a + \infty = \infty$
        %\item $a - \infty = -\infty$
        %\item $a \times \infty = \infty$ if $a \not= 0$, $0 \times \infty = 0$
        %\item $-\infty < a < \infty$
        %\item $\infty + \infty = \infty$
        %\item $-\infty - \infty = -\infty$
    %\end{itemize}

    %But $\infty - \infty$ is not defined

%\end{frame}


\begin{frame}
    
    \Fact If $A \subset B$, then $\sup A \leq \sup B$


    \begin{figure}[h]
       \begin{center}
        \scalebox{.4}{\input{sup_ab.pdf_t}}
       \end{center}
    \end{figure}

    Proof: Let $A \subset B$
    
    If $\sup B = \infty$ then the claim is trivial so suppose $\bar b = \sup B < \infty$

    By definition, $\bar b \in U(B)$, so $b \leq \bar b$ for all $b \in B$

    Since each $a \in A$ is also in $B$, we then have $a \leq \bar b$ for all $a \in A $

    It follows that $\bar b \in U(A)$
    
    Hence $\sup A \leq \bar b$

\end{frame}


\begin{frame}
    
    Let $A$ be any set bounded from above and let $s := \sup A$

    \Fact There exists a sequence $\{x_n\}$ in $A$ with $x_n \to s$

    Proof: Note that 
    %
    $$
    \forall \, n \in \NN, \;\; \exists \, x_n \in A \; \st \; x_n > s - \frac{1}{n}
    $$

    \begin{figure}[h]
       \begin{center}
        \scalebox{.4}{\input{sup_seq.pdf_t}}
       \end{center}
    \end{figure}

    (Otherwise $s$ is not a sup, because $s-\frac{1}{n}$ is a smaller upper bound)

    The sequence $\{x_n\}$ lies in $A$ and converges to $s$

\end{frame}



\begin{frame}
    
    A \navy{lower bound} of $A \subset \RR$ is any $\ell \in \RR$ with $\ell \leq a$ for all $a \in A$  

    If $i \in \RR$ is an lower bound for $A$ with $i \geq \ell$ for every
    lower bound $\ell$ of $A$, then $i$ is called the
    \navy{infimum} of $A$  

    Write $i = \inf A$

    \Egs

    \begin{itemize}
        \item If $A = [0, 1]$, then  $\inf A = 0$
        \item If $A = (0, 1)$, then  $\inf A = 0$
    \end{itemize}
    
    \Fact Every nonempty subset of $\RR$ bounded from below has an infimum

    If $A$ is unbounded below then we set $\inf A = -\infty$

    
\end{frame}






\section{Max and Min}



\begin{frame}
    \frametitle{Maxima and Minima of Sets}

    In optimization we're mainly interested in maximizing / minimizing
    functions

    If we maximize a function, say, then the problem looks like
    %
    \begin{equation*}
        \max_{\boldx \in A} f(\boldx)
    \end{equation*}

    As we'll see, the problem is the same as finding the largest number in the range of $f$

    That is, the largest number in the set
    %
    \begin{equation*}
        f(A) := \setntn{f(\boldx)}{\boldx \in A}
    \end{equation*}

    So let's start by thinking about the largest value in a set

\end{frame}



\begin{frame}
    
    We call $a^*$ the \navy{maximum} of $A \subset \RR$ and write $a^* = \max A$ if
    %
    $$
        a^* \in A 
        \qquad \text{and} \qquad
        a \leq a^*
        \text{ for all } 
        a \in A 
    $$
    %

    \begin{itemize}
        \item \Eg If $A = [0, 1]$ then $\max A = 1$
    \end{itemize}

    \vspace{1em}

    We call $a^*$ the \navy{minimum} of $A \subset \RR$ and write $a^* = \min A$ if
    %
    $$
        a^* \in A 
        \qquad \text{and} \qquad
        a^* \leq a
        \text{ for all } 
        a \in A 
    $$
    %

    \begin{itemize}
        \item \Eg If $A = [0, 1]$ then $\min A = 0$
    \end{itemize}


\end{frame}

\begin{frame}
    \frametitle{Existence of Max and Min}

    %If $A \subset \RR$ is finite then $\max A$ and $\min A$ always exist

    %\vspace{0.5em}

    %\Eg 
    
    %\begin{itemize}
        %\item $\max\{2, 4, 6, 8\} = 8$
        %\vspace{0.5em}
        %\item $\min\{2, 4, 6, 8\} = 2$
    %\end{itemize}
    

    %\vspace{1em}
    
    %Common notation for max and min over pairs:
   %%
   %\begin{itemize}
       %\item $x \vee y := \max\{x, y\}$ 
        %\vspace{0.5em}
       %\item $x \wedge y := \min\{x, y\}$
   %\end{itemize}


%\end{frame}


%\begin{frame}

    %Some facts about max and min for pairs:

    %\vspace{1em}
    
    %\Fact For any $x, y \in \RR$ and $a \in \RR_+ := [0, \infty)$, we have
    %%
    %\begin{enumerate}
        %\item $x + y = x \vee y + x \wedge y$
        %\item $|x - y| = x \vee y - x \wedge y$
        %\item $|x - y| = x + y - 2 (x \wedge y)$
        %\item $|x - y| = 2 ( x \vee y) -x -y$
        %\item $a(x \vee y) = (ax ) \vee (ay)$
        %\item $a(x \wedge y) = (ax ) \wedge (ay)$
    %\end{enumerate}

%\end{frame}

%\begin{frame}

    %Let's prove that $x + y = x \vee y + x \wedge y$

    %Pick any $x, y \in \RR$

    %Suppose first that $x \leq y$

    %Then 
    %% 
    %$$x \vee y + x \wedge y = y + x$$
    
    %Suppose instead that $x > y$

    %Then 
    %% 
    %$$x \vee y + x \wedge y = x + y$$
    

%\end{frame}




%\begin{frame}

    For infinite subsets of $\RR$, max and min may not exist  
    
    \vspace{1em}

    \Eg $\max \NN$ does not exist

    Suppose to the contrary that $n^* = \max \NN$

    By the definition of the maximum, $n^* \in \NN$

    Now consider 
    %
    \begin{equation*}
        n^{**} := n^* + 1
    \end{equation*}

    Clearly 
    %
    \begin{equation*}
        n^{**} \in \NN
        \quad \text{and} \quad 
        n^{**} > n^*
    \end{equation*}

    This contradicts the definition of $n^*$

\end{frame}


\begin{frame}
    
    \Eg $\max (0, 1)$ does not exist

    \vspace{0.6em}

    Suppose to the contrary that $a^* = \max (0, 1)$

    By the definition of the maximum, $a^* \in (0, 1)$
    
    Hence $a^* < 1$

    Now consider 
    %
    \begin{equation*}
        a^{**} := (1 + a^*)/2
    \end{equation*}

    Clearly 
    $$
        a^{**} \in (0, 1) \text{ and } a^{**} > a^*
    $$

    Contradicts hypothesis that $a^*$ is the maximum

\end{frame}



\begin{frame}
    \frametitle{Max/Min vs Sup/Inf}

    When max and min exist they agree with sup and inf 

    \vspace{1em}
    
    \Facts Let $A$ be any subset of $\RR$
    
    \begin{enumerate}
        \item If $\sup A \in A$, then $\max A$ exists and $\max A = \sup A$
            \vspace{0.5em}
        \item If $\inf A \in A$, then $\min A$ exists and $\min A = \inf A$
    \end{enumerate}

    \vspace{1em}

    Proof of case 1: Let $a^* := \sup A$ and suppose $a^* \in A$

    We want to show that $\max A = a^*$

    Since $a^* \in A$, we need only show that $a \leq a^*$ for all $a \in A$

    This follows from $a^* = \sup A$, which implies $a^* \in U(A)$

\end{frame}







\section{Existence of Optima}





\begin{frame}
    \frametitle{Existence of Max and Min for Sets}

    \vspace{1em}

    \Fact If $F \subset \RR$ is a closed and bounded, then 
    $\max F$ and $\min F$ both exist

    \vspace{1em}

    Proof for the max case:

    Since $F$ is bounded, 
    
    \begin{itemize}
        \item $\sup F$ exists 
        \vspace{0.5em}
        \item $\exists$ a sequence $\{x_n\} \subset F$ with $x_n \to \sup F$
    \end{itemize}

    Since $F$ is closed, this implies that $\sup F \in F$

    Hence $\max F$ exists and $\max F = \sup F$

\end{frame}








%\begin{frame}

    %Let $A \subset \RR^K$, let $F \colon A \to \RR^J$ and let $K \subset A$

    %Let  $F(K) := \setntn{\boldy \in \RR^J}{\boldy = F (\boldx) \text{ for some } \boldx \in A}$

    %\vspace{0.4em}

    %\Fact If $K$ is closed and bounded and $F$ is continuous, then $F(K)$ is
    %also closed and bounded

    %\vspace{-1em}

    %\begin{figure}
       %\begin{center}
        %\scalebox{.4}{\includegraphics{compact_image.pdf}}
       %\end{center}
    %\end{figure}



%\end{frame}


%\begin{frame}
    
    %Proof: Let $F$ and $K$ be as specified
    
    %\vspace{0.4em}

    %We claim every sequence in $F(K)$ has a subsequence converging
    %to a point in $F(K)$

    %\vspace{0.4em}
    
    %Let $\{\boldy_n\} \subset F(K)$ 
    
    %\vspace{0.2em}
    %By definition, we can take $\{\boldx_n\} \subset K$ with $F(\boldx_n) = \boldy_n$ for each $n$

    %\vspace{0.2em}
    %Since $K$ compact, $\exists$ subsequence $\{\boldx_{n_k}\}$ with $\boldx_{n_k} \to \boldx \in K$

    %\vspace{0.2em}
    %By continuity of $F$ we have $F (\boldx_{n_k}) \to F (\boldx)$

    %\vspace{0.2em}
    %Since $\boldx \in K$ we have $F (\boldx) \in F(K)$ 

    %\vspace{0.2em}
    %In summary, $\boldy_{n_k} = F (\boldx_{n_k}) \to$ a point in $K$

%\end{frame}






\section{Optimizing Functions}


\begin{frame}
    \frametitle{Optimizing Functions}

    Now we turn to extrema (sup / max / etc.) for functions

    This is not a new concept --- it's just about extrema of sets

    ...but the sets are the range of functions

    \vspace{0.5em}

    In particular
    %
    \begin{itemize}
        \item The sup of a function $f$ is just the sup of its range
            \vspace{0.5em}
        \item The max of a function $f$ is just the max of its range
            \vspace{0.5em}
        \item etc.
    \end{itemize}

    Througout we use the notation
    %
    $$
        f(A) := \setntn{f(\boldx)}{\boldx \in A}
    $$

\end{frame}


\begin{frame}
    \frametitle{Sup and Inf for Functions}

    \vspace{0.5em}

    Let $f \colon A \to \RR$, where $A$ is any set 

    \vspace{1.5em}

    The \navy{supremum of $f$ on $A$} is defined as
    %
    \begin{equation*}
         \sup_{\boldx \in A} f(\boldx) 
         := \sup f(A)
    \end{equation*}

    \vspace{0.8em}

    The \navy{infimum of $f$ on $A$} is defined as
    % 
    \begin{equation*}
        \inf_{\boldx \in A} f(\boldx) 
        := \inf f(A)
    \end{equation*}

\end{frame}

\begin{frame}

    \begin{figure}
       \begin{center}
        \scalebox{.5}{\input{func_sup.pdf_t}}
       \end{center}
       \caption{The supremum of $f$ on $A$}
    \end{figure}

\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.5}{\input{func_inf.pdf_t}}
       \end{center}
       \caption{The infimum of $f$ on $A$}
    \end{figure}

\end{frame}





\begin{frame}
    \frametitle{Max and Min for Functions}

    Let $f \colon A \to \RR$ where $A$ is any set

    \vspace{1em}

    The \navy{maximum} of $f$ on $A$ is defined as 
    %
    \begin{equation*}
        \max_{\boldx \in A} f(\boldx) 
        := \max f(A)
    \end{equation*}


    \vspace{1em}

    The \navy{minimum} of $f$ on $A$ is defined as 
    %
    \begin{equation*}
        \min_{\boldx \in A} f(\boldx) 
        := \min f(A)
    \end{equation*}

\end{frame}




\begin{frame}
    
    A \navy{maximizer} of $f$ on $A$ is a point $\bolda^* \in A$ such that 
    $$
        f(\bolda^*) = \max_{\boldx \in A} f(\boldx)
    $$
    %

    Equivalent:
    %
    \begin{equation*}
        \bolda^* \in A \text{ and } f(\bolda^*) \geq f(\boldx) \text{ for all
        } \boldx \in A
    \end{equation*}


    \vspace{1em}

    The set of all maximizers denoted by 
    
    $$\argmax_{\boldx \in A}f(\boldx)$$

\end{frame}



\begin{frame}

    A \navy{minimizer} of $f$ on $A$ is a point $\bolda^* \in A$ such that 
    $$
        f(\bolda^*) = \min_{\boldx \in A} f(\boldx)
    $$

    Equivalent:
    %
    \begin{equation*}
        \bolda^* \in A \text{ and } f(\bolda^*) \leq f(\boldx) \text{ for all
        } \boldx \in A
    \end{equation*}

    \vspace{1em}
   
    The set of all minimizers denoted by 
    
    $$\argmin_{\boldx \in A}f(\boldx)$$

\end{frame}




\begin{frame}

    Now we come to the famous {\bf Weierstrass extreme value theorem}

    \vspace{0.6em}

    \Fact If $f$ is continuous and $A$ is closed and bounded, then $f$ has both a
    maximizer and a minimizer in $A$

    \vspace{0.6em}

    Proof sketch for the max case: 

    Can show under the assumptions that $f(A)$ is closed and bounded

    \begin{itemize}
        \item proof uses Bolzano--Weierstrass theorem, details omitted
    \end{itemize}

    Hence the max of $f(A)$ exists, and we can write
    %
    $$
        M^* := \max f(A) := \max \setntn{f(\boldx)}{\boldx \in A}
    $$

    The point $\boldx^* \in A$ such that $f(\boldx^*) = M^*$ is a maximizer

\end{frame}



\begin{frame}
    
        
    \Eg Consider the problem
    $$ 
        \max_{c_1, c_2} \; U(c_1, c_2) :=  \sqrt{c_1} + \beta \sqrt{c_2}  
    $$
    \vspace{-1em}
    $$
        \st \; c_2 \leq (1 + r)(w - c_1), 
        \quad c_i \geq 0 \text{ for } i = 1,2
    $$
    where
    %
    \begin{itemize}
        \item $r=$ interest rate, $w=$ wealth, $\beta=$ discount factor
        \item all parameters $> 0$ 
    \end{itemize}

    Let $B$ be all $(c_1, c_2)$ satisfying the constraint

    \Ex Show that the budget set $B$ is a closed, bounded subset of $\RR^2$

    \Ex Show that $U$ is continuous on $B$

    We conclude that a maximizer exists

\end{frame}










\section{Properties of Optima}


\begin{frame}
    \frametitle{Properties of Optima}

    We now state some useful facts regarding optima

    \vspace{0.6em}
    Sometimes we state properties about sups and infs

    \begin{itemize}
        \item rather than max and min
    \end{itemize}
    
    \vspace{0.6em}

    This is so we don't have to keep saying ``if it exsits"

    \vspace{0.6em}

    But remember that if it does exist then the same properties apply

    \begin{itemize}
        \item if a max exists, then it's a sup, etc.
    \end{itemize}

\end{frame}





\begin{frame}
    
    \Fact If $A \subset B$ and $f \colon  B \to \RR$, then
    %
    $$
        \sup_{\boldx \in A} f(\boldx) \leq \sup_{\boldx \in B} f(\boldx)
        \qquad \text{and} \qquad
        \inf_{\boldx \in A} f(\boldx) \geq \inf_{\boldx \in B} f(\boldx)
    $$


    \begin{figure}[h]
       \begin{center}
        \scalebox{.4}{\input{sup_ab_func.pdf_t}}
       \end{center}
    \end{figure}
    

\end{frame}


\begin{frame}
    
    Proof, for the sup case: 
    
    Let $A$, $B$ and $f$ be as in the statement of the fact

    We already know that $C \subset D \implies \sup C \leq \sup D$

    Hence it suffices to show that $f(A) \subset f(B)$, because then
    %
    \begin{equation*}
        \sup_{\boldx \in A} f(\boldx) 
        := \sup f(A)
        \leq \sup f(B) 
        =: \sup_{\boldx \in B} f(\boldx)
    \end{equation*}

    To see that $f(A) \subset f(B)$, take any $y \in f(A)$

    By definition, $\exists \, \boldx \in A$ such that $f(\boldx) = y$

    Since $A \subset B$ we must have $\boldx \in B$ 
    
    So $f(\boldx) = y$ for some $\boldx \in B$, and hence $y \in f(B)$

    Thus $f(A) \subset f(B)$ as was to be shown

\end{frame}


\begin{frame}

    \Eg ``If you have more choice then you're better off"

    \vspace{0.5em}

    Consider the problem of maximizing utility
    %
    $$
        U(x_1, x_2) = \alpha \log(x_1) + \beta \log(x_2)
    $$
    %
    over all $(x_1, x_2)$ in the budget set
    %
    $$
        B(m) 
        := \left\{ 
            (x_1, x_2) \in \RR^2
            \;:\;
            x_i > 0 \text{ and } p_1 x_1 + p_2 x_2 \leq m
        \right\}
    $$
    %
    Thus, we solve
    %
    $$
        \max_{\boldx \in B(m)} U(\boldx)
    $$

    Clearly $m \leq m' \implies B(m) \subset B(m')$

    \vspace{0.5em}
    
    Hence the maximal value goes up as $m$ increases

\end{frame}


\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{bset1.pdf}}
           \caption{Budget set $B(m)$}
       \end{center}
    \end{figure}

\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{bset2.pdf}}
           \caption{Budget set $B(m')$}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \Eg Let $y_n$ be income and $x_n$ be years education

    Consider regressing income on education:
    %
    $$
        y_n = \alpha + \beta x_n + \epsilon_n  
    $$
    
    We have data for $n = 1, \ldots, N$ individuals

    Successful regression is often associated with large $R^2$ 

    \begin{itemize}
        \item A measure of ``goodness of fit"
    \end{itemize}
    
    Large $R^2$ occurs when we have a small sum of squared residuals
    %
    \begin{equation*}
        {\rm ssr}_a := 
        \min_{\alpha, \beta} 
        \; \sum_{n=1}^N \, (y_n - \alpha - \beta x_n)^2
    \end{equation*}

\end{frame}

\begin{frame}
    
    However, we can always reduce the ssr by including irrelevant variables

    \begin{itemize}
        \item e.g., $z_n = $ consumption of bacon in kgs per annum
    \end{itemize}
    %
    \begin{equation*}
        {\rm ssr}_b := 
        \min_{\alpha, \beta, \gamma} 
        \; \sum_{n=1}^N \, (y_n - \alpha - \beta x_n - \gamma z_n)^2
        \, \leq {\rm ssr}_a
    \end{equation*}

    Proof: Let 
    %
    \begin{equation*}
        \boldtheta 
        := (\alpha, \beta, \gamma),
        \qquad
        f(\boldtheta) 
        := 
        \sum_{n=1}^N \, (y_n - \alpha - \beta x_n - \gamma z_n)^2
    \end{equation*}
    %
    Then 
    %
    \begin{equation*}
        {\rm ssr}_b 
        = \min_{\boldtheta \in \RR^3} f(\boldtheta)
        \leq 
        \min_{\substack{\boldtheta \in \RR^3 \\ \gamma = 0}} f(\boldtheta)
        = {\rm ssr}_a
    \end{equation*}

\end{frame}



\begin{frame}
    
    \Fact If $f \colon A \to \RR$, then 
    %
    $$
        \bolda^* \in \argmax_{\boldx \in A}f(\boldx)
        \; \iff \;
        \bolda^* \in \argmin_{\boldx \in A} -f(\boldx)
    $$

    \vspace{-1em}

    \begin{figure}
       \begin{center}
        \scalebox{.45}{\includegraphics{max_min.pdf}}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}

    Proof: Let's prove that, when $g = -f$,
    %
    $$
        \bolda^* \in \argmax_{\boldx \in A}f(\boldx)
        \; \implies \;
        \bolda^* \in \argmin_{\boldx \in A}g(\boldx)
    $$

    To begin, let $\bolda^*$ be a maximizer of $f$ on $A$

    Then, for any given $\boldx \in A$ we have $f(\bolda^*) \geq f(\boldx)$
    \vspace{0.3em}
    $$
    \fore
        -f(\bolda^*) \leq -f(\boldx)
    $$
    \vspace{-0.3em}
    $$
    \fore
        g(\bolda^*) \leq g(\boldx)
    $$

    Hence $\bolda^*$ is a minimizer of $g$ on $A$

    \begin{itemize}
        \item because the last inequality was shown for any $\boldx \in A$
    \end{itemize}


\end{frame}


\begin{frame}[fragile]

    \Eg Most numerical routines provide minimization only

    \vspace{0.5em}
    Suppose we want to maximize $f(x) = 3 \ln x - x$ on $(0, \infty)$

    \vspace{0.5em}
    We can do this by finding the minimizer of $-f$

    \vspace{0.5em}

    \begin{pythoncode}
In [1]: from scipy.optimize import fminbound
In [2]: import numpy as np

In [3]: f = lambda x: 3 * np.log(x) - x
In [4]: g = lambda x: -f(x)  # Find min of -f

In [5]: fminbound(g, 1, 100)
Out[5]: 3.0000015012062393
    \end{pythoncode}

\end{frame}





\begin{frame}

    Given $A \subset \RR^K$, let 
    %
    \begin{itemize}
        \item $f \colon A \to B \subset \RR$ 
        \item $h \colon B \to \RR$ and $g := h \circ f$ 
    \end{itemize}
    
    \Fact If $h$ is strictly increasing, then 
    $$\argmax_{\boldx \in A}f(\boldx) =\argmax_{\boldx \in A}g(\boldx)$$

    \vspace{0.5em}

    Proof of $\subset$: Let $\bolda^* \in \argmax_{\boldx \in A}f(\boldx)$ 
    
    If $\boldx \in A$, then $f(\boldx) \leq f(\bolda^*)$, and hence $h(f(\boldx)) \leq h(f(\bolda^*)) \quad$ 

    In other words, $g(\boldx) \leq g(\bolda^*)$ for any $\boldx \in A$

    Hence $\bolda^* \in \argmax_{\boldx \in A} g(\boldx)$ as claimed
    


\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.45}{\includegraphics{max_preserved.pdf}}
        \caption{Increasing transform $h(x) = \exp(x/2)$ preserves the maximizer}
       \end{center}
    \end{figure}

\end{frame}




\begin{frame}

    \Eg A well known statistical problem is to maximize the exponential likelihood
    function:
    %
    \begin{equation*}
        \max_{\lambda > 0} L(\lambda)
        \quad \text{where} \quad
        L(\lambda) 
        := \lambda^N \exp \left(-\lambda \sum_{n=1}^N x_n \right)
    \end{equation*}

    It's easier to maximize the log-likelihood function
    %
    \begin{equation*}
        \ell(\lambda) 
        := \log(L(\lambda))
        = N \log(\lambda) - \lambda \sum_{n=1}^N x_n 
    \end{equation*}

    The unique solution
    %
    \begin{equation*}
        \hat \lambda := \frac{N}{\sum_{n=1}^N x_n}
    \end{equation*}
    
    is also the unique maximiser of $L(\lambda)$
    

\end{frame}


\begin{frame}

    In the next few slides
    %
    \begin{enumerate}
        \item $A$ is any set
        \vspace{0.5em}
        \item $f$ is some function from $A$ to $\RR$
        \vspace{0.5em}
        \item $g$ is some function from $A$ to $\RR$
    \end{enumerate}

    \vspace{1em}
    
    To simplify notation, we define

    
    $$
        \inf f 
        := \inf_{\boldx \in A} f(\boldx) 
    $$

    and

    $$
        \sup f 
        := \sup_{\boldx \in A} f(\boldx) 
    $$

\end{frame}




\begin{frame}

    \Fact 
    %
    $$
    f(\boldx) \leq g(\boldx) \text{ for all } \boldx \in A
        \implies
        \sup f \leq \sup g
    $$

    \vspace{1em}

    Proof: Fix any such functions $f$ and $g$ and any $\boldx \in A$ 
    
    We have 
    % 
    $$
    f(\boldx) \leq g(\boldx) \leq \sup g
    $$

    Hence $\sup g$ is an upper bound for $\setntn{f(\boldx)}{\boldx \in A}$

    Since the supremum is the least upper bound, this gives

    \begin{equation*}
        \sup f \leq \sup g
    \end{equation*}

\end{frame}


\begin{frame}

    \Fact 
    %
    $$
        \sup_{\boldx \in A} (f(\boldx) + g(\boldx)) 
        \leq \sup_{\boldx \in A} f(\boldx) + \sup_{\boldx \in A} g(\boldx)
    $$

    \vspace{1em}

    Proof: Fix any such functions $f$ and $g$ and any $\boldx \in A$ 
    
    We have 
    % 
    $$
    f(\boldx) \leq \sup f
    \quad \text{and} \quad 
    g(\boldx) \leq \sup g
    $$
     % 
     \vspace{-0.5em}
    \begin{equation*}
        \fore
        f(\boldx) + g(\boldx) \leq \sup f + \sup g
    \end{equation*}
     % 
     \vspace{-0.3em}
    \begin{equation*}
        \fore
        \sup (f + g) \leq \sup f + \sup g
    \end{equation*}

\end{frame}





\begin{frame}

    \Fact 
    %
    $$
        | \sup_{\boldx \in A} f(\boldx) - \sup_{\boldx \in A} g(\boldx) | \leq
        \sup_{\boldx \in A} |f(\boldx) - g(\boldx)|
    $$

    \vspace{0.5em}
    Proof: Picking any such $f, g$, we have
    %
    \begin{align*}
        \sup f = \sup (f - g + g) 
           & \leq \sup (f - g) + \sup g
        \\
           & \leq \sup | f - g | + \sup g
    \end{align*}
    %
    \begin{equation*}
        \therefore \; \sup f - \sup g \leq \sup | f - g |
    \end{equation*}

    \vspace{0.5em}

      Same argument reversing roles of $f$ and $g$ finishes the proof

\end{frame}




\begin{frame}
    \frametitle{Introduction}
    

    In this lecture we study topics such as

    \begin{itemize}
        \item Convexity / concavity
                    \vspace{0.4em}
            \begin{itemize}
                \item and uniqueness in optimization
                    \vspace{0.4em}
                \item sufficient conditions for optimality
                    \vspace{0.4em}
                \item how to detect these properties?
            \end{itemize}
    \end{itemize}

                    \vspace{0.4em}

    \begin{itemize}
        \item Zeros of functions
                    \vspace{0.4em}
            \begin{itemize}
                \item solving nonlinear equations
                    \vspace{0.4em}
                \item existence of solutions
                    \vspace{0.4em}
                \item applications
            \end{itemize}
    \end{itemize}

\end{frame}




\section{Convex Sets}

\begin{frame}
   \frametitle{Convex Sets}

   Uniqueness of optima often connected to convexity / concavity

   \begin{itemize}
       \item Convexity is a shape property for sets
           \vspace{0.5em}
       \item Convexity and concavity are shape properties for functions
   \end{itemize}

   However, only one fundamental concept: convex sets

   \vspace{1em}

   A set $C \subset \RR^K$ is called \navy{convex} if
   %
   $$
   \boldx,  \boldy \text{ in } C  \text{ and } 0 \leq \lambda \leq 1
   \; \implies \;
   \lambda \boldx + (1 - \lambda) \boldy \in C
   $$

   Remark: This is vector addition and scalar multiplication

\end{frame}

\begin{frame}

    Convexity $\iff$ line between any two points in $C$ lies in $C$

    \vspace{1em}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\input{convex.pdf_t}}
       \end{center}
    \end{figure}

\end{frame}

\begin{frame}

    A non-convex set

    \vspace{1em}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\input{non_convex.pdf_t}}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \Eg The ``positive cone" $P := \setntn{\boldx \in \RR^K}{ \boldx \geq
    \boldzero }$ is convex

    \vspace{0.8em}

    To see this, pick any $\boldx$, $\boldy$ in $P$ and any $\lambda \in [0, 1]$

    Let $\boldz := \lambda \boldx + (1 - \lambda) \boldy$ and let $z_k :=
    \bolde_k' \boldz$

    Since 
    %
    \begin{itemize}
        \item $z_k = \lambda x_k + (1 - \lambda) y_k$
        \item $x_k \geq 0$ and $y_k \geq 0$
    \end{itemize}
    
    It is clear that $z_k \geq 0$ for all $k$

    Hence $\boldz \in P$ as claimed

\end{frame}


\begin{frame}

    \Eg Every $\epsilon$-ball is convex


    \vspace{0.6em}

    Proof: Fix $\bolda \in \RR^K$, $\epsilon > 0$ and let
    $B_\epsilon(\bolda)$ be the $\epsilon$-ball
    
    Pick any $\boldx$, $\boldy$ in $B_\epsilon(\bolda)$ and any $\lambda \in [0, 1]$

    The point $\lambda \boldx + (1 - \lambda) \boldy$ lies in
    $B_\epsilon(\bolda)$ because
    %
    \begin{align*}
        \| \lambda \boldx + (1 - \lambda) \boldy - \bolda \|
        & = \| \lambda \boldx - \lambda \bolda 
            + (1 - \lambda) \boldy - (1 - \lambda) \bolda \|
        \\
        & \leq \| \lambda \boldx - \lambda \bolda \|
            + \| (1 - \lambda) \boldy - (1 - \lambda) \bolda \|
        \\
        & = \lambda \| \boldx - \bolda \|
            + (1 - \lambda) \| \boldy - \bolda \|
        \\
        & < \lambda \epsilon + (1 - \lambda) \epsilon
        \\
        & = \epsilon
    \end{align*}
    
    
\end{frame}


\begin{frame}
    
    \Eg Let $\boldp \in \RR^K$ and let $M$ be the ``half-space"
    %
    \begin{equation*}
        M := \setntn{\boldx \in \RR^K}{ \boldp' \boldx \leq m}
    \end{equation*}

    The set $M$ is convex

    \vspace{0.5em}

    Proof: Let $\boldp$, $m$ and $M$ be as described

    Fix $\boldx$, $\boldy$ in $M$ and $\lambda \in [0, 1]$ 
    
    Then $\lambda \boldx + (1 - \lambda) \boldy \in M$ because
    %
    \begin{multline*}
        \boldp'[\lambda \boldx + (1 - \lambda) \boldy ] =
         \\
         \lambda \boldp'\boldx + (1 - \lambda) \boldp'\boldy 
         \leq \lambda m + (1 - \lambda) m
            = m
    \end{multline*}

    Hence $M$ is convex

\end{frame}

\begin{frame}
    
    \Fact If $A$ and $B$ are convex, then so is $A \cap B$

    \vspace{1em}

    Proof:  Let $A$ and $B$ be convex and let $C := A \cap B$

    Pick any $\boldx$, $\boldy$ in $C$ and any $\lambda \in [0, 1]$

    Set 
    %
    $$\boldz := \lambda \boldx + (1 - \lambda) \boldy$$

    Since $\boldx$ and $\boldy$ lie in $A$ and $A$ is convex we have $\boldz
    \in A$

    Since $\boldx$ and $\boldy$ lie in $B$ and $B$ is convex we have $\boldz
    \in B$

    Hence $\boldz \in A \cap B$

\end{frame}

\begin{frame}

    \Eg Let $\boldp \in \RR^K$ be a vector of prices and consider the budget set
    %
    $$
        B(m) := \setntn{\boldx \in \RR^K}{ \boldx \geq \boldzero \text{ and }
        \boldp' \boldx \leq m}
    $$

    The budget set $B(m)$ is convex

    \vspace{1em}

    To see this, note that $B(m) = P \cap M$ where
    %
    $$
        P := \setntn{\boldx \in \RR^K}{ \boldx \geq \boldzero }
        \qquad
        M := \setntn{\boldx \in \RR^K}{ \boldp' \boldx \leq m}
    $$
    
    We already know that
    %
    \begin{itemize}
        \item $P$ and $M$ are convex, intersections of convex sets are convex
    \end{itemize}

    Hence $B(m)$ is convex
    
    
\end{frame}


\section{Convex/Concave Functions}


\begin{frame}
    \frametitle{Convex Functions}

    Let $A \subset \RR^K$ be a convex set and let  $f$ be a function from $A$ to $\RR$

    \vspace{1em}

    $f$ is called \navy{convex} if 
            $$
                f(\lambda \boldx + (1 - \lambda) \boldy)
                    \leq \lambda f(\boldx) + (1 - \lambda) f(\boldy)
            $$
    for all $\boldx, \boldy \in A$ and all $\lambda \in [0, 1]$

    \vspace{1em}

    $f$ is called \navy{strictly convex} if 
            $$
                f(\lambda \boldx + (1 - \lambda) \boldy)
                < \lambda f(\boldx) + (1 - \lambda) f(\boldy)
            $$
    for all $\boldx, \boldy \in A$ with $\boldx \not= \boldy$ and all $\lambda \in (0, 1)$

\end{frame}




\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\input{convex_function.pdf_t}}
        \vspace{1em}
        \caption{A strictly convex function on a subset of $\RR$}
       \end{center}
    \end{figure}

\end{frame}




\begin{frame}

    \Fact $f \colon A \to \RR$ is convex if and only if its \navy{epigraph}
    %
    $$
    E_f := \setntn{(\boldx, y) \in A \times \RR}{f(\boldx) \leq y}
    $$
    is a convex subset of $\RR^K \times \RR$


    \begin{figure}
       \begin{center}
           \scalebox{.4}{\includegraphics{epigraph.pdf}}
       \end{center}
    \end{figure}
    
\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center}
        \scalebox{.4}{\includegraphics{qform_pd.pdf}}
        \caption{A strictly convex function on a subset of $\RR^2$}
       \end{center}
    \end{figure}

\end{frame}


\begin{frame}
    
    \Eg $f(\boldx) = \|\boldx\|$ is convex on $\RR^K$ 
    
    To see this recall that, by the properties of norms,
    %
    \begin{align*}
        \|\lambda \boldx + (1 - \lambda) \boldy\|
        & \leq \|\lambda \boldx\| + \|(1 - \lambda) \boldy\|
        \\
        & = \lambda \|\boldx\| + (1 - \lambda) \|\boldy\|
    \end{align*}
    %
    That is,
    $$
    f(\lambda \boldx + (1 - \lambda) \boldy)
        \leq 
        \lambda f(\boldx) + (1 - \lambda) f(\boldy)
    $$

    \vspace{2em}

    \Eg $f(x) = \cos(x)$ is \underline{not} convex on $\RR$ because
    %
    $$
    1 = f(2\pi) = f(\pi/2 + 3\pi/2) > f(\pi)/2 + f(3\pi)/2 =  -1
    $$


\end{frame}


\begin{frame}

    \Fact If $\boldA$ is $K \times K$ and positive definite, then 
    %
    \begin{equation*}
        Q(\boldx) = \boldx' \boldA \boldx
        \qquad (\boldx \in \RR^K)
    \end{equation*}
    %
    is strictly convex on $\RR^K$

    Proof: Fix $\boldx, \boldy \in \RR^K$ with $\boldx \not= \boldy$ and $\lambda \in (0, 1)$

    \Ex Show that 
    %
    \begin{align*}
        \lambda Q(\boldx) + (1 - \lambda) Q(\boldy)
        & - Q(\lambda \boldx + (1 - \lambda) \boldy)
        \\
        & = \lambda (1 - \lambda) (\boldx - \boldy)' \boldA (\boldx - \boldy)
    \end{align*}


    Since $\boldx - \boldy \not= \boldzero$ and $0 < \lambda < 1$, the right
    hand side is $> 0$

    Hence
    %
    \begin{equation*}
        \lambda Q(\boldx) + (1 - \lambda) Q(\boldy)
        > Q(\lambda \boldx + (1 - \lambda) \boldy)
    \end{equation*}
    
    

\end{frame}



\begin{frame}
    \frametitle{Concave Functions}

    Let $A \subset \RR^K$ be a convex and let  $f$ be a function from $A$ to $\RR$

    \vspace{1em}

    $f$ is called \navy{concave} if 
            $$
                f(\lambda \boldx + (1 - \lambda) \boldy)
                    \geq \lambda f(\boldx) + (1 - \lambda) f(\boldy)
            $$
    for all $\boldx, \boldy \in A$ and all $\lambda \in [0, 1]$

    \vspace{1em}

    $f$ is called \navy{strictly concave} if 
            $$
                f(\lambda \boldx + (1 - \lambda) \boldy)
                > \lambda f(\boldx) + (1 - \lambda) f(\boldy)
            $$
    for all $\boldx, \boldy \in A$ with $\boldx \not= \boldy$ and all $\lambda \in (0, 1)$

\end{frame}



\begin{frame}

    \Ex Show that 
    %
    \begin{enumerate}
        \item $f$ is concave if and only if $-f$ is convex
            \vspace{0.5em}
        \item $f$ is strictly concave if and only if $-f$ is strictly convex
    \end{enumerate}
    
            \vspace{2em}
    
    \Fact $f \colon A \to \RR$ is concave if and only if its \navy{hypograph}
    %
    $$
        H_f := \setntn{(\boldx, y) \in A \times \RR}{f(\boldx) \geq y}
    $$
    is a convex subset of $\RR^K \times \RR$


\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.5}{\includegraphics{hypograph.pdf}}
       \end{center}
    \end{figure}
    
\end{frame}

\begin{frame}
    \frametitle{Preservation of Shape}

    Let $A \subset \RR^K$ be convex and let $f$ and $g$ be functions from $A$
    to $\RR$

    \vspace{1em}

    \Fact If $f$ and $g$ are convex (resp., concave) and $\alpha \geq 0$, then
    %
    \begin{itemize}
        \item $\alpha f$ is convex (resp., concave)
        \item $f + g$ is convex (resp., concave)
    \end{itemize}

    \vspace{1em}
    \Fact If $f$ and $g$ are strictly convex (resp., strictly concave) and $\alpha > 0$, then
    %
    \begin{itemize}
        \item $\alpha f$ is strictly convex (resp., strictly concave)
        \item $f + g$ is strictly convex (resp., strictly concave)
    \end{itemize}

\end{frame}

\begin{frame}
    
    Let's prove that $f$ and $g$ convex $\implies h := f + g$ convex

    \vspace{0.6em}

    Pick any $\boldx, \boldy \in A$ and $\lambda \in [0, 1]$

    We have
    %
    \begin{align*}
        h(\lambda \boldx + (1 - \lambda) \boldy)
        & = f(\lambda \boldx + (1 - \lambda) \boldy)
        + g(\lambda \boldx + (1 - \lambda) \boldy)
        \\
        & \leq 
        \lambda f(\boldx) + (1 - \lambda) f(\boldy)
        +
        \lambda g(\boldx) + (1 - \lambda) g(\boldy)
        \\
        & =
        \lambda [f(\boldx) + g(\boldx)]
        + (1 - \lambda) [f(\boldy) +  g(\boldy)]
        \\
        & =
        \lambda h(\boldx) + (1 - \lambda) h(\boldy)
    \end{align*}

    Hence $h$ is convex

\end{frame}



\begin{frame}
    \frametitle{Derivative Conditions}
    
    The $i,j$-th cross partial of $f \colon A \to \RR$ at $\boldx \in A$ is
    %
    \begin{equation*}
        f_{ij}(\boldx)
            := \frac{\partial^2}{\partial x_i \partial x_j} 
            f(\boldx)
            \qquad (1 \leq i, j \leq K)
    \end{equation*}

    We say that $f$ is \navy{a $C^2$ function} if these partials are all
    continuous in $\boldx$ for all $\boldx \in A$

    The \navy{Hessian matrix} of $f$ at $\boldx$ is the matrix of cross
    partials
    %
    \begin{equation*}
        H(\boldx)
        :=
        \begin{pmatrix}
            f_{11}(\boldx)  & \cdots & f_{1K}(\boldx)  \\
                            & \vdots &                 \\
            f_{K1}(\boldx)  & \cdots & f_{KK}(\boldx)   
        \end{pmatrix}
    \end{equation*}

\end{frame}



\begin{frame}
    
    \Fact If $f \colon A \to \RR$ is a $C^2$ function where $A \subset \RR^K$
    is open and convex, then
    %
    \begin{enumerate}
        \item $H(\boldx)$ nonnegative definite for all $\boldx \in A$
            $\iff f$ convex
            \vspace{0.5em}
        \item $H(\boldx)$ nonpositive definite for all $\boldx \in A$
            $\iff f$ concave
    \end{enumerate}
    %
            \vspace{0.5em}
    In addition,
    %
    \begin{enumerate}
        \item $H(\boldx)$ positive definite for all $\boldx \in A$
            $\implies f$ strictly convex
            \vspace{0.5em}
        \item $H(\boldx)$ negative definite for all $\boldx \in A$
            $\implies f$ strictly concave
    \end{enumerate}
    %

    \vspace{1em}

    Proof: Omitted

\end{frame}


\begin{frame}
    
    \Eg Let $A := (0, \infty) \times (0, \infty)$ and let $U \colon A \to \RR$ be the
    utility function
    %
    \begin{equation*}
        U(c_1, c_2) = \alpha \ln c_1 + \beta \ln c_2
    \end{equation*}

    Assume that $\alpha$ and $\beta$ are both strictly positive

    \Ex Show that the Hessian at $\boldc := (c_1, c_2) \in A$ has the form
    %
    \begin{equation*}
        H(\boldc)
        :=
        \begin{pmatrix}
            - \frac{\alpha}{c_1^2}  &  0  \\
            0  &  - \frac{\beta}{c_2^2}  
        \end{pmatrix}
    \end{equation*}

    \Ex Show that any diagonal matrix with strictly negative elements along
    the principle diagonal is negative definite

    Conclude that $U$ is strictly concave on $A$

\end{frame}



\section{Uniqueness of Optimizers}


\begin{frame}
    \frametitle{Uniqueness of Maximizers and Minimizers}

    \vspace{0.5em}

    Let $A \subset \RR^K$ be convex and let $f \colon A \to \RR$

    \vspace{0.5em}

    \Facts
    %
    \begin{enumerate}
        \item If $f$ is strictly convex, then $f$ has at most one minimizer on $A$
            \vspace{0.5em}
        \item If $f$ is strictly concave, then $f$ has at most one maximizer on $A$
    \end{enumerate}

            \vspace{1.0em}

    Interpretation, strictly concave case:

    \begin{itemize}
        \item we don't know in general if $f$ has a maximizer
            \vspace{0.5em}
        \item but if it does, then it has exactly one
            \vspace{0.5em}
        \item in other words, we have uniqueness
    \end{itemize}

\end{frame}


\begin{frame}
    
    Proof for the case where $f$ is strictly concave:

    Suppose to the contrary that 
    %
    \begin{itemize}
        \item $\bolda$ and $\boldb$ are distinct points in $A$
            \vspace{0.5em}
        \item both are maximizers of $f$ on $A$
    \end{itemize}
    
    By the def of maximizers, $f(\bolda) \geq f(\boldb)$ and $f(\boldb) \geq f(\bolda)$

    Hence we have $f(\bolda) = f(\boldb)$

    By strict concavity, then
    %
    \begin{equation*}
        f\left( \frac{1}{2} \bolda + \frac{1}{2} \boldb \right)
        > \frac{1}{2} f( \bolda) + \frac{1}{2} f( \boldb)
        = \frac{1}{2} f( \bolda) + \frac{1}{2} f( \bolda)
        = f(\bolda)
    \end{equation*}

    This contradicts the assumption that $\bolda$ is a maximizer

\end{frame}


\begin{frame}
    \frametitle{A Sufficient Condition}

    We can now restate more precisely optimization results stated in the
    introductory lectures

    Let $f \colon A \to \RR$ be a $C^2$ function where $A \subset \RR^K$
    is open, convex

    Recall that $\boldx^* \in A$ is a stationary point of $f$ if
    %
    \begin{equation*}
            \frac{\partial}{\partial x_i} 
            f(\boldx^*)
            = 0
            \quad \text{for all $i$ in }  1, \ldots,  K
    \end{equation*}

    \Fact If $f$ and $A$ are as above and $\boldx^* \in A$ is stationary, then
    %
    \begin{enumerate}
        \item $f$ strictly concave $\implies$ $\boldx^*$ is the unique maximizer of $f$ on $A$
            \vspace{0.5em}
        \item $f$ strictly convex $\implies$ $\boldx^*$ is the unique
            minimizer of $f$ on $A$
    \end{enumerate}
    %

\end{frame}

\begin{frame}
    
    \begin{figure}
       \begin{center}
           \scalebox{.5}{\includegraphics{concave_max.pdf}}
       \end{center}
    \end{figure}

\end{frame}


